{
    "article_id": "729004",
    "article_name": "Как устроен massively parallel processing (MPP) в Trino",
    "content": "Из нашей повседневной практики доподлинно известно, что массивно(массово?)-параллельные вычисления это круто. Но что именно означает этот термин, и как \"массивность\" и \"параллельность\" реализованы в конкретной системе? В данной статье мы ответим на оба вопроса, проанализировав внутреннюю архитектуру популярного MPP-движка для больших данных \nTrino\n.\nЧто такое MPP?\nПопулярные термины зачастую скрывают за собой вязкую смесь технологий и маркетинга, за которыми не всегда легко уловить суть. Напишем в поисковике запрос \"massively parallel processing\" и прочитаем несколько технических статей от известных вендоров. \nКомпания IBM \nпишет\n:\nParallel processing environments are categorized as \nsymmetric multiprocessing\n (\nSMP\n) or \nmassively parallel processing\n (\nMPP\n) systems.\nIn a symmetric multiprocessing (SMP) environment, multiple processors share other hardware resources.\nIn a massively parallel processing (MPP) system, many computers are \nphysically housed in the same chassis\n.\nВсе неплохо, но причем тут \"chassis\"? Далее в этой же статье:\nIn an MPP environment, performance is improved because \nno resources must be shared\n among physical computers.\nIn an MPP system, a file system is commonly \nshared\n across the network.\nСтоп, стоп! Так shared или не shared?\nЛадно, посмотрим на документацию \nGreenplum\n:\nMPP (\nalso known as a \nshared nothing\n architecture\n) refers to systems with two or more processors that cooperate to carry out an operation, each processor with its own memory, operating system \nand disks\n. \nЗдесь коллеги из Pivotal скромно ставят знак равенства между \"massively parallel processing\" и \"shared nothing\", что конечно же некорректно в 2023 году. Например, как нам быть со Snowflake, который тоже \nзаявляет\n о своей \"массивности\", но не использует shared-nothing архитектуру?\nSnowflake’s architecture is a hybrid of traditional shared-disk and shared-nothing database architectures. Similar to shared-disk architectures, Snowflake uses a central data repository for persisted data that is accessible from all compute nodes in the platform. But similar to shared-nothing architectures, Snowflake processes queries using MPP (massively parallel processing) compute clusters where each node in the cluster stores a portion of the entire data set locally. \nНа самом деле, за всем многообразием MPP-систем скрывается компактный общий знаменатель, хорошо описанный в \nВикипедии\n:\nМассово-параллельная архитектура — класс архитектур параллельных вычислительных систем. Особенность архитектуры состоит в том, что \nпамять физически разделена\n.\nЕще одно емкое определение от \nMicrosoft\n:\nMassively Parallel Processing (MPP) is the coordinated processing of a single task by multiple processors, each processor using its \nown OS and memory\n and communicating with each other using some form of messaging interface.\nИными словами, это MPP — это несколько машин, скоординированно выполняющих одну и ту же задачу. Очевидно, что под это определение подходит большое количество продуктов порой с совершенно разными архитектурами и характеристиками.\nРассмотрим реализацию идей MPP на примере Trino.\nЧто такое Trino?\nTrino\n — это распределенный аналитический SQL-движок для выполнения федеративных запросов. Технически Trino представляет собой набор вычислительных узлов, которые подключаются ко внешним источникам данных. При получении SQL-запроса, Trino определяет, какие источники данных задействованы, подтягивает необходимые данные из источников, после чего осуществляет финальную обработку в кластере и отдает результат пользователю.\nТаким образом, Trino можно отнести к классу MPP-движков (много машин) с shared storage (данные находятся удаленно) архитектурой. Ну а наша команда занимается разработкой продукта \nCedrusData\n — форк Trino на стероидах для российского рынка, где нашим основным фокусом на текущий год является улучшение производительности.\nРеализация MPP в Trino\nОдна из важных \"фишек\" Trino — возможность запускать SQL-запросы к озерам данных (напр., S3-совместимому хранилищу). Рассмотрим реализацию MPP в Trino на примере SQL-запроса к набору файлов Parquet в озере данных.\nApache Parquet\n - это популярный колоночный формат хранения данных. Отдельный файл Parquet хранит набор записей. Большие датасеты обычно организованы в несколько файлов Parquet. Аналитические движки вроде Trino или Apache Spark рассматривают совокупность таких файлов как единую логическую \"таблицу\".\nРассмотрим выполнение следующего SQL-запроса к озеру данных (схема \nTPC-DS\n):\nSELECT d_year, d_moy, SUM(ss_net_paid)\nFROM store_sales JOIN date_dim ON ss_sold_date_sk = d_date_sk\nGROUP BY d_year, d_moy\nWHERE ss_customer_sk = <...>\nВ данном запросе мы считаем объем продаж по месяцам для определенного клиента, соединяя таблицу фактов \nstore_sales\n со справочником \ndate_dim\n. Таблица \nstore_sales\n представляет собой большое количество Parquet файлов, партиционированных по дате. Таблица \ndate_dim\n представлена единственным файлом Parquet.\n├── date_dim\n    └── part1.parquet\n├── store_sales\n│   ...\n│   ├── ss_sold_date_sk=2450836\n│   │   ├── part1.parquet\n│   │   └── part2.parquet\n│   ├── ss_sold_date_sk=2450837\n│   │   ├── part1.parquet\n│   │   ├── part2.parquet\n│   │   └── part3.parquet\n│   ...   \nПараллелизм фрагментов\nРаспределенные SQL-движки выполняют запросы, задействуя несколько узлов. Для выполнения некоторых операций узлы должны обмениваться друг с другом промежуточными результатами. Например, для выполнения операции агрегации необходимо убедиться, что все записи с одними и теми же значениями атрибутов \nGROUP BY\n оказались на одном узле.\nДля организации выполнения распределенного запроса планировщик движка анализирует, какое распределение данных необходимо конкретным операторам, и вставляет специальные операторы \nExchange\n (\nShuffle\n), которые перераспределяют данные в кластере по мере необходимости. Далее план запроса \"нарезается\" на фрагменты по границам \nExchange\n таким образом, что каждый фрагмент имеет строго один \"выход\" и один или несколько \"входов\" (таблица или другой фрагмент). Подробнее про операторы Exchange/Shuffle можно почитать в нашем \nблоге\n.\nВ случае Trino план до оптимизации будет выглядеть следующим образом:\nAggregation\n  Join\n    TableScan[store_sales]\n    TableScan[date_dim]\nЗа расстановку операторов \nExchange\n в Trino отвечает правило \nAddExchanges\n, которое определяет требования операторов к распределению входящего потока данных, и вставляет \nExchange\n там, где требуемое и текущее распределения не совпадают. \nК слову, в общем случае существует несколько допустимых расстановок \nExchange\n для конкретного SQL-запроса (подумайте, какие есть альтернативные расстановки в рассматриваемом запросе, и напишите их в комментариях). Стоимость выполнения планов с разными расстановками \nExchange\n может сильно варьировать, а значит нам нужна \ncost-based оптимизация\n. К сожалению, Trino расставляет \nExchange\n эвристически. Нам это не нравится, поэтому в CedrusData мы делаем \nновый cost-based оптимизатор\n на основе алгоритма Cascades (пользуясь случаем, передаем привет \nApache Calcite\n и \ngporca\n). \nВернемся к основной теме. План Trino после работы правила \nAddExchanges\n:\nAggregation[FINAL]              // Финальная агрегация\n  Exchange[HASH[d_year, d_moy]] // Перераспределение по [d_year, d_moy]\n    Aggregation[PARTIAL]        // Предварительная агрегация по [d_year, d_moy]\n      Join\n        Filter[ss_customer_sk]\n          TableScan[store_sales]\n        Exchange[BROADCAST]     // Доставить данные из date_dim к store_sales\n          TableScan[date_dim]\nПлан после разбиения на фрагменты (см. \nPlanFragmenter\n):\n-- Fragment 1: \nExchange[BROADCAST]\n  TableScan[date_dim]\n\n-- Fragment 2:\nExchange[HASH[d_year, d_moy]]\n  Aggregation[PARTIAL]\n    Join\n      Filter[ss_customer_sk]\n        TableScan[store_sales]\n      RemoteFragment[id=1]\n\n-- Fragment 3:\nAggregation[FINAL]\n  RemoteFragment[id=2]\nНа данном этапе мы впервые задумываемся о параллелизме. В простейшем случае мы можем выполнять фрагменты один за другим согласно графу зависимостей: \n1->2->3\n . Однако, если фильтр по \nss_customer_sk\n является высокоселективным, более выгодной стратегией может оказаться одновременное выполнение первого и второго фрагментов.\nВ Trino за выбор стратегии вертикального параллелизма фрагментов отвечает компонент \nExecutionPolicy\n. \nИсторически Trino использовал \nнаивный алгоритм\n, который безусловно запускал все фрагменты параллельно. Подобная стратегия плохо работает в ряде случаев, так как множество фрагментов начинают конкурировать за ограниченные ресурсы кластера (сеть, CPU, RAM). Например, это особенно актуально для \nдинамических фильтров\n, про которые мы подробно поговорим в другой раз.\nСейчас Trino использует так называемый \nphased алгоритм\n. Данный алгорит старается отложить выполнение тех фрагментов, которые с большой вероятностью будут заблокированы ожиданием результатов от других фрагментов. В большинстве случаев это приводит к ускорению запросов: \"нижние\" фрагменты сталкиваются с меньшей конкуренцией за ресурсы, быстрее завершают свою работу, а следовательно быстрее разблокируют \"родителей\". \nВ нашем примере, если проигнорировать нюансы работы динамических фильтров (про которые, как было сказано выше, мы подробно поговорим в другой раз), Trino посчитает, что первый и второй фрагменты можно выполнять параллельно. Таким образом, сканирование обеих таблиц будет происходить одновременно. Выполнение третьего фрагмента же начнется по мере появления результатов промежуточных агрегаций из второго фрагмента.\nПараллелизм чтения данных\nВторой важнейший источник \"массивности\" параллелизма в Trino — параллельное чтение данных. Ключевой абстракцией является \nSplit\n — часть данных таблицы источника, которая может быть отсканирована независимо. В процессе анализа запроса Trino обращается через специальный SPI к метаданным источника, что бы получить \nсписок сплитов\n для конкретной таблицы. Чем больше сплитов можно выделить для таблицы, тем выше максимально возможный уровень параллелизма чтения.\nЛогика разбиения таблицы на сплиты зависит от типа источника. Рассмотрим второй фрагмент, в котором происходит чтение большой таблицы \nstore_sales\n, состоящей из нескольких файлов Parquet. Сначала Trino обращается к Hive Metastore и файловой системе озера (HDFS, S3, и т.п.) для получения списка файлов таблицы. В простейшем случае одному сплиту будет соответствовать один файл. Если конкретный файл является достаточно большим, имеет смысл разбить его на несколько непересекающихся диапазонов. В таком случае, для одного файла может быть создано несколько сплитов. В Trino за выбор размера сплита в озере данных отвечают \nпараметры\n \nhive.max-initial-split-size\n и \nhive.max-split-size\n . По умолчанию, Trino будет стремиться \"нарезать\" файл на сплиты размером от 32 до 64 мегабайт. Например:\nSplit 1: /stores_sales/ss_sold_date_sk=2450836/part1.parquet\nSplit 2: /stores_sales/ss_sold_date_sk=2450836/part2.parquet [offset=0, length=32Mb]\nSplit 3: /stores_sales/ss_sold_date_sk=2450836/part2.parquet [offset=32Mb, length=64Mb]\nВажно отметить, что такое грубое разбиение файлов на части может не соответствовать их внутренней структуре. Например, поставленное фактически наобум смещение в 32Mb почти никогда не будет совпадать с границей \nrow group\n Parquet. Это не является проблемой: в процессе обработки сплита Trino корректно скорректирует границы чтения файла таким образом, что бы ничего не пропустить, и не сделать дубликаты.\nВ общем случае таблица может состоять из тысяч файлов, поэтому получение списка сплитов является асинхронным процессом.\nПо мере появления информации о новых сплитах Trino начинает рассылать запросы на обработку сплитов по узлам. При этом Trino отслеживает текущее состояние узлов, и пытается сбалансировать распределение сплитов таким образом, что бы нагрузка распределялась по узлам равномерно. Это позволяет Trino избегать ситуаций \"hot partition\", характерных для shared-nothing систем вроде Greenplum, когда неравномерность распределения нагрузки по партишенам (например, разные размеры партишенов или особый интерес пользователей к данным из конкретного партишена) приводит к перегрузке отдельных узлов.\nПример распределения сплитов по узлам.\nСтоит отметить, что в Trino так же присутствует возможность задать более строгие правила распределения сплитов по узлам, но об этом мы подробнее поговорим в другой \"другой\" раз.\nЛокальный параллелизм\nНам осталось рассмотреть, как Trino выполняет фрагменты. Фрагмент по своей сути является набором операторов, через которые необходимо \"прогнать\" данные, поступающие из таблиц или других фрагментов (сплиты).\nНекоторые операторы фрагмента могут быть блокирующими. Оператор называется блокирующим, если он может произвести первый результат только после получения всех входных данных от дочернего оператора. Примером такого оператора является построение хэш-таблицы для выполнения hash join. Trino предварительно разбивает фрагмент по границам блокирующих операторов на отдельные пайплайны (\npipeline\n). Например, второй фрагмент в нашем запросе может быть разбит на два пайплайна:\nExchange[HASH[d_year, d_moy]]\n  Aggregation[PARTIAL]\n    Join\n      Filter[ss_customer_sk]\n        TableScan[store_sales]\n      RemoteFragment[id=1]\nПример разбиения фрагмента на пайплайны.\nПосле получения сплита на конкретном узле, Trino определяет, какому пайплайну он принадлежит. Далее Trino создает исполняемый экземпляр пайплайна, называемый драйвер (\nDriver\n), и ставит его в очередь на исполнение. Таким образом, в рамках одного узла Trino может одновременно выполнять несколько драйверов, которые представляют разные пайплайны, разные фрагменты и, разумеется, разные запросы. Поистине массивный параллелизм!\nДрайвер представляет собой исполняемое дерево физических операторов, и сплит, который необходимо обработать. Сплиты могут иметь различное количество ассоциированных с ними данных (например, большой или маленький файлы). Кроме того, некоторые драйвера могут требовать значительно больше времени для обработки данных, чем другие. Таким образом, Trino необходимо управлять порядком и временем работы драйверов.\nДля определения порядка запуска драйверов Trino использует многоуровневую priority queue (\nMultilevelSplitQueue\n). Для каждого драйвера идет подсчет суммарного времени выполнения. По мере того как драйвер накапливает время, он последовательно перемещается между уровнями очереди: от нулевого к четвертому. Планировщик стремится распределить время выполнения таким образом, что бы драйвера на нулевом уровне получали в среднем в два раза больше времени, чем драйвера на первом уровне, в четыре раза больше времени, чем драйвера на втором уровне, и т.д. Мультипликатор можно изменить с помощью параметра конфигурации \ntask.level-time-multiplier\n. Таким образом, Trino отдает предпочтение более \"легким\" запросам, но в то же время и длительные запросы гарантированно рано или поздно получат свой квант времени.\nЦелевое распределение времени по уровням.\nВыбор очередного драйвера для исполнения состоит из двух шагов:\nОпределить уровень, который должен получить следующий квант процессорного времени.\nВзять из выбранного уровня драйвер, который до сих пор получил наименьшее количество процессорного времени.\nПосле того как драйвер выбран, начинается его выполнение. Планировщик отводит драйверу квант времени, равный одной секунде. Если квант времени истек (см. \nDriverYeildSignal\n), или драйвер не может продолжать свое выполнение (например, закончились входные данные или заполнен output buffer), выполнение драйвера останавливается, и он снова попадает в очередь на исполнение. Таким образом, блокировка драйвера не приводит к снижению общей пропускной способности узла.\nПроцесс повторяется до тех пор, пока драйвер не завершит обработку сплита. Квантование времени позволяет Trino гарантировать (с поправкой на непредсказуемые блокирующие операции чтения из источника), что один или несколько тяжелых запросов не смогут надолго заблокировать прогресс конкурентных задач.\nВыводы\nМассивно(массово?)-параллельная архитектура, в которой несколько вычислительных узлов занимаются выполнением общей задачи, является де-факто стандартом обработки больших объемов данных. \nВ Trino за \"массивность\" отвечают следующие технические решения:\nВертикальный параллелизм выполнения запроса за счет разбиения на фрагменты.\nВертикальный параллелизм выполнения фрагмента на узле за счет разбиения на пайплайны.\nГоризонтальный параллелизм выполнения пайплайнов за счет разбиения данных таблицы на сплиты, обрабатываемые драйверами.\nЛокальный scheduler, который параллельно выполняет множество драйверов.\nВсе вместе это позволяет Trino быстро обрабатывать большие объемы данных. Используйте Trino для бигдаты, и бигдата скажет вам спасибо.\nВ следующей статье мы подробно рассмотрим реализацию одной из важнейших оптимизаций в Trino, без которой никакой MPP не сможет обеспечить нам приемлемую производительность — \nдинамические фильтры\n. До встречи!\n \n ",
    "tags": [
        "trino",
        "mpp",
        "sql",
        "bigdata"
    ]
}