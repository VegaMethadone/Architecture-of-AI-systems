{
    "article_id": "726504",
    "article_name": "Обучаем с помощью LlamaIndex и OpenAI GPT-3 отвечать по вашей базе знаний",
    "content": "От переводчика.\nУ меня накопилось куча всяких там данных, документов, pdf, doc, видосов на ютюбе,  которые я бы хотел проиндексировать, и чтобы можно было по этой базе знаний у нейронки что-нибудь спрашивать. \nТак же статья может пригодиться, если вы хотите собрать базу знаний по какой-то компании и затем заставить нейронку отвечать на вопросы пользователей. Например, чтобы ИИ прочитала кучу скучной документации, регламентов работы и прочего. \nПока выбираю, на чем это лучше сделать. Вот наткнулся на нижеследующий вариант, который решил попробовать.\nИспользование chatGPT с вашими документами, всякими файлами, изображениями и даже видео\nМышоЛама используя Midjourney \nЯ был очень очарован, когда ChatGPT недавно выпустил свои плагины. Несмотря на множество объявлений microsoft, Google и других технологических компаний, OpenAI опять впереди планеты всей.\nОдин конкретный случай, который меня очень заинтересовал, заключался в том, что вы можете передавать ChatGPT свои файлы и документы, а потом запрашивать их с помощью плагина извлечения. Я был заинтригован и  хотел прикинуть, как реализовать что-то подобное.\nВ конце концов, это привело меня к \nLlamaIndex\n, классному проекту, который обеспечивает интерфейс между LLM по вашему выбору и вашими собственными данными. Я был удивлен и впечатлен различными типами источников данных и широким спектром поддержки, которая доступна, как для разных типов файлов на вашем компьютере, так и для различных источников данных через Интернет. Итак, мне пришлось написать программу, чтобы все это попробовать.\nPowerpoint, PDF и Word\nВот пример того, как я использовал LlamaIndex для различных типов документов на моем компьютере. В этом случае я использовал его с Powerpoint, PDF и документом Word соответственно.\nPowerpoint, PDF и Word   также можно использовать для наборов данных. \nCSV\nВот пример использования \nнабора данных CSV, который содержит данные о различных странах\n, которые я случайным образом нашел в Интернете.\nИзображения\nВы даже можете использовать его на изображениях! Я сфотографировал квитанцию из медицинской клиники, которую недавно посетил, используя свой телефон, и заюзал фотку в качестве данных. Я также сфотографировал страницу из эпохального труда \"Мифический человеко-месяц, или Как создаются программные системы\", которая просто лежала на моем столе (длинная история, в другой раз).\nYouTube\nСамым большим сюрпризом может быть то, что я могу даже использовать его на видео! Я взял \nвидео с YouTube\n (недавнее видео с плодовитого канала Max Tech), преобразовал его в MP4 и использовал его в качестве данных.\nПросто выдрал субтитры видео с помощью \nWhisper\n и, что примечательно, достаточно точно. \nПрежде чем показывать код, давайте сделаем небольшую остановку, чтобы понять, как все это работает.\nКак работает LlamaIndex\nБольшие языковые модели (LLM) работают удивительно хорошо, обучаясь на огромных объемах данных. Не смотря на то что LLM работает хорошо, очень часто он просто не имеет доступа к определенным данным и, следовательно, не всегда полезен, так как банально не обладает нужными знаниями.\nДля того чтобы ему предоставить эти данные, LLM можно \nточно настроить\n. Это означает, что LLM предоставляются дополнительные данные, относящиеся к конкретной задаче. Тонкая настройка предполагает изменение всех параметров модели. Хотя объем данных может быть небольшим, объем изменений довольно значителен, особенно если это большой LLM, такой как GPT-3 с 175 миллиардами параметров.\nДругой способ предоставления данных заключается в проектировании подсказки определенным образом, позволяющим предоставлять данные LLM без тонкой настройки. Исследователи ИИ еще не полностью понимают, как в этом случае поведет себя LLM, но они обнаружили, что если они предоставят список пар ввода-вывода и, наконец, предоставят входные данные, LLM может дать красивые ответы. Этот вид инженерии подсказок называется \nконтекстным обучением\n. Буквально, вы обучаете модель через разработку подсказки определенным образом.\nИ вот как работает LlamaIndex. Он предоставляет соединители с различными источниками данных, которые могут быть отправлены в индексы, хранящиеся в виде файлов в файловой системе или в векторных базах данных, таких как \nPinecone\n или \nWeaviate\n. Вы запрашиваете эти индексы, а затем используете результаты в качестве контекста вместе с вашей подсказкой, которые затем отправляете в LLM, в данном случае GPT-3 OpenAI, по умолчанию.\nПереходим к коду\nПока все это звучит довольно просто, не так ли?  Я решил назвать предстоящий проект \nKancil\n (\nKancil\n, в честь \nсамого маленького копытного животного в мире, неуловимого мышиного оленя или шевротайна\n). Ну, это так, чисто для прикола.\nЗагрузка файлов в индекс\nKancil\n состоит из 2 частей. Во-первых, загрузка файлов в индекс. Я мог бы сделать его удобным и наворотить еще интегрирование с веб-приложением, но я поленился и в конечном итоге загрузил файлы в каталог с помощью скрипта Python.\nУ меня есть файл, который я вызываю для загрузки файлов из каталога load (указывается через переменные среды или файл)\nload.py.env\nОт переводчика\nВ переменной окружения вам надо будет прописать значения для:\nOPENAI_API_KEY (гуглите, если нету, бесплатно)\nLOAD_DIR (папка, откуда брать данные)\nINDEX_FILE (путь к файлу, где будет храниться индекс, напр. /tmp/index.json)\ngui  - папочка gui юзается во фласке\nimport os\nfrom llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\nfrom dotenv import load_dotenv\nload_dotenv()\n\nprint(\"Loading files in the load directory...\")\n# Load and save the index to disk\ndocuments = SimpleDirectoryReader(\n    input_dir=os.environ[\"LOAD_DIR\"]).load_data()\nindex = GPTSimpleVectorIndex.from_documents(documents)\n\n# Save the index to disk\nindex.save_to_disk(os.environ[\"INDEX_FILE\"])\nprint(\"Done.\")\nКод относительно прост. Я читаю все файлы в каталоге с помощью \nSimpleDirectoryReaderDocument\n, а затем загружаю  полученные данные в index. Затем я создаю индекс из документов и сохраняю его на диск. Этот индекс будет использоваться веб-приложением позже.\nЗапуск веб-приложения\nЯ использовал Flask для создания веб-приложения. Я просто запускаю простенький сервер  localhost на порту 3838, накидал я это с помощью модуля webbrowser  и назвал файл kancil.py  \nimport logging\nimport webbrowser\nfrom contextlib import redirect_stdout\nfrom io import StringIO\nfrom dotenv import load_dotenv\nload_dotenv()\n\nfrom server import server\n\nlogger = logging.getLogger(__name__)\n\nif __name__ == '__main__':\n    # open browser in a separate thread\n    with redirect_stdout(StringIO()):            \n        webbrowser.open(\"http://localhost:3838\")\n    # start server\n    server.run(\"127.0.0.1\", 3838, debug=True)\nСамо приложение находится в файле server.py . Во-первых, я загружаю сохраненный файл индекса или создаю индекс с нуля, если он еще не существует. Затем, используя индекс, я вызываю метод и отправляю ему запрос:\nimport os\nfrom llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\nfrom flask import Flask, render_template, jsonify, request\n\nindex = None\n# set up the index, either load it from disk to create it on the fly\ndef initialise_index():\n    global index\n    if os.path.exists(os.environ[\"INDEX_FILE\"]):\n        index = GPTSimpleVectorIndex.load_from_disk(os.environ[\"INDEX_FILE\"])\n    else:\n        documents = SimpleDirectoryReader(os.environ[\"LOAD_DIR\"]).load_data()\n        index = GPTSimpleVectorIndex.from_documents(documents)\n\n# get path for GUI  \ngui_dir = os.path.join(os.path.dirname(__file__), 'gui')  \nif not os.path.exists(gui_dir): \n    gui_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'gui')\n\n# start server\nserver = Flask(__name__, static_folder=gui_dir, template_folder=gui_dir)\n\n# initialise index\ninitialise_index()\n\n@server.route('/')\ndef landing():\n    return render_template('index.html')\n\n@server.route('/query', methods=['POST'])\ndef query():\n    global index\n    data = request.json\n    response = index.query(data[\"input\"])\n    return jsonify({'query': data[\"input\"], \n                    'response': str(response), \n                    'source': response.get_formatted_sources()})\nОтвет возвращается в отображаемый пользовательский интерфейс.\nПользовательский интерфейс\nИспользуем довольно простенький \nindex.html\n<!DOCTYPE html>\n<html>\n<head lang=\"en\">\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n    <link rel=\"icon\" href=\"data:;base64,iVBORw0KGgo=\">\n    <link href=\"{{ url_for('static', filename='css/bootstrap.min.css') }}\" rel=\"stylesheet\">\n    <script src=\"{{ url_for('static', filename='js/bootstrap.bundle.min.js') }}\"></script>\n    <script src=\"{{ url_for('static', filename='js/jquery-3.6.4.min.js') }}\"></script>\n    <script src=\"{{ url_for('static', filename='kancil.js') }}\"></script>\n</head>\n<body>\n    <div class=\"container\">    \n        <div class=\"row pt-3\">\n            <div class=\"col-md-12\">                    \n            <label for=\"prompt\" class=\"form-label fs-3 text-secondary\">What would you like to know?</label>\n            </div>\n            <form>\n                <div class=\"col-md-12\">                    \n                    <div class=\"input-group mb-3\">\n                        <textarea class=\"form-control\" id=\"prompt\" rows=\"2\"></textarea>\n                        <button class=\"btn btn-outline-secondary\" type=\"button\" id=\"send\">ok</button>\n                    </div>\n                </div>\n            </form>    \n        </div>\n        \n        <div class=\"row pt-3\">\n            <div class=\"col-md-12\">\n                <div id=\"response\"></div>\n            </div>\n        </div>\n\n        <div class=\"row pt-3\">\n            <div class=\"col-md-12\">\n                <div id=\"source\"></div>\n            </div>\n        </div>        \n    </div>\n    \n</body>\n</html>\nВызов производится в файле kancil.js. Когда кнопка будет нажата, она отправит запрос на наш обработчик в файле server.py, а  ответ мы получим на  странице index.html  \n\n$(document).ready(function(){  \n    t = 0;\n    $('#send').click(function(e){\n        e.preventDefault();\n        var prompt = $(\"#prompt\").val().trimEnd();\n        console.log(prompt);\n        if(prompt == \"\"){\n            $(\"#response\").text(\"Please ask a question.\");\n        }\n        else{            \n            function myTimer() {\n                $(\"#response\").html(\"<p>Waiting for response ... \" + t + \"s</p>\");\n                t++;\n            }\n            const myInterval = setInterval(myTimer, 1000);          \n            $.ajax({\n                url: \"/query\",\n                method:\"POST\",\n                data: JSON.stringify({input: prompt}),\n                contentType:\"application/json; charset=utf-8\",\n                dataType:\"json\",\n                success: function(data){\n                    $(\"#response\").html(\"<p>\" + data.response + \"</p>\");                    \n                    $(\"#response\").append(\"<small class='text-secondary'>Responded in \" + t + \" seconds</small>\");\n                    $(\"#source\").html(\"<small class='text-secondary'>\" + data.source + \"</small>\");    \n                    clearInterval(myInterval);\n                    t = 0;\n                }\n              })   \n              \n        }\n    });     \n});  \nНу вот и готово!\nВы можете задаться вопросом, и все? Где же тут  тогда используется GPT-3? Ну, это делает LlamaIndex за кадром. Вы можете настроить его, но по умолчанию он использует API GPT-3 text-davinci-003. Вам необходимо поместить ключ OpenAI API в переменную среды (проще всего просто вставить его в файл), после этого LlamaIndex найдет его и использует для вызова API OpenAI.\nТак что да, вот и все.\nВы можете получить код из \nhttps://github.com/sausheong/kancil\nТехника подсказок\nКажется довольно невероятным, что вы можете буквально спроектировать подсказку таким образом, чтобы вы могли получить ответы от GPT-3 на основе ваших собственных данных. И вам даже не нужно тонко настраивать какие-либо модели. С LlamaIndex это выглядит еще более удивительно, потому что теперь вы можете подключаться к нескольким источникам данных и использовать GPT-3 всего с несколькими строками кода. Ни в коем случае этот метод не идеален, и результаты иногда странные, но все равно круто.\nEnjoy!\n \n ",
    "tags": [
        "flask",
        "chatgpt",
        "LlamaIndex"
    ]
}