{
    "article_id": "726460",
    "article_name": "Модульное глубокое обучение",
    "content": "В этом материале приведён краткий обзор использования модульного подхода в задачах глубокого обучения. Более детальный разбор этой темы вы можете найти \nздесь\n. Если вас интересует модульный подход к тонкой настройке (дообучению) моделей обработки естественного языка — взгляните на наше учебное руководство 2022 года по \nEMNLP\n. Дополнительные материалы по модульному глубокому обучению вы можете найти на \nэтом\n ресурсе.\nТематические исследования модульных подходов к глубокому обучению. Слева направо: фреймворк \nMAD-X\n, модель \nPolytropon\n, \nтрансформер\n, построенный по модели «смесь экспертов»\nПередовые модели глубокого обучения, развитие которых подстёгивают законы масштабирования, становятся всё больше и больше. Эти модели представляют собой монолитные структуры. Их, с момента, когда они ещё совершенно ничего не «знают», подвергают предварительному обучению, проводя его силами технических специалистов по тщательно продуманному плану. Тонкая настройка этих моделей, в силу их размера, превратилась в дорогое удовольствие, а её альтернативы, вроде контекстуального обучения, на практике часто дают неустойчивые результаты. Но, в то же время, эти модели всё ещё плохо показывают себя в решении многих задач — таких, как формирование суждений путём манипулирования символами, темпоральное понимание различных явлений, генерирование многоязычных текстов и т.п.\nМодульный подход способен помочь нам в решении некоторых из этих исключительно сложных задач. Разбивая модели на модули, мы можем отделить фундаментальные знания и способности к рассуждению о языке, о видимых объектах, и о прочем подобном, от возможностей, нужных для решения конкретных задач в различных предметных областях. Модульный подход, кроме того, даёт нам гибкий инструмент расширения моделей в расчёте на новые условия, даёт механизм дополнения их новыми возможностями.\nЗдесь я поделюсь с вами самыми важными наблюдениями и выводами, которые касаются различных аспектов темы модульного глубокого обучения.\nКлассификация\nМы производим категоризацию модульных подходов к глубокому обучению по четырём измерениям:\nВычислительные функции: как реализован модуль.\nФункции маршрутизации: как выбирают активные модули.\nАгрегирующие функции: как агрегируются выходные данные активного модуля.\nУсловия обучения: как обучают модули.\nНиже мы приведём результаты изучения различных конфигураций этих компонентов.\nТематические исследования модульных подходов к глубокому обучению. Зелёные компоненты показывают различные \nфункции маршрутизации\n. Компоненты, окрашенные в разные оттенки красно-лилового цвета — это \nвычислительные функции\n. (a) Фреймворк \nMAD-X\n использует слои адаптеров с фиксированной маршрутизацией для прямого межъязыкового переноса данных. Модель \nPolytropon\n использует малоранговые адаптеры с жёстко заданной маршрутизацией для адаптации к задаче обучения на малых наборах данных. \nТрансформеры\n, построенные по модели «смесь экспертов», используют многослойные персептроны с применением гибкой top-k-маршрутизации для масштабирования моделей до более крупных размеров.\nВычислительные функции\nМы представляем нейронную сеть \n в виде композиции функций \n, каждая из которых имеет собственный набор параметров \n. Функция может представлять собой слой или компонент слоя — такой, как линейное преобразование.\nМы выделяем три базовых типа вычислительных функций, которые «сшивают» модуль, имеющий параметры, с функциями модели:\nКомпозиция параметров. Модули модифицируют модель на уровне индивидуальных весов: \n.\nКомпозиция входов. Вход функции x конкатенируется с параметрами модуля: \n.\nКомпозиция функций. Выходы функции модели и модуля комбинируются: \n.\nМы приводим обзор трёх вычислительных функций (в дополнение к гиперсети) в виде части архитектуры трансформеров на следующем рисунке:\nРазличные вычислительные функции в архитектуре трансформера. Модульные компоненты, ориентированные на конкретную задачу, показаны пурпурным и красно-лиловым цветами. (a) Композиция параметров: разреженная подсеть в линейном слое, представляющая часть механизма множественного внимания. (b) Композиция входов: механизм \nнастройки префиксов\n расширяет входные данные путём присоединения эмбеддингов к матрицам ключей и значений в слое трансформера. © Композиция функций: в каждый слой добавлены ориентированные на конкретную задачу суживающие слои, которые трансформируют скрытое представление. (d) Гиперсеть: небольшая отдельная нейронная сеть, генерирующая параметры модуля, зависящие от метаданных.\nКомпозиция параметров\nМы выделяем два основных способа использования модулей для изменения параметров модели:\nКорректировка разреженного подмножества параметров.\nКорректировка параметров в низкоразмерном пространстве.\n«Разреженные» методы близко связаны с сокращением избыточных нейронов в скрытых слоях искусственной нейронной сети и с \nгипотезой лотерейного билета\n. Такие методы могут быть структурированы и могут применяться лишь к особым группам параметров.\nКомпозиция входов\nОбучение на основе подсказок можно рассматривать как поиск текстовой подсказки, ориентированной на конкретную задачу, эмбеддинг которой вызывает желаемое поведение системы. Альтернативой этому является непосредственное обучение на основе непрерывных подсказок — на входе модели или в каждом её слое.\nКомпозиция функций\nЭто — наиболее общая категория вычислительных функций. Она вбирает в себя стандартные многозадачные методы обучения, модули, которые адаптируют к чему-либо заранее обученные модели (известные как «адаптеры), методы изменения масштаба моделей. Кроме того, методы композиции параметров и входов можно выразить в виде композиции функций. Ниже, в качестве примера, показаны три метода композиции функций.\nРазличные подходы к композиции функций. (a) \nПоследовательный сужающий адаптер\n: первая архитектура адаптера, предложенная для применения с трансформерами, состоит из двух суживающих слоёв, размещённых после слоя множественного внимания и слоя прямого распространения. (b) \nПараллельный сужающий адаптер\n: в предварительно обученном трансформере для слоя внутреннего внимания и слоя прямого распространения суживающие преобразования выполняются параллельно. © \n(IA)\n3\n: операции изменения масштаба выполняются внутри слоёв внутреннего внимания и прямого распространения.\nГенерирование параметров модуля\nВместо того чтобы формировать параметры модуля непосредственно, в ходе обучения модели, их можно сгенерировать с помощью вспомогательной модели (гиперсети), зависящей от дополнительной информации и от метаданных.\nНиже приведена краткая сводка по сильным и слабым сторонам различных вычислительных функций. Подробности об этом ищите \nздесь\n и \nздесь\n.\nСравнение различных вычислительных функций\nЭффективность параметров\nЭффективность обучения\nЭффективность выводов\nПроизводительность\nПригодность для композиции\nКомпозиция параметров\n+\n-\n++\n+\n+\nКомпозиция входов\n++\n--\n--\n-\n+\nКомпозиция функций\n-\n+\n-\n++\n+\nФункции маршрутизации\nФункция маршрутизации \n определяет то, какие модули будут активными, основываясь на заданных входных данных. Выбор модулей осуществляется путём назначения всем модулям из списка \n баллов \n. Ниже будет приведён обзор различных методов маршрутизации.\nРазличные методы маршрутизации. (a) Фиксированная маршрутизация: образцы передаются модулю на основе заранее заданной логики. (b) Маршрутизация на основе обучения (жёсткая): правила маршрутизации вырабатываются в ходе обучения и не меняются. © Маршрутизация на основе обучения (гибкая): гибкий выбор модулей с использованием весов.\nФункция маршрутизации может быть фиксированной. Тогда все решения, связанные с маршрутизацией, принимаются на основе того, что заранее известно о задаче. Но возможен и альтернативный подход — маршрутизация на основе обучения модели. Методы маршрутизации на основе обучения отличаются тем, насколько точно задан выбор маршрута. При жёсткой маршрутизации модель учится делать точный, неизменный выбор конкретного маршрута. При гибкой маршрутизации выбор зависит от распределения вероятностей, связанных с модулями.\nФиксированная маршрутизация\nПри фиксированной маршрутизации используются метаданные — такие, как описание особенностей задачи. Это позволяет принимать определённые решения о маршрутизации до обучения модели. Фиксированная маршрутизация используется в большинстве методов композиции функций. В частности, речь идёт о многозадачном обучении и о применении адаптеров. При применении фиксированной маршрутизации осуществляется выбор разных модулей для различных аспектов целевого окружения. Например — это может быть задача и язык в обработке естественных языков, или робот и задача при обучении с подкреплением. Это позволяет обобщать модели на неизвестные сценарии.\nМаршрутизация на основе обучения\nМаршрутизация на основе обучения обычно реализуется на базе многослойных персептронов. Этот подход к маршрутизации предусматривает необходимость борьбы с дополнительными сложностями. Среди них — нестабильность обучения, коллапс модуля, переобучение модели. Существующие методы маршрутизации на основе обучения часто не отличаются оптимальностью. Дело в том, что они представлены модулями, которые, во-первых, не являются достаточно специализированными, и во вторых — недостаточно интенсивно используются. Но, когда между задачей и соответствующей возможностью системы нет чёткой однозначной связи, маршрутизация на основе обучения — это единственный подходящий вариант.\nЖёсткая маршрутизация на основе обучения\nЖёсткая маршрутизация на основе обучения моделирует выбор того, будет ли модуль активным, основываясь на бинарной логике. Так как модель невозможно научить принятию чётких решений напрямую, пользуясь методом градиентного спуска, системы учат, пользуясь обучением с подкреплением, эволюционными алгоритмами или стохастической репараметризацией.\nГибкая маршрутизация на основе обучения\nМетоды гибкой маршрутизации, основанной на обучении, обходят необходимость точного выбора модуля. Они действуют через изучение взвешенной комбинации показателей в форме распределения вероятностей между доступными модулями. Классический пример этого — модель «смесь экспертов». Так как активация всех модулей — это ресурсозатратная операция, свежие методы гибкой маршрутизации на основе обучения ориентируются на выбор top-\n и даже top-1 модулей. Маршрутизация на уровне токенов ведёт к более эффективному обучению, но ограничивает выразительные возможности модульных представлений.\nУровень маршрутизации\nСистема маршрутизации может выбирать модули на глобальном уровне, воздействуя на всю сеть, может выполнять распределение задач для каждого конкретного слоя, или даже принимать иерархические решения о маршрутизации. Ниже показаны схемы различных уровней маршрутизации.\nРазличные уровни маршрутизации. (a) Послойная маршрутизация: индексы выбираются на основании входа текущего слоя. (b) Примитивная глобальная маршрутизация: для всех слоёв модели выбираются одни и те же индексы модулей. © Глобальная маршрутизация: конфигурация (возможно — особая для каждого из слоёв) выбирается на глобальном уровне.\nАгрегирующие функции\nАгрегирующая функция определяет то, как комбинируются выходы модулей, выбранных посредством механизма маршрутизации. На практике системы маршрутизации и агрегации часто объединяют. Агрегирующие функции могут быть классифицированы аналогично вышеописанным вычислительным функциям. Но вычислительные функции соединяют соответствующие части моделей с компонентами модулей, а агрегирующие функции отвечают за объединение множества компонентов модуля на разных уровнях:\nАгрегация параметров. Агрегируются параметры модулей: \nАгрегация представлений. Агрегируются модульные представления: \nАгрегация входов. Параметры модуля конкатенируются на уровне входов: \nАгрегация функций. Агрегируются модульные функции: \nАгрегация параметров\nАгрегирование информации из множества модулей путём интерполяции их весов тесно связано с линейной связностью решений. Это показывает, что при определённых условиях, таких, как одни и те же инициализационные значения, две сети связаны линейным путём невозрастающей ошибки. Основываясь на этом предположении, в модели, с использованием арифметических операций, могут быть проведены модульные изменения. Делается это для того чтобы убрать или извлечь какую-либо информацию из модели.\nАгрегация представлений\nВ качестве альтернативы — выходы различных модулей можно интерполировать путём агрегации скрытых представлений модулей. Один из способов выполнения такой агрегации заключается в изучении системой взвешенной суммы представлений. Это похоже на то, как система маршрутизации учится назначать балл i каждому из модулей. Модель, кроме того, может изучить взвешенные данные, которые учитывают скрытые представления. Например — посредством механизма внутреннего внимания.\nАгрегация входов\nЕсли модели предоставлена подсказка, то передача ей нескольких инструкций или нескольких образцов, выполняемая путём конкатенации, может рассматриваться как форма агрегации входных данных. «Мягкие» подсказки можно изучать в различных ситуациях. Например — в таких, как работа с задачами и языком, или с атрибутами и объектами. Их агрегация выполняется путём конкатенации.\nАгрегация функций\nИ наконец — мы можем агрегировать модули на уровне функций, изменяя порядок выполнения вычислений. Можно агрегировать их последовательно, когда выход одного модуля становится входом другого, после чего это повторяется для следующих модулей. Для создания более сложных конфигураций модулей можно агрегировать их иерархически, основываясь на древовидной структуре.\nУсловия обучения\nПоследнее измерение из тех, по которым мы можем дифференцировать модульные методы глубокого обучения — это то, как обучаются модели. Мы выделяем три стратегии модульного обучения:\nСовместное обучение.\nНепрерывное обучение.\nРетроспективная адаптация.\nСовместное обучение\nВ многозадачной системе обучения модульные компоненты, ориентированные на конкретную задачу, обучают совместно для смягчения катастрофических взаимных помех, используя фиксированную маршрутизацию или обучаемую систему маршрутизации. Совместное обучение, кроме того, может дать полезные инициализационные данные для модульных параметров, может позволить использовать дополнительные модульные компоненты по мере развития модели.\nНепрерывное обучение\nВ ходе непрерывного обучения в модель, со временем, добавляют новые модули. Параметры предыдущих модулей обычно «замораживают», а новые модули подключают к существующим различными способами.\nРетроспективная адаптация\nМетоды этой группы ещё известны как «тонкая настройка моделей с эффективным использованием параметров», так как они обычно используются для адаптации больших, заранее обученных моделей, к целевой ситуации. Мы рассматриваем подобные методы для задач обработки естественного языка в \nэтом\n учебном руководстве.\nЦели применения модульного подхода в глубоком обучении\nМногие из вышеописанных методов оцениваются на основе их способности масштабировать большие модели, или на основе того, как они поддерживают перенос знаний, полученных в одной области, на другую область с применением небольшого количества образцов. Модульный подход, кроме того, чрезвычайно важен и в других сферах, в том числе — в задачах планирования и систематического обобщения, в число которых входят:\nИерархическое обучение с подкреплением.\nКонструирование нейропрограмм.\nПричинно-следственный анализ.\nНиже приведено схематическое изображение этих задач.\nРазличные цели модульного глубокого обучения. (a) Иерархическое обучение с подкреплением: \nскетчи политик\n состоят из высокоуровневых политик (инструкций, относящихся к задачам), которые отвечают за выбор низкоуровневых политик (опций), определяющих выбор действия. (b) Конструирование программ: \nдифференцируемый нейрокомпьютер\n использует рекуррентный нейроконтроллер, который итеративно получает входные данные из окружающей среды, записывает данные в память и читает их из неё, генерируя, в итоге, выходные данные. © Причинно-следственный анализ: \nпричинно-независимые механизмы\n маршрутизируют трансформированный образец к эксперту, который накладывает его на исходное распределение. Состязательный дискриминатор пытается различить реконструированный и исходный образцы.\nИерархическое обучение с подкреплением\nДля того чтобы организовать обучение модели, проводимое через длительные промежутки времени, или проводимое при использовании сильно разреженных и отложенных наград в обучении с подкреплением, часто полезно обучить модель промежуточным абстракциям, известным, как опции или навыки, в форме переносимых подполитик. С изучением подполитик связаны сложности, имеющие отношение к специализации и мониторингу, а так же — к пространству действий и опций. Среди стратегий, используемых для борьбы с этими сложностями, можно отметить применение внутренних наград, подцелей, а так же — языка в качестве промежуточного пространства.\nСимуляция программ\nМодульный подход, кроме того, можно использовать для симуляции программ. Делается это путём динамического конструирования графа программы на основе входных данных, или путём глобального конструирования графа на основе описания задачи. В дополнение к системам маршрутизации и к вычислительным функциям, подобные архитектуры могут быть расширены за счёт внешней памяти. Симуляция программ полезна в тех случаях, когда решение задачи основано на решении подзадач в правильной последовательности.\nПричинно-следственный анализ\nМодульный подход в методах причинно-следственного анализа отражает модульность в (физических) механизмах мира. Считается, что модули независимы и подходят для многократного использования, поэтому модели машинного обучения, копирующие эту структуру, отличаются повышенной устойчивостью к вмешательствам и к сдвигам локального распределения. Среди сложностей, характерных для применения модулей в этой сфере, можно отметить специализацию каждого модуля на конкретном механизме, а так же — совместное изучение абстрактных представлений и их взаимодействий в графе причинно-следственных связей.\nПрименение модульного подхода в трансферном обучении\nПредставленные методы используются для решения самых разных задач. Сначала мы расскажем об их типичных применениях в сфере обработки естественного языка, а потом проведём аналогии, касающиеся их использования в задачах обработки речи, в задачах компьютерного зрения, а так же — в других сферах машинного обучения.\nМашинный перевод\nВ сфере машинного перевода применялись билингвальные адаптеры. Они позволяли адаптировать большие многоязычные нейросетевые модели к конкретному направлению перевода между двумя языками. Этот подход был расширен, что привело к появлению более эффективных монолингвальных адаптеров. Для обеспечения положительного переноса между языками использовались гиперсети. Здесь применялись и другие подходы — такие, как подсети, ориентированные на конкретный язык или на конкретную предметную область. Применяются здесь и модели типа «смесь экспертов».\nМежъязыковой перенос\nЯзыковые модели комбинируют с модулями задач для того чтобы обеспечить перенос, с языка источника на целевой язык, больших моделей, подвергшихся тонкой настройке в расчёте на конкретную задачу. В рамках этого подхода предложено множество вариантов систем. Это, например, обучение адаптеров для языковых пар или для семейств языков, обучение подсетей, ориентированных на языки и задачи, использование гиперсетей для генерирования различных компонентов.\nАдаптация к предметной области\nМодульные системы, представляющие конкретную предметную область, обучают, используя адаптеры или подсети. Обычно для этого задействуют набор разделяемых модулей и модулей предметной области, которые обучают совместно, используя дополнительную регуляризацию или члены, описывающие потери, в параметрах модулей.\nВнедрение знаний\nМодули, кроме прочего, можно использовать для хранения внешних знаний и их внедрения в модели. Этот подход можно скомбинировать со знаниями о языке, о предметной области или о задаче. Тут обычно пользуются такой стратегией: обучают модели на синтетических данных, созданных на основе информации, имеющейся в базе знаний.\nОбработка речи\nВ сфере машинной обработки речи проводились эксперименты с теми же методами, что и в сфере обработки естественного языка. Главные отличия заключаются в том, что базовая модель, применяемая при обработке речи — это обычно разновидность wav2vec, и в том, что модульные представления оптимизировались с учётом цели нейросетевой темпоральной классификации. Модульный подход здесь применяется чаще всего в форме обучения адаптеров для автоматического распознавания речи.\nКомпьютерное зрение и кроссмодальное обучение\nВ сфере компьютерного зрения наиболее часто применяются модули, представляющие собой адаптеры и подсети, основанные на моделях ResNet или Vision Transformer. В мультимодальном обучении и задачи, и информация о модальности захватываются в различных модулях для разных приложений. Например, недавно появившаяся модель \nFlamingo\n использует «замороженные» предварительно обученные зрительную и языковую модели. В её рамках обучают новые адаптерные слои для того чтобы регулировать языковые представления с помощью визуальных данных, поступающих на вход модели.\nО дальнейших исследованиях\nДальнейшие исследования сферы модульного глубокого обучения включают в себя, кроме прочих, следующие направления:\nКомбинирование различных вычислительных функций.\nУглубление понимания природы и различий разных модульных представлений.\nИнтеграция систем обучаемой маршрутизации в предварительно обученные модели.\nИзмерение эффективности работы различных методов маршрутизации.\nНепосредственное создание информации, хранящейся в подсетях.\nРазработка обучаемых методов агрегирования.\nСоздание расширяемых модульных многозадачных моделей.\nИтоги\nМы представили систему классификации модульных инструментов глубокого обучения по четырём основным измерениям. Принимая во внимание тенденцию к предварительному обучению всё больших и больших моделей, мы считаем, что модульный подход будет иметь исключительно важное значение в подобных проектах. Он позволит наладить более стабильную разработку моделей путём разбиения их на модули. Развитие модульного подхода, кроме того, приведёт к созданию таких методов работы, которые будут направлены на устранение существующих ограничений, и таких, которые можно будет использовать в различных нейросетевых архитектурах. Надо сказать и о том, что модульный подход может способствовать сдвигам в среде разработчиков моделей. Сейчас разработка моделей сконцентрирована в небольшом количестве крупных организаций, а может случиться так, что созданием модульных компонентов будет заниматься множество небольших компаний и индивидуальных разработчиков.\nО, а приходите к нам работать? 🤗 💰\nМы в \nwunderfund.io\n занимаемся \nвысокочастотной алготорговлей\n с 2014 года. Высокочастотная торговля — это непрерывное соревнование лучших программистов и математиков всего мира. Присоединившись к нам, вы станете частью этой увлекательной схватки.\nМы предлагаем интересные и сложные задачи по анализу данных и low latency разработке для увлеченных исследователей и программистов. Гибкий график и никакой бюрократии, решения быстро принимаются и воплощаются в жизнь.\nСейчас мы ищем плюсовиков, питонистов, дата-инженеров и мл-рисерчеров.\nПрисоединяйтесь к нашей команде\n.\n \n ",
    "tags": [
        "искусственный интеллект",
        "глубокое обучение"
    ]
}