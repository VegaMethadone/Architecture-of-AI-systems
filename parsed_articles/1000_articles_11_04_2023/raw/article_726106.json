{
    "article_id": "726106",
    "article_name": "Нейросетевое генеративное искусство: как программисту стать художником",
    "content": "Наверняка на Хабре есть люди, уже глубоко изучившие генерацию картинок с помощью нейросетей. Но много и тех, кто ещё не разбирался, почему у Stable Diffusion в названии есть слово «диффузия» и чем разновидности нейросетей различаются.\nДля тех, кто не готов забираться в глубокие дебри, но хочет в один присест наверстать всё главное, может быть полезен доклад \nДмитрия Сошникова\n \n@shwars\n с нашего мероприятия \nTechTrain\n. Поэтому мы сделали для Хабра текстовую расшифровку (видеозапись также \nприлагаем\n).\nДокладу всего полгода, но за это время нейросети успели развиться ещё сильнее. Так что, если тема вам интересна, заодно порекомендуем и наш следующий \nTechTrain\n, который пройдёт уже завтра (1 апреля): там будет целый ряд докладов про AI, в том числе \nновый\n от Дмитрия. Участие бесплатно.\nДалее повествование идёт от лица Дмитрия.\nМагия программирования и генеративное искусство  \nЯ очень люблю компьютерное искусство, это новый способ выразить себя. Когда смотришь на картины, думаешь: «Хорошо людям — они могут своё воображение представлять на холсте. Мне бы тоже так». И оказывается, что, будучи программистом, тоже можно рисовать.\nТо, что вы видите сейчас, нарисовано нейросетью. А всего лишь год назад было сложно себе представить, что внутри компьютера в результате процесса умножения больших матриц будут получаться рисунки в ответ на текстовый запрос. Это ощущается как магия. Как говорил фантаст Артур Кларк: «Любая достаточно продвинутая технология неотличима от магии». \nИ магия сопровождает программирование на каждом шаге. Можно написать простую программу, которая сможет генерировать сколь угодно сложные объекты:\nЭто так называемая снежинка Коха: фрактал, который потенциально может бесконечно детализироваться (достаточно указать, какой уровень детализации мы хотим). Магия программирования в том, что небольшие конечные объекты способны генерировать потенциально бесконечную сложность.\nНа этом основаны художественные приёмы. Есть целое направление компьютерного генеративного искусства, где стандартом является язык Processing — модифицированная версия Java с рисовательными примитивами. Могу рекомендовать книгу «\nGenerative Art: A Practical Guide Using Processing\n» Мэтта Пирсона. \nС этим связан интересный сайт \nopenprocessing.org\n, где собраны работы разных людей. Давайте посмотрим на \nодну\n из них. На первый взгляд, здесь рисуются случайные кривые, но потом они складываются в фотографию:\nНа OpenProcessing можно заглянуть в код любой работы, можно сделать свой форк. В этой работе код — всего лишь два небольших файла на Processing.\nОдин из них выглядит так\nvar imgs = [];\nvar imgIndex = -1;\nvar img;\nvar paint;\nvar subStep = 800;\nvar z = 0;\nvar isStop = false;\nvar count = 0;\n\nfunction preload() {\n  imgs[0] = loadImage(\"test1.png\");\n  imgs[1] = loadImage(\"test2.png\");\n  imgs[2] = loadImage(\"test3.png\");\n}\n\nfunction setup() {\n\tif(windowWidth < 600)\n  \tcreateCanvas(windowWidth, windowWidth);\n\telse \n  \tcreateCanvas(600, 600);\n  img = createImage(width, height);\n  nextImage();\n  paint = new Paint(createVector(width/2, height/2));\n  background(255, 255, 255);\n  colorMode(RGB, 255, 255, 255, 255);\n}\n\nfunction draw() {\n  //console.log(frameRate());\n  if (!isStop) {\n  \tfor (var i = 0 ; i < subStep ; i++) {\n      paint.update();\n      paint.show();\n      z+= 0.01;\n    }\n  }\n\tcount++;\n\tif (count > width) {\n\t\tisStop = true;\n\t}\n\t//background(255);\n\t//image(img, 0, 0, width, height);\n}\n\nfunction fget(i, j) {\n  var index = j * img.width + i;\n  index *= 4;\n  return color(img.pixels[index], img.pixels[index+1], img.pixels[index+2], img.pixels[index+3]);\n}\n\nfunction fset(i, j, c) {\n  var index = j * img.width + i;\n  index *= 4;\n  img.pixels[index] = red(c);\n  img.pixels[index+1] = green(c);\n  img.pixels[index+2] = blue(c);\n  img.pixels[index+3] = alpha(c);\n}\n\nfunction keyPressed() {\n  console.log(key);\n  if (key === 's' || key === 'S') {\n    isStop = !isStop;\n  } \n}\nfunction mouseClicked() {\n  nextImage();\n\tisStop = false;\n\tcount = 0;\n}\nfunction touchStarted() {\n  nextImage();\n\tisStop = false;\n\tcount = 0;\n}\n\nfunction nextImage() {\n\tif (!img) return;\n  imgIndex = (++imgIndex) % imgs.length;\n  var targetImg = imgs[imgIndex];\n  img.copy(targetImg, 0, 0, targetImg.width, targetImg.height, 0, 0, img.width, img.height);\n  //img.resize(width, height);\n  img.loadPixels();\n  clear();\n}\n\nТут мы пишем код, который делает что-то красивое, и человек целиком выступает в роли автора, а компьютер — просто исполнитель. \nНо существуют также искусственный интеллект и машинное обучение. Можно «накормить» компьютер картинами, например, из сборника художественных работ WikiArt, и пусть он сам научится рисовать. \nИскусственный интеллект можно использовать в генеративном искусстве двумя способами. Первый подход — просто использовать возможности искусственного интеллекта, чтобы реализовать идею человека. Второй — натренировав искусственный интеллект на картинах, начать воспринимать его как соавтора.\nПример первого подхода — когнитивный портрет. Мы берем фотографии людей и накладываем их друг на друга автоматически таким образом, что определенные части лица совпадают:\nГлаза и рот — опорные точки лица. Мы хотим сделать так, чтобы они совпали, и получился бы усредненный портрет. О том, как это сделать, я ранее \nрассказывал\n своём блоге.\nЕщё я сделал картину, которая называется «Взросление»:\nЯ взял фотоархив, с помощью автоматического распознавания лиц вычленил фотографии своей дочери и разложил их по возрасту в пять разных кучек. И склеил так, что детская фотография перетекает в более взрослую. О «Взрослении» у меня тоже есть отдельный \nпост\n. А ещё есть \nвидео взросления\n.\nКак научить ИИ рисовать самостоятельно?  \nЧтобы ответить на этот вопрос, сначала нужно разобраться, как работают нейросети. \nНейросеть — подобие того, как работает наш мозг. У неё есть искусственные нейроны, которые суммируют сигналы со своих входов и передают на выход. На следующем уровне сигнал из предыдущих слоев нейронов опять суммируется и передается на выход. В итоге получается нужный нам результат.\nНапример, если я хочу научиться отличать кошку от собаки на фотографии, то мне нужно на вход подать изображение этой кошки. Каждый пиксель на фотографии будет входом в нейронную сеть. Если у меня есть картинка размером 100 на 100 пикселей, то это значит, что будет 10 000 входных нейронов. Будет два выхода — «кошка» или «собака». Если это кошка, то выход должен быть 1-0, а если это собака, то 0-1. Мы будем показывать нейросети много картинок и ожидать, что она научится распознавать их правильно. \nЗдесь есть проблема. В реальной жизни кошка может оказаться не в середине, а в углу фотографии. Поскольку каждый пиксель жестко привязан к какому-то нейрону, то оказывается, что идею кошки такой нейросети очень сложно ухватить. Ей нужно подстраиваться под возможность наличия кошки в каждом месте изображения. Но когда человек смотрит на картинку — он сканирует ее и ищет типичные признаки. Нужно, чтобы нейросети действовали примерно так же.\nСвёрточные сети  \nЕсть архитектура «свёрточные сети» (convolutional neural networks, CNN). Они устроены таким образом: сеть «бежит» по изображению фильтром — матрицей, которую мы умножаем на локальную окрестность.\nЕсли мы возьмем такую матрицу, где один столбец яркий, другой столбец — наоборот, тёмный, и мы будем умножать это на картинку, то все вертикальные штрихи на картинке будут усиливаться. Это устроено так: если есть какая-то граница, то одна её часть умножается на что-то большое, вторая — на что-то маленькое. И получается большой отклик. \nА если мы, например, возьмем горизонтальную матрицу, то будут фиксироваться горизонтальные штрихи. Возьмем наклонную матрицу — будут фиксироваться наклонные. На основе этого и работают архитектуры нейронных сетей.\nТак же устроено зрение простейших насекомых. Биологи обнаружили, что в их глазах есть фильтры, которые выглядят, как разные наклонные объекты, из которых формируется первый уровень понимания действительности.\nИз этих штрихов мы можем комбинировать более сложные объекты, например, глаза. А на следующем уровне комбинировать из этих кусочков сами лица. Каждый уровень нейронной сети может обнаруживать какие-то паттерны. \nПрименив это иерархически, мы учимся выхватывать из изображения нужные нам вещи и узнавать объекты.\nА как генерировать? Автоэнкодер\nМы подавали на вход изображение, применяли иерархическое вычленение признаков и получали на выходе некоторый вектор чисел. Этот вектор описывает всё, что было в фотографии. И поверх него мы можем обучить классификатор, который будет говорить нам, кошка это или собака, картина или фотография. \nНо пока что мы ничего не генерируем. А как же обучить нейросеть генерировать изображения? \nПростейшая идея — сделать сеть, которая называется «автоэнкодер». Мы подаём ей на вход изображение и на выходе просим восстановить такое же: \nПо сути, нейросеть сталкивается с задачей сжатия информации: сначала ей надо сжать изображение в вектор, а затем из него восстановить исходное изображение. Для этого ей нужно внутри признаков в свёртке запоминать какие-то паттерны. \nКак это происходит? Представьте, что вам нужно воссоздать картину по минимуму информации. Например, вы с другом договорились: он смотрит на картину, говорит вам какое-то количество информации, а вы потом должны эту картину нарисовать. Если он говорит одно слово («портрет»), то вы нарисуете что-то не очень похожее на оригинал. Если скажет «портрет женщины», вы нарисуете чуть более похожую вещь. А по описанию «портрет женщины с белыми волосами, сидящей за столом и держащей перед собой компьютер» вы нарисуете что-то, ещё более похожее на оригинал — руководствуясь своим пониманием того, что такое женщина, компьютер и белые волосы.\nКогда тренируют автоэнкодер, ожидают, что нейросеть тоже научится запоминать эти вещи внутри себя. Она будет называть их словами: это будут паттерны, которые она будет выдавать в ответ на определенные значения вектора. Такой автоэнкодер достаточно часто используется на практике, но в чистом виде для генерации он работает не очень хорошо. Как выглядела бы такая генерация: мы бы отрезали начало и использовали только вторую часть свёртки. На вход мы бы подавали случайные вектора и смотрели бы, что получается. \nНужно очень сильно постараться, чтобы подать автоэнкодеру на вход вектор, генерирующий что-то красивое. Обычно получается случайное сочетание образов, далёкое от реальности, как сон — настолько странное, что в этом сложно увидеть что-то «хорошее». \nРешаем проблему случайного вектора\nСуществует более интересная архитектура — генеративно-состязательные сети (generative adversarial networks, GAN). У них тоже есть генератор, очень похожий на вторую часть автоэнкодера: мы подаем на вход случайный вектор, а на выходе генерируется изображение.\nНо есть и другая сеть, дискриминатор. Ей мы подаём на вход не только сгенерированное изображение, но и настоящее. Задача дискриминатора — суметь отличить сгенерированное.\nВ результате две сети учатся совместно. В самом начале генератор создаёт мусор, и дискриминатору очень просто: он легко отличает мусор от настоящего изображения. \nДальше мы говорим: «Раз дискриминатор определил, что вот эта картина — хорошая, давай дальше обучаться на ней». Нейросети ведь обучаются алгоритмом обратного распространения. Тогда генератор начинает создавать что-то более похожее на то, что нам нужно, и улучшается. Он улучшается настолько, что дискриминатор перестает отличать одно от другого. Значит, теперь нам нужно дообучать уже дискриминатор. И мы учим их по очереди, пока не получится так, что генератор научается создавать что-то действительно подходящее. \nВроде бы это хороший способ, но на практике работает не всегда. Вот пример того, что нейросеть считает хорошими картинами, хотя человек назовёт хорошим пейзажем только то, что справа:\nПоэтому, чтобы получить какой-то интересный результат, нам обязательно нужно участие человека. \nИскусство или нет?  \nЕсли, мы обучили нейросеть, и она способна по любому случайному вектору генерировать какую-то картину, считать ли это искусством? Можно подискутировать в комментариях, но пока что вброшу такую метрику: был прецедент, когда картина, сгенерированная искусственным интеллектом, была продана на аукционе за 432 с половиной тысячи долларов. \nГруппа энтузиастов взяла код и смогла \nпродать\n результат. Исходный автор кода ничего не получил, и разыгралась не очень красивая история. Но нам важно другое — есть прецедент. Раз за это готовы платить как за искусство, значит, наверное, это искусство? \nКак научить рисовать ребенка\nКогда ребенок учится рисовать, он смотрит на реальный мир, учится воспроизводить что-то похожее. Потом ему показывают разные картины художников — он смотрит на технику, учится воспроизводить отдельные элементы… И нейросеть делает похоже:   \nКонечно, она делает чуть хуже. Но она «понимает», что на картине есть характерный для холста узор, какие элементы нужны для изображения человека (например, глаза), какова примерная композиция портрета.\n \nМожно в таймлапсе \nпосмотреть\n, как такая нейросеть училась — процесс занял 11 часов. Её просят что-нибудь сгенерировать, и в начале получается не очень хорошо, но  чем дальше она учится — тем получается что-то более похожее на изображение. Справа — цветы, слева — портрет.\nДля портрета нейросети очень сложно нащупать правильное размещение всех фрагментов — например, то, что у человека два глаза, и они обязательно на голове. Это неочевидные факты, которые она долго-долго пытается понять, но в конечном итоге, пытаясь нарисовать какое-то лицо с тремя глазами, она все-таки понимает, что, наверное, должно быть два глаза, и учится этот паттерн отличать. \nНазываем то, что нарисовали  \nЧеловек, помимо чисто визуальных паттернов, знает языковые паттерны. Он называет портрет портретом. Поэтому, чтобы хорошо рисовать, нам важно скрестить возможности по пониманию текста и созданию изображения.\nДля естественного языка тоже есть нейросети. Это направление стало очень бурно развиваться в 2018-2019 годах, стали обучать генеративные модели. Сначала использовали так называемые рекуррентные сети, которые предсказывают по нескольким предыдущим словам следующее.\nНапример, мы говорим: «Я люблю играть в…». Эти слова известны, а нужно предсказать, какое слово будет следующим. Например футбол — с вероятностью 0,34, шашки — уже 0,12, а огурец — 0. Потому что играть в огурец не принято. \nМы даем нейросети большие массивы текстов, и получаем авторегрессионные модели, которые предсказывают следующее слово.\nНесколько лет назад в Microsoft мы делали небольшую выставку в рамках одной из конференций, где показывали, как нейросеть учится рисовать картины и генерировать текст. Обучали по буквам генерировать сказки. В начале нейросеть не очень понимает, что нужно, дальше она начинает видеть какие-то паттерны, слоги, из них складывать слова, и в конечном итоге получается уже более осмысленный текст:  \nЭта сеть обучалась несколько часов на персональном компьютере. Но если потратить несколько месяцев и миллионы или миллиарды долларов, то удаётся обучить существенно более серьезные модели. Модель под названием GPT-3 оказалась способна генерировать очень разумный текст. В примере белый текст — то, что написал человек. \nДальше мы просим нейросеть сказать, а что же могли обнаружить ученые. И сеть пишет текст, по стилистике типичный для газетной статьи, с цитатами и статистикой.\nА если бы мы в начале делали затравку, например в виде стихов, то на выходе мы бы получали тоже стихи. Вот пример, где использовалась модель Yandex YaLM:\nЗдесь желтое — то, что человек написал или вручную подправил. Остальное — то, что способна сгенерировать нейросеть. В таком стихе есть даже какой-то смысл. Хотя он пока что не выходит за рамки того, что мог бы написать десятилетний ребенок. \nУдивительно, что всё происходит путем умножения чисел внутри компьютера. Компьютер, безусловно, не понимает, что он делает, и не испытывает по этому поводу эмоций. Он просто умножает числа, эти числа потом складываются в слова, и на выходе получается что-то осмысленное. В этом есть магия и чудо. \nВ 2020 году я на «ЛитРес:Самиздате» выпустил \nкнижку\n с короткими рассказами, написанными совместно с нейросетью. Она называется «Жизнеописание Сергея Сергеевича в рассказах, правдивых и не очень». Также недавно в продажу вышла книга “\nНейро Пепперштейн\n”, написанная сетью, которую обучили стилю конкретного писателя. В ней — рассказы и самого Павла Пепперштейна, и написанные нейросетью. \nМультимодальные модели\nПонимание текста — одна возможность, генерировать изображение — другая. Интереснее всего их совмещать. Получаются мультимодальные модели: мы хотим научить компьютер тоже называть картинки словами. \nСущественным достижением был выпуск модели под названием CLIP от OpenAI. Она обучалась на парах «картинка и подпись к этой картинке».\nНапример, ей дают картинку собачки, и пишут, что это щенок. И дальше эта модель обучается. Сначала с помощью нейросети мы кодируем текст в некоторый вектор для каждого из входных фрагментов. Затем кодируем картинку в вектор с помощью энкодера. \nДальше мы подстраиваем нейросеть таким образом, чтобы для картинки, которая соответствует подписи, расстояние междувекторами было минимальным. А для неправильных картинок — тех, которые сопоставлены с неправильной подписью — наоборот максимальным. \nТак мы обучаем энкодер и декодер генерировать векторы, которые для одинаковых картинок и одинаковых подписей имеют близкое расстояние между собой. По расстоянию между векторами мы можем сравнивать, насколько картинка соответствует текстовому описанию. Это и есть то, что делает CLIP: ей можно дать текст и картинку, и на выходе понять, насколько они между собой близки.\nЭто открыло возможности для генеративных моделей нового уровня. Архитектура VQGAN + CLIP, оказавшаяся популярной год назад, использует некоторую генеративную сеть, похожую на автоэнкодер (его вторую часть). \nЗадача этой сети — подобрать правильный входной вектор, чтобы картинка была осмысленной. Мы подаем на вход случайный вектор, на выходе получается непонятно что. Этот результат мы подаем на вход CLIP’у и сравниваем с текстовым описанием.\nНапример, мы хотим получить картинку «Мальчик с пингвином». CLIP говорит: «это не очень похоже». Этот результат дальше используем для оптимизации — отправляем его обратно в VQGAN,  чтобы изменить вектор на более подходящий.\nМы действуем итерационно, подгоняя входной вектор, чтобы картинка всё больше и больше подходила на текстовое описание.\nЭто дает хорошие результаты. Например, по запросу «импрессионистская картина молодой женщины» — получаются вот такие вот картины. \nМы можем использовать разные генераторы, а CLIP выступает в роли дискриминатора, который смотрит, насколько картинка похожа на то, что нам нужно.\nГод назад ко дню учителя я делал коллекцию учителей, сгенерированных с помощью VQGAN + CLIP.  Получались такие картины. \nОни говорят о том, что нейросеть «понимает», чем учитель математики отличается от учителя географии. \nДругой проект: мы использовали нейросеть для того, чтобы получить описание существующей картины, и по этому описанию сгенерировать что-то другое. Это как «испорченный телефон»: один человек смотрит на картину, пытается ее описать, а другой рисует по описанию.\n \nОтносительно недавно OpenAI представила свою модель под названием DALL-E 2, а Google — свою Imagen. И они могут генерировать довольно фотореалистичные изображения по текстовому описанию.  \nВот еще пример котика, сидящего на окне на фоне большого города, сгенерированного с помощью \nрусскоязычной модели ruDALL-E\n, выпущенной в «свободное обращение» командой Сбера.\nЧтобы попробовать это самостоятельно, вы можете зайти на сайт \nruDALL-E\n или \ncraiyon.com\n. Это урезанная версия DALL-E 2, в которой вы можете ввести описание на английском языке и сгенерировать изображение.\nДиффузные модели  \nШокирующим событием, которое потрясло весь мир генеративного искусства, был выпуск модели Stable Diffusion в открытый доступ. В отличие от других закрытых моделей, вы можете не просто использовать эту через веб-интерфейс, а скачать её веса и самостоятельно начать генерировать изображения, которые будут похожи на настоящие картины. \nЕсли вы поэкспериментируете со Stable Diffusion, вы поймете, что нейросеть знает в лицо многих актеров, знает различные техники различных художников… Датасет, на котором она обучалась, действительно большой. \nКак такие модели работают и почему Stable Diffusion так называется? Дело в том, что они основаны на принципе диффузии. Если у нас есть изображение и мы начнём перемешивать его пиксели, то постепенно придём к «шуму». Этот процесс перемешивания называется диффузией. \nМы можем обучать нейронную модель обратной диффузии — по шуму восстанавливать смысл. Как если бы человеку показали размытую картину за стеклом и сказали: «Что там нарисовано?»   \nЧеловек сказал бы: «Эйфелева башня». Потому что он знает, как выглядит Эйфелева башня. Можно обучать нейросеть делать то же самое. Но делать это в пространстве пикселей оказывается слишком сложно — их слишком много.\nПоэтому Stable Diffusion использует идею латентной диффузии. Сначала с помощью автоэнкодера мы переходим в пространство векторов меньшей размерности, и уже в нём совершаем процесс диффузии. А затем — процесс обратной диффузии, управляемый с помощью текста. \nКогда мы даем текстовый запрос для процесса обратной диффузии — мы делаем так, чтобы ответ был похож на этот текстовый запрос — с помощью сети, которая похожа по архитектуре на CLIP. \nВ итоге мы получаем вектор в так называемом латентном пространстве и с помощью декодера декодируем его в финальное изображение. \nНам не нужно понимать мельчайшие детали, чтобы сгенерировать картину. Мы можем сначала сгенерировать некоторый вектор, а декодер восстановит все детали. \nПримерно так устроены такие модели, и результаты у них совершенно другие. Вот проект, который я сделал на день учителя в этом году. На сайте \nhttp://teachers.experient.art\n можно посмотреть на то, как Stable Diffusion видит себе различных учителей.\nЧто почитать и как попробовать\nУ меня в блоге есть \nстатья\n, в которой я описываю особенности нейрогенеративных моделей и их влияние на будущее искусства. А также \nещё одна\n, в которой непосредственно говорится про то, как сгенерировать изображение.\nКак попробовать самостоятельно? Для «питонистов» есть Jupyter Notebook и Google Colab. Вы \nможете\n скачать исходники и поэкспериментировать: \nЕсли вы хотите показать это кому-то, кто не понимает, что такое Google Colab и Python,  есть инструмент \nbeta.dreamstudio.ai\n. Они дают сгенерировать бесплатно какое-то количество изображений.\nЕще есть инструмент \nneural.love\n, который тоже пытаются монетизировать, но некоторые простые изображения там тоже можно генерировать бесплатно.\nВыводы\nИскусственный интеллект — эффективный инструмент для творчества. Его можно использовать для простых задач: например, извлечь опорные точки. Но он может и сам делать значительную часть работы.\nОднако он не придумает идею. Именно человеку хочется что-то выразить — либо рисуя самостоятельно, либо с помощью Stable Diffusion получая  результаты и отбирая из них лучшее. Вклад человека всегда очень велик. Не надо считать, что компьютер рисует всё сам и заменит художника.\nЕще один важный момент: искусственный интеллект способен привнести случайность. Когда я попробовал писать книгу совместно с искусственным интеллектом, я испытал примерно такое же чувство, как когда мы в детстве с моим приятелем писали книжку по очереди. Ты написал какую-то часть, и ждешь — а что же другой человек напишет. Он же наверняка что-то оригинальное придумает. Это очень приятное чувство, и с искусственным интеллектом ты ощущаешь примерно то же самое. Он придумывает что-то интересное, а человек подстраивает это под свою исходную идею. \nЧто дальше?\nДмитрий выступил с этим докладом в октябре. Казалось бы, прошло всего несколько месяцев. Но нейросети уже успели сделать шаг вперед — GPT-4 генерирует текст не на уровне 10-летнего ребенка, а \nпишет дипломы за студентов\n (кстати, вот ещё одна статья Дмитрия про \nGPT в образовании\n) и код за разработчиков, а генерацию изображений начали использовать в реальных продуктах. Например, недавно вышло \nдополнение к популярной настолке «Имаджинариум»\n, созданное с помощью нейросети. Люди осваивают искусство промптинга — при правильном запросе нейросеть результаты, не отличающиеся от работ digital-художников. Для таких запросов уже существуют \nспециальные каталоги\n.  \nА вот так по мнению нейросети выглядит \nTechTrain\n — наш бесплатный IT-фестиваль. Он пройдет 1 апреля и будет посвящен роли искусственного интеллекта в разработке и жизни. Обсудим и машинное обучение в целом, и конкретные технологии. А также снова попробуем ответить на вопрос, как ИИ изменит нашу жизнь еще через полгода. Переходите по \nссылке\n, чтобы участвовать.  \n \n ",
    "tags": [
        "Нейросети",
        "искусство",
        "искусственный интеллект",
        "генерация изображений",
        "генерация текста",
        "доклад",
        "ai",
        "машинное обучение"
    ]
}