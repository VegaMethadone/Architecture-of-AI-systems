{
    "article_id": "728444",
    "article_name": "Почему открытое письмо против ИИ — не лучшая идея",
    "content": "\r\nВ конце марта на сайте Future of Life появилось письмо, которое вызвало широкий общественный резонанс и дискуссии в СМИ. В письме авторы выражают свою обеспокоенность тем, что технологии искусственного интеллекта, особенно нейросети, могут быть использованы для создания вредоносного или обманного контента и влиять на общественное мнение. Они призывают к более ответственному и безопасному подходу к развитию и применению таких технологий. \n\r\n\n\r\nПисьмо подписали более 2600 независимых исследователей, а также Илон Маск — глава Tesla, SpaceX и Twitter, Эван Шарп — сооснователь Pinterest и Стив Возняк — сооснователь всеми известной компании Apple. Они хотят обратить внимание на то, что нейросети, которые в наше время очень много значат и многим людям облегчают жизнь, также могут иметь негативные последствия, если не контролировать их разработку и использование. Так ли это?\n\r\n\n\r\n\n▍ С чего всё началось?\n\r\nВ прошлом месяце OpenAI представила GPT-4 — большую языковую модель, которая в состоянии решать сложные композиционные задачи и даже проходить стандартизированные тесты на уровне человека. Однако это расширение возможностей искусственного интеллекта вызвало опасения некоторых специалистов, которые считают, что мы развиваемся в сторону систем AGI быстрее, чем планировали ранее. \n\r\n\n\r\nИнститут будущего жизни предупреждает, что последние достижения в области ИИ привели к развитию «бесконтрольной гонки» за созданием и внедрением сложных нейросетевых моделей, которые трудно прогнозировать и контролировать. Отсутствие стратегического планирования и управления этими системами вызывает беспокойство, и разработка мощных ИИ-систем должна проходить лишь после тщательной оценки и контроля их воздействия. В письме авторы подчёркивают: «Системы ИИ с интеллектом, конкурирующим с человеком, могут представлять серьёзные риски для общества и человечества, как показали обширные исследования и признания ведущих лабораторий ИИ. Продвинутый ИИ может представлять собой глубокое изменение в истории жизни на Земле, и его развитие следует планировать и управлять им с соответствующей осторожностью и ресурсами».\n\r\n\n\r\n\nВ общих чертах\n\r\n\n\r\n\nАвторы письма опасаются, что технологии искусственного интеллекта могут быть использованы для создания вредоносного или обманного контента и влиять на общественное мнение. \n\r\n\nОни утверждают, что не ставят своей целью полностью прекратить развитие технологии, а лишь хотят «выйти из опасной гонки», в которой участники рынка конкурируют за создание более мощных и сложных ИИ-моделей без должного учёта рисков. \n\r\n\nОни предлагают в течение полугода объединить усилия и разработать единые для рынка правила безопасности, а также способы контроля экспериментов с искусственным интеллектом. \n\r\n\nПо их словам, нужно также подключить правительства, чтобы те сформировали ведомства, которые будут сертифицировать продукты на основе ИИ, смогут отличать реальный контент от «синтетического» и следить за утечками ИИ-моделей. \n\r\n\n\r\nПисьмо вызвало большой интерес и поддержку от различных сторон. Под ним более тысячи подписей руководителей международных компаний, сотрудников отрасли, университетских профессоров и граждан разных стран мира. Письмо поддержали инженеры и исследователи Google и Microsoft — крупнейших компаний в области ИИ, а также учёные и профессора из многочисленных университетов — Йельского, Оксфордского, Гарвардского, Кембриджского, Стэнфордского, Массачусетского технологического и других. Они выразили свою готовность сотрудничать с другими заинтересованными сторонами для создания более безопасного и ответственного ИИ.\n\r\n\n\r\nОднако не все считают письмо достаточно эффективным способом решения проблемы. Некоторые критики указывают на то, что оно не имеет юридической силы и не гарантирует выполнения предложенных мер. Также они сомневаются в том, что правительства и корпорации будут заинтересованы в ограничении своих возможностей по использованию ИИ для своих целей.\n\r\n\n\r\n\n\r\n\nКлючевой аргумент\n\r\nАвторы письма считают, что нейросети, которые способны конкурировать с человеческим интеллектом, могут нести «серьёзные риски для общества и человечества». Они утверждают, что «Мощные системы искусственного интеллекта не должны быть разработаны без уверенности, что их эффекты будут положительными, а их риски будут управляемыми».\n\r\n\n\r\n\n«Системы искусственного интеллекта, конкурирующие с человеком, могут представлять серьёзную опасность для общества и человечества, как показали обширные исследования и как признали ведущие лаборатории искусственного интеллекта. Как указано в широко одобренных принципах ИИ Asilomar, продвинутый ИИ может запустить глубокие изменения в истории жизни на Земле, и его следует разрабатывать и управлять им с соразмерной тщательностью и ресурсами. К сожалению, такого уровня планирования и управления не существует, даже несмотря на то, что в последние месяцы лаборатории искусственного интеллекта увязли в неконтролируемой гонке по разработке и развёртыванию всё более мощных „цифровых умов“, которые никто — даже их создатели — не могут понять, работу которых не могут прогнозировать или надёжно контролировать».\n\r\n\nКакие примеры потенциальных угроз они имеют в виду?\n\r\n\n▍ Потеря рабочих мест из-за автоматизации\n\r\nПо прогнозам, между 2020 и 2025 годами 85 миллионов рабочих мест будут утрачены из-за замены человеческого труда ИИ-системами в различных отраслях, таких как маркетинг, производство и здравоохранение. Это может привести к социальным проблемам, неравенству и безработице. С другой стороны, это может создать новые рабочие места, но они будут требовать высокой квалификации и технических навыков, которыми не обладает большинство работников.\n\r\n\n\r\n\n▍ Нарушение конфиденциальности\n\r\nИИ-системы способны собирать, анализировать и использовать огромные объёмы информации о людях для различных целей, таких как реклама, персонализация, предсказание поведения и т. д. Это может привести к дезинформации или злоупотреблению данными, а также к манипуляции или влиянию на общественное мнение.\n\r\n\n\r\n\n▍ Алгоритмический биас\n\r\nИИ-системы обучаются на основе данных, которые собраны в интернете. Как всем известно, интернет — не всегда добрая и радужная штука. Если эти данные неполные, нефильтрованные или несбалансированные по каким-либо признакам, то ИИ может выдавать неправильные или дискриминационные результаты.\n\r\n\n\r\n\nК чему призывают авторы письма?\n\r\nОни предлагают, чтобы лаборатории ИИ, которые занимаются разработкой более мощных и сложных систем, чем GPT-4, остановили свою деятельность на шесть месяцев. За это время они должны разработать и внедрить протоколы безопасности, которые будут контролировать внешние независимые эксперты. Авторы письма считают, что это необходимо для того, чтобы сделать нейросети более качественными и безопасными. Они перечисляют ряд характеристик, которые должны быть улучшены у нынешних мощных нейросетей. Это точность, безопасность, интерпретируемость, прозрачность, надёжность, согласованность, доверие и лояльность.\n\r\n\n\r\n\nПочему это далеко не лучшая идея?\n\r\n\n\r\n\nВо-первых, письмо не даёт чётких критериев и механизмов для оценки и контроля рисков, связанных с ИИ. Оно лишь призывает к шестимесячной паузе в экспериментах с ИИ, но не объясняет, как это поможет предотвратить или снизить потенциальные угрозы. \n\r\n\nВо-вторых, письмо игнорирует положительные эффекты и возможности, которые может дать ИИ для человечества. Оно фокусируется только на негативных сценариях и опасностях, не учитывая те области, где ИИ может помочь улучшить жизнь людей и решить глобальные проблемы. \n\r\n\nВ-третьих, письмо может быть контрпродуктивным и вызвать обратный эффект. Оно способно спровоцировать сопротивление и недоверие к тем, кто подписал его, а также к тем, кто поддерживает его. Оно может также стимулировать скрытые или нелегальные эксперименты с ИИ, которые будут ещё более опасными и неконтролируемыми.\n\r\n\n\r\nВ исследовательских лабораториях сидят квалифицированные специалисты, которые уделяют огромное внимание безопасности и этике. Их работа основана на научных исследованиях, а не на голливудских фильмах, поэтому стоит оценивать реальные результаты. Обучение новых моделей не будет прекращено, так как это неэффективное предложение, которое не приведёт к улучшению ситуации. Необходимо продолжать развивать более оптимальные способы контроля, которые уже применяют большие компании. Несложно подметить что гонка ИИ становится всё жарче и останавливаться никто не намерен. Ставки слишком высоки. Временная остановка разработок ИИ может дать преимущество другим компаниям или странам, которые не подчиняются петиции или не согласны с ней. \n\r\n\n\r\n\nВывод\n\r\nВажно помнить, что искусственный интеллект — это инструмент, который может быть использован как во благо, так и во зло, в зависимости от человека, который его контролирует.\n\r\n\n\r\nОдним из основных приоритетов для создателей искусственного интеллекта является обеспечение того, чтобы эти системы работали в интересах человечества, а не против него. Это требует постоянного сотрудничества между учёными, инженерами, правительственными организациями и обществом в целом, чтобы создать регулирующую систему, которая будет обеспечивать безопасное и этичное использование ИИ.\n\r\n\n\r\nВместо остановки развития искусственного интеллекта, мы должны настаивать на инновациях и развитии новых методов, которые помогут нам обеспечить контроль и безопасность. Это может включать в себя создание механизмов для проверки и отслеживания его работы, а также разработку систем, которые позволят обнаруживать и устранять нежелательное поведение.\n\r\n\n\r\nИ да, вывод написан GPT-4.\n\r\n\n\r\n\n\r\n\nTelegram-канал с розыгрышами призов, новостями IT и постами о ретроиграх 🕹️\n \n ",
    "tags": [
        "ruvds_статьи",
        "нейросети",
        "искусственный интеллект",
        "ИИ",
        "AL",
        "gpt-4",
        "gpt"
    ]
}