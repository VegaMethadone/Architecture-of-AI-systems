{
    "article_id": "727348",
    "article_name": "Как я решила попробовать себя в ML: анализ эмоциональной окраски отзывов с Кинопоиска 2.0",
    "content": "Недавно я решила попробовать реализовать задачу анализа эмоциональной окраски отзывов с Кинопоиска. Я бы хотела поделиться своим опытом и описать шаги, которые использовала для реализации стоящей передо мною задачей. \nИтак, в самом начале у меня был только датасет и опорный план для дальнейшей реализации всего этого дела, приступим :)\nШаг 1: получение данных | \nmain.py\n + \nreviews_data.zip\nСкачиваем \njson-файлы\n с отзывами и затем читаем данные из файла. Добавляем полученные отзывы в общий список.\nimport json\n\n\nwith open(\"reviews_data.json\", \"r\") as read_file:\n    load_reviews = json.load(read_file)\n\n    \nreviews = []\nreviews += load_reviews['good']\nreviews += load_reviews['bad']\nreviews += load_reviews['neutral']\nШаг 2: предобработка данных | \nPreprocessing.py\n  \nНа этом этапе удаляем все ненужные символы, такие как цифры, знаки препинания и английские буквы. \nimport re\nfrom nltk.corpus import stopwords\n\n\ndef delete_symbols(review): \n    new_review = []\n    for word in review:\n        reg_sumbols = re.sub('[^\\w\\s]+|[\\d]+', ' ', word)\n        reg_eng_symbols = re.sub('[a-zA-Z\\s]+', ' ', reg_sumbols)\n        new_review.append(reg_eng_symbols.split())\n    return new_review\ndelete_symbols(review)\n: функция удаляет ненужные символы из текстовых данных, включая неалфавитно-цифровые символы и цифры. Она возвращает список слов, которые были разделены из исходного отзыва.\nДальше используем библиотеку NLTK для удаления стоп-слов из текста. Это поможет получить более точные результаты анализа.\ndef delete_stop_words(review_words): \n    stop_words = list(stopwords.words('russian'))\n    deleted_stop_words = []\n    for w in review_words:\n        word = w.lower()\n        if word not in stop_words and word != \"\\n\":\n            deleted_stop_words.append(word)\n    return deleted_stop_words\ndelete_stop_words(review_words)\n: функция удаляет стоп-слова из списка слов, переданного ей. Стоп-слова - это распространенные слова в языке (такие как \"и\", \"в\", \"на\" и т.д.), которые не несут много смысла и обычно удаляются из текстовых данных. Эта функция использует модуль \nstopwords\n из Natural Language Toolkit (NLTK), чтобы получить список стоп-слов для русского языка.\nПриводим все слова в отзывах к нижнему регистру. \ndef lower_review(review_words): \n    lower_reviews = []\n    for rev in review_words:\n        deleted_stop_words = delete_stop_words(rev)\n        lower_reviews.append(deleted_stop_words)\n    return lower_reviews\nlower_review(review_words)\n: функция приводит все слова в текстовых данных к нижнему регистру. Это делается, чтобы гарантировать, что слова, которые появляются в разных регистрах, рассматриваются как одно и то же слово. Функция берет список слов, который был обработан функцией \ndelete_stop_words()\n, и возвращает список списков, где каждый внутренний список содержит слова одного отзыва в нижнем регистре.\nШаг 3: векторизация |\n \nVectorization.py\n  \nПреобразовываем полученные слова в векторы при помощи модели FastText.\nimport json\nimport fasttext\n\n\nft = fasttext.load_model('cc.ru.300.bin')\n\n\ndef get_vector(review):\n    sentence = \"\".join(s + \" \" for s in review)\n    sentence_vector = ft.get_sentence_vector(sentence)\n    return sentence_vector\nФункция \nget_vector(review)\n использует модель \nfasttext\n для получения векторного представления отзыва. Поочередно происходит векторизация отзывов из списков \ngood\n, \nbad\n и \nneutral\n в файле \nreviews_data.json\n. Векторизованные отзывы записываются в новый файл \nvectorized.json\n. \ndef vectorization_reviews(lower_reviews):\n    with open(\"reviews_data.json\", \"r\") as read_file:\n        reviews = json.load(read_file)\n    for i in range(1000):\n        reviews['good'][i] = get_vector(lower_reviews[i]).tolist()\n    for i in range(1000):\n        reviews['bad'][i] = get_vector(lower_reviews[i + 1000]).tolist()\n    for i in range(1000):\n        reviews['neutral'][i] = get_vector(lower_reviews[i + 2000]).tolist()\n    with open('vectorized.json', 'w') as outfile:\n        json.dump(reviews, outfile)\nФункция \nvectorization_reviews(lower_reviews)\n вызывает функцию \nget_vector(review)\n и производит векторизацию всех отзывов.  \nШаг 4: KDTree | \nKDTree_vec.py\n  \nИспользуем KDTree(K-d-дерево) для нахождения наиболее похожих отзывов на введенный пользователем отзыв. \nfrom scipy import spatial\n\n\ndef find_similar(vec_reviews, user_review):\n    treee = spatial.KDTree(vec_reviews)\n    similar_review = treee.query(user_review, 10)\n    print()\n    return similar_review[1]\nФункция \nfind_similar\n, использует библиотеку \nscipy\n для поиска наиболее похожих векторов на заданный вектор.\nvec_reviews\n: матрица векторов отзывов\nuser_review\n: вектор, который нужно найти в матрице\nДля этого функция создает объект \nKDTree\n из библиотеки \nscipy\n. Затем функция использует метод \nquery\n, чтобы найти 10 наиболее похожих векторов на заданный вектор. Результатом работы функции является массив индексов 10 наиболее похожих векторов в матрице \nvec_reviews\n.\nШаг 5: завершение | \nmain\nИспользуем классификационную модель, чтобы определить эмоциональную окраску полученных отзывов. Работаем с методом опорных векторов (SVM) для классификации отзывов на положительные, отрицательные и нейтральные. \nПодведём итоги :)\nВ процессе написания данной статьи мы рассмотрели шаги для реализации анализа эмоциональной окраски отзывов с помощью методов машинного обучения. Мы использовали библиотеки для предобработки текста, векторизации и поиска ближайших соседей.\nНадеюсь, что материал был полезен и поможет Вам создать собственный анализатор эмоциональной окраски текстов.  \nCсылочки:\n \nGitHub\n, \nTelegram\n \n ",
    "tags": [
        "python",
        "машинное обучение",
        "классификатор текстов",
        "kd-дерево"
    ]
}