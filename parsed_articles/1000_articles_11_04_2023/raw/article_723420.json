{
    "article_id": "723420",
    "article_name": "ArrayPool<T>: подводные камни",
    "content": "\r\n\nАвтоматическая сборка мусора упрощает разработку программ, избавляя от необходимости отслеживать жизненный цикл объектов и удалять их вручную. Однако, чтобы сборщик мусора был полезным инструментом, а не главным врагом на пути к высокой производительности — иногда имеет смысл помогать ему, оптимизируя частые аллокации и аллокации больших объектов.\n\r\n\nДля уменьшения аллокаций в современном .NET предусмотрены \nSpan/Memory<T>\n, \nstackalloc\n с поддержкой \nSpan\n, структуры и другие средства. Но если без объекта в куче не обойтись, например, если объект слишком большой для стека, или используется в асинхронном коде — этот объект можно переиспользовать. И для самых крупных объектов — массивов, в .NET встроены несколько реализаций \nArrayPool<T>\n.\n\r\n\nВ этой статье я расскажу о внутреннем устройстве реализаций \nArrayPool<T>\n в .NET, о подводных камнях, которые могут сделать пулинг неэффективным, о concurrent-структурах данных, а также о пулинге объектов, отличных от массивов.\n\r\n\nAllocator vs Pool\n\r\n\nПул можно рассматривать как аллокатор объектов. У них одинаковый интерфейс с двумя методами, выполняющими функции \nnew\n и \ndelete\n. Хорошая реализация нативного аллокатора также переиспользует память: при \ndelete\n участок памяти не сразу отдаётся операционной системе обратно, а переиспользуется для новых объектов внутри программы, т.е. работает как пул.\n\r\n\nВозникает вопрос, зачем делать пул managed-объектов, вместо перехода на нативный аллокатор?\n\r\n\n\r\n\nЭто требует меньших изменений в коде, который эти объекты использует. Также, \nнекоторые\n API принимают \nArraySegment<T>\n, а не \nMemory<T>\n\r\n\nЭто помогает сохранить код кроссплатформенным. Использование стороннего аллокатора обычно предполагает подключение нативной \nбиблиотеки\n.\n\r\n\nManaged-объекты могут иметь ссылки на другие managed-объекты. Ссылаться на managed-объекты из памяти, которой не управляет сборщик мусора нельзя — при дефрагментации кучи и перемещении объектов такие ссылки не будут обновлены и станут невалидными.\n\r\n\n\r\n\nОтличие пула от аллокатора в том, что пулу можно не сохранять часть объектов, например, при превышении вместимости — тогда их соберёт сборщик мусора. Аллокатор же должен гарантировать отсутствие утечек памяти.\n\r\n\nArrayPool<T>\n\r\n\nВ .NET встроены две разные реализации абстрактного класса \nArrayPool<T>\n. Т.к. невозможно сохранить массивы для каждого из размера по-отдельности (их будет слишком много), при вызове \n.Rent(N)\n пул возвращает массив размера N или больше. Внутри пул хранит массивы с длинами, равными степеням двойки.\n\r\n\nПервая — для пулов, создающихся с помощью \nArrayPool<T>.Create()/ArrayPool<T>.Create(maxArrayLength, maxArraysPerBucket)\n. Вторая — статический \nArrayPool<T>.Shared\n. Также можно сделать свою реализацию \nArrayPool<T>\n.\n\r\n\nРазбор отличий пулов, добываемых через \nArrayPool<T>.Shared\n и \nArrayPool<T>.Create(...)\n начнём с бенчмарка. Кроме этих реализаций, протестируем также реализацию, которая ничего не переиспользует, а просто аллоцирует новые массивы и бросает их на совесть GC.\n\r\n\n// Threads = 16\n// Iterations = 64*1024\n// ArraySize = 1024\n[Benchmark]\npublic void ArrayPoolConcurrent()\n{\n  var tasks = new Task[Threads];\n  for (int i = 0; i < Threads; i++)\n  {\n    tasks[i] = Task.Run(() =>\n    {\n      for (int j = 0; j < Iterations; j++)\n      {\n        var arr = pool.Rent(ArraySize);\n\n        // имитация использования массива сложностью O(ArraySize)\n        // не просто так же он нам нужен, чтобы сразу вернуть в пул?\n        Random.Shared.NextBytes(arr);\n        pool.Return(arr);\n      }\n    });\n  }\n\n  Task.WaitAll(tasks);\n}\n\r\n\n|      Pool |        Mean |     Allocated |\n|----------:|------------:|--------------:|\n|    Create |   170.09 ms |       2.77 KB |\n|    Shared |    14.96 ms |       2.41 KB |\n|       new |    69.77 ms | 1072085.02 KB |\n\r\n\nКод, не связанный с работой с пулом занимает ~13 ms, что было замерено отдельно.\n\r\n\nПул, созданный через \nArrayPool<byte>.Create()\n оказался медленнее даже аллокации (оператора \nnew\n), и гораздо медленнее \nArrayPool<byte>.Shared\n (за вычетом оверхеда). Но не торопитесь делать выводы по одному бенчмарку, тестирующему лишь частный случай, и списывать эту реализацию пула — далее мы разберёмся, как эти пулы устроены внутри, и почему результат получился таким.\n\r\n\nОтмечу, что пул — лишь вспомогательный компонент, и бенчмаркать нужно алгоритм или сервис, в котором пул используется — сравнивать, улучшилась ли производительность от переиспользования объектов. Да и самая быстрая реализация пулинга — та, которая не содержит никакой синхронизации с другими потоками. В однопоточном коде проще сохранить массив в поле класса и всегда использовать его, без всяких пулов. А если нужно хранить несколько объектов в однопоточном алгоритме, то подойдут обычные \nQueue<T>/Stack<T>\n.\n\r\n\nArrayPool<T>.Create()\n\r\n\n\r\n\nМетоды \n.Create()\n и \n.Create(maxArrayLength, maxArraysPerBucket)\nсоздают \nConfigurableArrayPool<T>\n. Здесь нужно быть осторожным — значение максимальной длины массива в этом пуле по умолчанию — всего лишь \n1024 * 1024\n, при её превышении массивы будут аллоцироваться и не сохраняться в пуле. Поэтому, если \nArrayPool\n создаётся для больших массивов — параметры придётся переопределить.\n\r\n\nРеализация \nConfigurableArrayPool<T>\n очень \nпроста\n:\n\r\n\n\r\n\nмассивы в пуле сгруппированы по размерам (размер — всегда степень двойки)\n\r\n\nмассивы одного размера хранятся в списке (на основе массива)\n\r\n\nкаждый список защищён от многопоточного доступа с помощью SpinLock\n\r\n\n\r\n\nКак раз из-за блокировки эта реализация пула не масштабируется на множество потоков. В итоге основным её применением становятся большие массивы, в случае с которыми время работы с пулом пренебрежимо мало по сравнению с обработкой данных, которыми эти массивы заполняются.\n\r\n\nArrayPool<T>.Shared\n\r\n\n\r\n\nЭто статический пул, разделяемый всем кодом в программе. Реализация называется \nTlsOverPerCoreLockedStacksArrayPool<T>\n и на данный момент, никаких настроек не имеет. Максимальный размер массива в пуле — 2^30 элементов. \nTls\n в названии значит \nThreadLocalStorage\n, исходя из этого можно догадаться, за счёт чего этот пул работает быстро в многопоточной среде.\n\r\n\nВ этом пуле реализовано двухуровневое хранение объектов. Первый уровень — локальный набор массивов для каждого потока. Хранится в \n[ThreadStatic]\n поле. Доступ к локальной части пула не требует синхронизации с другими потоками. Однако, локально хранится максимум по одному массиву каждого размера. Использование статического поля здесь возможно, т.к. пул глобальный, т.е. создаётся в единственном экземпляре. В нестатическом пуле для этой оптимизации придётся использовать \nThreadLocal\n.\n\r\n\nВторой уровень — разделяемый между потоками. Но в отличие от \nConfigurableArrayPool<T>\n, для каждого размера хранится не один список массивов, а несколько — по количеству логических ядер (max. \n64\n), каждый из списков защищён отдельной блокировкой. Это снижает конкуренцию между потоками — теперь они идут под разные блокировки, а не под одну. Вместо \nSpinLock\n в реализации \nShared\n пула используется обычный \nlock/Monitor\n.\n\r\n\nSpeed — Memory tradeoff\n\r\n\nОптимизация с thread local слотом имеет свою цену: в пуле может скопиться большое количество крупных массивов, раздувая память приложения. В примере ниже первый поток создаёт новый массив и возвращает его в пул. Этот массив попадает в Thread Local слот первого потока. В итоге, при попытке получить массив того же размера из другого потока — переиспользования не произойдёт и будет выделен новый массив. В итоге для больших массивов может быть выгоднее использовать реализацию пула с общим набором объектов для всех потоков.\n\r\n\n// thread 1\nvar pool = ArrayPool<long>.Shared;\nvar arr1 = pool.Rent(1024*1024*1024);\npool.Return(arr1);\n\nTask.Run(() =>\n{\n  // thread 2\n  var arr2 = pool.Rent(1024 * 1024 * 1024);\n  Console.WriteLine(arr1 == arr2);\n}).Wait();\n\r\n\nОчистка памяти при GC\n\r\n\nОтчасти для решения предыдущей проблемы, в \nArrayPool<T>\n предусмотрена очистка памяти при сборке мусора. С помощью хака с \nфинализатором\n пул узнаёт о срабатываниях сборщика мусора и периодически выбрасывает избыток массивов, помогая освободить память.\n\r\n\nНе всегда это поведение желательное. Преждевременное удаление больших массивов, находящихся в Large Object Heap, может привести к излишней фрагментации кучи. Это ещё один повод задуматься об использовании другого механизма для переиспользования больших массивов.\n\r\n\nШтраф за невозврат массива в пул\n\r\n\nБенчмарк, с которого мы начали, в случае \nArrayPool<T>.Shared\n \"пробивает\" только первый уровень пулинга — thread local. Возникает вопрос, насколько производителен второй уровень — per core locked stacks, особенно учитывая то, что в нём есть блокировка. Для замера, сделаем бенчмарк, использующий сразу два массива из пула.\n\r\n\n\n                        \nКод бенчмарка\n\n                        \n// Threads = 16\n// Iterations = 1024\n// ArraySize = 1024\n[Benchmark]\npublic void ArrayPoolConcurrent_TwoArrays()\n{\n  var tasks = new Task[Threads];\n  for (int i = 0; i < Threads; i++)\n  {\n    tasks[i] = Task.Run(() =>\n    {\n      for (int j = 0; j < Iterations; j++)\n      {\n        var arr1 = pool.Rent(ArraySize);\n        var arr2 = pool.Rent(ArraySize);\n        Random.Shared.NextBytes(arr1);\n        Random.Shared.NextBytes(arr2);\n        pool.Return(arr2);\n        pool.Return(arr1);\n      }\n    });\n  }\n\n  Task.WaitAll(tasks);\n}\n\n                    \n\r\n\n|      Pool |        Mean |      Allocated |\n|----------:|------------:|---------------:|\n| Allocator |      138 ms |  2146306.76 KB |\n|    Create |      230 ms |        2.88 KB |\n|    Shared |       33 ms |        2.53 KB |\n\r\n\nНагрузка, не связанная с пулом, заняла 24 ms — второй уровень \nShared\n пула уже не бесплатен, но пул по прежнему достаточно производителен — это достигается за счёт того, что потоки в бенчмарке берут массивы из разных списков и захватывают разные блокировки — contention не возникает.\n\r\n\nКаждый поток не всегда использует только один список. Если при \n.Rent(N)\n подходящий массив не был найден в \"своём\" списке, то по кругу обходятся все остальные, лишь после этого аллоцируется новый массив. И если не возвращать массивы в пул обратно, то каждый вызов \n.Rent(N)\n будет по очереди захватывать все блокировки, приводя к большому contention.\n\r\n\n|            Pool |        Mean | Lock Contentions |     Allocated |\n|----------------:|------------:|-----------------:|--------------:|\n|       Allocator |      138 ms |                - | 2146306.76 KB |\n|          Shared |       33 ms |                - |       2.53 KB |\n| Shared_NoReturn |      968 ms |        3666.6667 | 2144145.85 KB |\n\r\n\nОт этой проблемы есть некоторая \"защита\". Второй уровень пула (PerCoreLockedStacks) инициализируется только при первом возврате массива в него. Если нигде в программе массивы не возвращаются в пул, то \n.Rent(N)\n будет аллоцировать новый массив без захвата блокировок.\n\r\n\nТакже, вместимость Shared пула относительно небольшая — 8 массивов каждого размера в каждом PerCoreLockedStacks (т.е. 512 на размер максимум). И если требуется много массивов, каждый из которых будет использоваться долгосрочно — эффекта от пулинга не будет, т.к. неизбежно будут создаваться новые массивы, а потоки будут обходить блокировки в надежде найти хоть что-то в опустошенном пуле.\n\r\n\nДиагностики\n\r\n\nДля мониторинга работы стандартных реализаций \nArrayPool<T>\n предусмотрены события \nSystem.Buffers.ArrayPoolEventSource\n. Их можно получить через PerfView, dotnet trace, EventListener и другими способами. Основное событие, на которое имеет смысл смотреть — \nBufferAllocated\n — если аллоцируется много новых массивов, значит пулинг неэффективен. Проблему с lock contention в случае с \nShared\n пулом можно вычислить по событиям \nMicrosoft-Windows-DotNETRuntime/Contention/Start\n. Подробнее о диагностике .NET-приложений уже было описано в недавней \nстатье\n.\n\r\n\nПример стектрейса из PerfView:\n\r\n\nName\n Event Microsoft-Windows-DotNETRuntime/Contention/Start\n+ module coreclr <<coreclr!?>>\n + module System.Private.CoreLib.il <<System.Private.CoreLib.il!System.Buffers.TlsOverPerCoreLockedStacksArrayPool`1[System.Byte].Rent(int32)>>\n |+ module app <<app!PoolExperiments.<NoReturningMethod>b__24_0()>>\n\r\n\nУвы, о неэффективном использовании \nSpinLock\n нет никаких событий. Но о неэффективном использовании \nConfigurableArrayPool<T>\n можно узнать при профилировании или анализе дампа.\n\r\n\nПример стектрейса из \ndotnet dump\n:\n\r\n\nCall Site\n[HelperMethodFrame: 0000004e2f77f488] System.Threading.Thread.SleepInternal(Int32)\nSystem.Threading.Thread.Sleep(Int32) [/_/src/libraries/System.Private.CoreLib/src/System/Threading/Thread.cs @ 375]\nSystem.Threading.SpinWait.SpinOnceCore(Int32) [/_/src/libraries/System.Private.CoreLib/src/System/Threading/SpinWait.cs @ 196]\nSystem.Threading.SpinLock.ContinueTryEnter(Int32, Boolean ByRef) [/_/src/libraries/System.Private.CoreLib/src/System/Threading/SpinLock.cs @ 359]\nSystem.Buffers.ConfigurableArrayPool`1+Bucket[[System.Byte, System.Private.CoreLib]].Rent() [/_/src/libraries/System.Private.CoreLib/src/System/Buffers/ConfigurableArrayPool.cs @ 205]\nSystem.Buffers.ConfigurableArrayPool`1[[System.Byte, System.Private.CoreLib]].Rent(Int32) [/_/src/libraries/System.Private.CoreLib/src/System/Buffers/ConfigurableArrayPool.cs @ 88]\nPoolExperiments.<ConfigurableArrayPool_SpinLock>b__21_0() [Program.cs @ 162]\n\r\n\nПулы других объектов\n\r\n\nПока что речь шла лишь о реализациях \nArrayPool<T>\n — пулах массивов. Иногда требуется переиспользовать не только массивы, но и другие объекты. И их пулинг также имеет свои тонкости.\n\r\n\nВторой уровень \nArrayPool<T>.Shared\n для массивов одного размера — по сути уже потокобезопасный пул одинаковых объектов. Казалось бы, почему не взять его в качестве пула объектов? В некоторых сценариях такой подход будет удачным, но большинство объектов, не являющихся массивами — маленькие. С одной стороны, это увеличивает требования к производительности пула — overhead от пулинга уже не спрятать среди полезной нагрузки. С другой — позволяет не заморачиваться с многопоточностью и сделать \n[ThreadStatic]Stack/Queue<T>\n пул, локальный для каждого потока. Такой вариант не подходит для объектов, которые могут перемещаться из одного потока в другой, т.к. возникает риск опустошения пула в одном из потоков и переполнения в другом — разницы с аллокацией тогда не будет.\n\r\n\nТакже часто пул объектов реализуют на основе \nConcurrentQueue<T>\n. Например, так реализован \nDefaultObjectPool<T>\n из \nпакета\n \nMicrosoft.Extensions.ObjectPool\n, с тем отличием, что в нём есть ещё и первый уровень — поле \n_fastItem\n под хранение одного объекта. Это поле класса, не статическое, и не \n[ThreadStatic]/ThreadLocal\n. Работа с ним ведётся через \natomic\n инструкции (\nInterlocked\n).\n\r\n\nВместо \nConcurrentQueue<T>\n можно использовать другую структуру данных — \nConcurrentStack<T>\n или \nConcurrentBag<T>\n. Использовать стек не имеет смысла — он аллоцирует по ноде связного списка на каждый элемент, и хуже масштабируется — когда \nConcurrentQueue<T>\n разрывается потоками с двух разных сторон, вся нагрузка на стек приходится только на один его край, а LIFO-порядок не даёт никаких преимуществ для пула. \nConcurrentBag<T>\n работает быстрее в случае, когда добавления и удаления элементов происходят из одного потока за счёт \nThreadLocal<>\n внутри, но не догоняет по производительности алгоритм из \nArrayPool<T>.Shared\n.\n\r\n\nДля теста были сделаны несколько реализаций пулов — наивные поверх одной concurrent-коллекции, глобальной \nStack<T>\n под локом (с _fastItem слотом и без), отдельным ThreadLocal \nStack<T>\n для каждого потока, также взяты \nDefaultObjectPool<T>\n (и его модификация с \nThreadLocal\n) и \nArrayPool<T>.Shared\n для сравнения. В качестве объекта использовался массив размером 1 КБ для унификации кода с бенчмарком для \nArrayPool<T>\n.\n\r\n\n|                Pool |   Mean | Lock Contentions |   Allocated |\n|--------------------:|-------:|-----------------:|------------:|\n|       StackWithLock | 818 ms |        6476.0000 |     3.87 KB |\n|  StackWithLock+Slot | 685 ms |        3200.0000 |     3.30 KB |\n|  ConcurrentStackObj | 540 ms |                - | 65539.59 KB |\n|  ConcurrentQueueObj | 455 ms |                - |     3.82 KB |\n|   DefaultObjectPool | 320 ms |                - |     2.99 KB |\n| ThreadLocObjectPool | 208 ms |                - |     2.87 KB |\n|    ConcurrentBagObj |  77 ms |                - |     2.51 KB |\n|              Shared |  35 ms |           0.0667 |     2.49 KB |\n|   ThreadStaticStack |  27 ms |                - |     2.44 KB |\n\r\n\nСтруктуре данных недостаточно быть lock-free, чтобы быть быстрой — разница с обычной блокировкой оказалась всего ~2 раза. Атомарные инструкции — не магия, хоть они и работают быстрее блокировок, но atomic write масштабируется \nплохо\n. В итоге для большей производительности следует пользоваться теми же техниками, что в \nArrayPool<T>.Shared\n — thread local слоты и шардирование разделяемого между потоками хранилища, чтобы минимизировать любую синхронизацию потоков.\n\r\n\nВ критичных для производительности местах внутри \ndotnet/runtime\n реализованы свои пулы. Например, в \nPoolingAsyncValueTaskMethodBuilder\n, использующимся для снижения числа аллокаций в асинхронном коде, реализован двухуровневый пул — \n[ThreadStatic]\n слот + слоты по количеству ядер, работа с которыми ведётся через \nInterlocked\n.\n\r\n\nBounded queue\n\r\n\nМожет возникнуть желание сделать свой собственный пул. Например, есть подозрение, что мешают блокировки внутри стандартного \nShared\n пула — внешне это может проявляться как высокая tail latency в метриках сервиса (какие-то два потока делят один LockedStack — разделение происходит по ProcessorId). Или нужен эффективный пул для объектов с общим хранилищем между потоками.\n\r\n\nКазалось, можно взять \nArrayPool<T>.Shared\n и заменить \nLockedStacks\n на несколько \nConcurrentQueue<T>\n. Но важной деталью реализаций \nArrayPool<T>.Shared\n — поддержка ограничения числа элементов, хранящихся внутри пула. В пулах на основе \nConcurrentQueue<T>\n это реализуется с помощью дополнительного счётчика элементов, изменяющегося через \nInterlocked\n. Это, во-первых, замедляет работу с очередью, добавляя ещё одну точку синхронизации. Во-вторых, это неэстетично — \nConcurrentQueue<T>\n реализована поверх internal-класса \nConcurrentQueue<T>.Segment\n, который представляет из себя \nочередь\n с ограниченной ёмкостью. В результате, при использовании счётчика bounded-очередь обёрнута в unbounded, а поверх неё реализована bounded-очередь.\n\r\n\nВозникает желание извлечь \nConcurrentQueue<T>.Segment\n в отдельный класс и использовать его для реализации пула. Причём, не только у меня. Об этом есть очень старый \nproposal\n в dotnet/runtime, но, что называется, не договорились. Сам класс \nConcurrentQueue<T>.Segment\n легко отделяется от \nConcurrentQueue<T>\n и сразу готов к использованию.\n\r\n\nПротестируем такую реализацию. В случае, когда используется по одному LockedStack или очереди на каждое логическое ядро — разницы нет. Но если уменьшить шардирование в 4 раза, становится видна разница между разными вариантами. Это показывает, насколько важно предположение о том, что разные потоки будут пользоваться разными блокировками в структуре \nArrayPool<T>.Shared\n, а также то, что казалось бы легковесный \nInterlocked\n счётчик вокруг очереди всё же добавляет накладные расходы. Если же вас устраивает пул с неограниченной вместимостью, то \nConcurrentQueue<T>\n — хорошая структура для этой задачи.\n\r\n\nPer core data structures:\n|                       Pool |     Mean |\n|           BoundedQueuePool |    33 ms |\n|            ConcurrentQueue |    33 ms |\n|                     Shared |    33 ms |\n|    ConcurrentQueue+Counter |    33 ms |\n\n(ProcessorCount / 8) data structures:\n|                    Pool |        Mean | Lock Contentions |\n|------------------------ |------------:|-----------------:|\n|        BoundedQueuePool |       65 ms |                - |\n|         ConcurrentQueue |       65 ms |                - |\n|          Shared_Limited |      118 ms |            354.4 |\n| ConcurrentQueue+Counter |       75 ms |                - |\n\r\n\nВыводы\n\r\n\nПулинг объектов помогает снизить аллокации и нагрузку на сборщик мусора, но сами пулы — сложные структуры данных, и неудачная или неподходящая для конкретного профиля нагрузки реализация пула может испортить производительность. Так, даже оставаясь в рамках стандартных реализаций пулов, для небольших массивов, нужных на короткое время предпочтительно использовать масштабирующийся \nArrayPool<T>.Shared\n, а для больших массивов — пул, созданный через \nArrayPool<T>.Create(..., ...)\n как более вместительный и экономный в плане отсутствия разделения по потокам.\n\r\n\nТакже, если вам захотелось запулить все объекты в программе — подумайте дважды. Избыточный пулинг значительно усложняет код, особенно когда появляется ручной подсчёт ссылок и другие нетривиальные способы трекинга жизненного цикла объектов. Всё это не избавляет от риска использовать уже возвращённый в пул объект и не заметить этого. В некоторых случаях, проще положиться на сборщик мусора, или снизить число аллокаций другими инструментами, будто структуры, \nstackalloc\n, \nSpan/Memory<T>\n, или scatter-gather IO.\n\r\n\nКроме пулинга массивов и объектов есть и более сложные случаи, например динамические структуры данных — листы, хэш-таблицы; переиспользование объектов внутри concurrent структур данных. Но это уже совсем другая история.\n\r\n\nСсылки\n\r\n\n\r\n\nРепозиторий с бенчмарками\n\r\n\nМой канал (блог о .NET)\n\r\n\n \n ",
    "tags": [
        ".NET",
        "c#",
        "memory management",
        "pooling",
        "pool"
    ]
}