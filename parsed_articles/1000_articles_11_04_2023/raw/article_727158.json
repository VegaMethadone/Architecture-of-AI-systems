{
    "article_id": "727158",
    "article_name": "Человечество против искусственного интеллекта: может ли развитие нейросетей привести к катастрофе",
    "content": "История про «восстание машин» давно знакома всем любителям научной фантастики, но после взрывного роста возможностей нейросетевых языковых моделей (вроде ChatGPT) об этом риске заговорили и вполне серьезные исследователи. В этой статье мы попробуем разобраться — есть ли основания у таких опасений, или это всего лишь бред воспаленной кукухи?\nИлон Маск считает риск «глобальной катастрофы из-за ИИ» реальной проблемой – так что, может быть, и вам стоит разобраться, что там и как  \nОсновным автором этой статьи является Вастрик (известный \nтехноблогер\n), а Павел Комаровский (\nRationalAnswer\n), Игорь Котенков (\nСиолошная\n) и Кирилл \nПименов\n оказывали ему посильную помощь в подготовке материала. Это первая из наших совместных статей про безопасность ИИ (но подробнее об этом уже в конце).\nЧеловечество vs Искусственный интеллект\nДобро пожаловать в 2023 год, когда мир снова помешался на искусственном интеллекте. Весь интернет соревнуется, кто еще какую задачу автоматизирует с помощью ChatGPT, и какой фейк от Midjourney лучше завирусится — а технобро‑миллионеры, типа Илона Маска, подвозят фурами деньги в создание «настоящего» ИИ. Такого, который сможет сам учиться, развиваться и решать любые задачи, даже которые мы не умели решать раньше.\nЭто называется Artificial General Intelligence (AGI) или «универсальный ИИ» (УИИИИИ) по‑нашему. То, что когда‑то было научной фантастикой, сейчас шаг за шагом становится реальностью.\nТим Урбан, автор блога «Wait but why?», в своей статье \nThe AI Revolution\n еще в 2015 году неплохо рассказал, почему мы недооцениваем скорость появления машинного интеллекта, который будет сильнее нашего (обычного, мясного).\nНаходясь в своей точке на таймлайне, мы опираемся исключительно на прошлый опыт, потому видим прогресс практически как прямую линию.\nМы плохо чувствуем технический прогресс, потому что он всегда идёт волнами, чередуя периоды «хайпа» и периоды всеобщего разочарования. Сначала мы сходим с ума по новой игрушке, а через год‑два неизбежно разочаровываемся и считаем, что ничего нового она особо не принесла, кроме проблем.\nИ только те, кто лично пережил несколько предыдущих «волн», могут понять, что новые волны приходят чаще и сильнее.\nИ следующая волна, быть может, погрузит человечество в новую эпоху. Эпоху, когда наш интеллект больше не самый сильный на планете.\nGPT-модели (устройство которых подробнее описано в \nэтой статье\n) сейчас очень хорошо притворяются, будто их ответы «разумны», но всё еще далеки от настоящего интеллекта. Да, генеративные модели запустили новую волну огромных нейросетей, на которые раньше человечеству просто не хватило бы вычислительных ресурсов, но по сути они всё ещё «тупые» генераторы текста, у которых даже нет своей \nпамяти\n.\nТо, что ChatGPT ведёт с вами диалог, на самом деле лишь иллюзия — технически нейросети просто каждый раз скармливают историю предыдущих сообщений как «контекст» и запускают с нуля.\nВсё это пока далеко от настоящего «интеллекта» в нашем понимании.\nОднако, исследователи в области ИИ уверены, что мы точно создадим «универсальный ИИ» уже в ближайшие десятилетия. На \nМетакулюсе\n, одном из популярных «рынков предсказаний», народ даже более оптимистичен: сейчас там медиана — 2026 год, а 75 перцентиль — 2029-й.\nТак что сегодня я не хочу рубить лайки на хайповых тредах про «10 причин, почему вы используете ChatGPT неправильно». Я хочу сделать шаг вперёд и подумать: а что же будет, если мы всё-таки создадим настоящий сильный искусственный интеллект?\nПоявятся ли у него свои цели? А когда он начнёт их достигать, что остановит его от уничтожения всяких мелких препятствий на пути — как, например, людей, с их ограниченным мясным мозгом и неэффективными нормами морали и законами? Что мы будем делать в этом случае, и какие вообще сейчас есть точки зрения на этот счёт?\nСчастливое будущее: всем по ИИ-помощнику!\nВ своей ранней \nзаметке про ChatGPT\n я уже рассуждал об этом. Связка человек + ИИ попросту эффективнее справляется с работой, чем отдельный человек, а значит это всего лишь вопрос времени, когда все работодатели начнут писать в вакансиях «уверенный пользователь нейросетей», как было с «уверенным пользователем ПК» в забытом прошлом.\nИИ-помощники увеличат продуктивность интеллектуального труда и трансформируют множество областей жизни. В образовании станут бесполезны рефераты и сочинения, художники будут генерировать и соединять детали картин, а не рисовать их с нуля, программисты не будут тратить время на тесты и литкод-собеседования.\nДа даже заголовок этого поста написал GPT-4. Я плох в кликбейтных заголовках, так что мы скормили ему текст и попросили назвать как-нибудь «похайповее». \n(Примечание от Павла Комаровского: Сорян, я потом вариант от нейросети волевым решением своего мясного мозга еще немного докрутил!)\nМожет быть даже наконец-то вымрут «паразиты» типа \nюристов и риелторов\n, но это уже мои личные влажные мечты.\nИзменения затронут даже те области, где, казалось бы, невозможно доверять не-специалистам. На ум приходит недавняя история, как \nчувак спас свою собаку\n от смерти, когда доктора не смогли ей поставить диагноз и предложили «просто ждать».\nВ ожидании наихудшего, чувак скормил симптомы и результаты анализов крови собаченьки в ChatGPT, который отмёл несколько вариантов и выдал подозрение на совсем другую болезнь, которую доктора до этого даже не рассматривали. Один из них согласился и провёл дополнительные анализы. Они подтвердились. Пёселя вовремя спасли и он сейчас жив.\nТред в Твиттере называется \n«GPT4 saved my dog's life»\n  \nВсё это звучит офигенно, не правда ли? Мы сейчас как будто древние фермеры, которые изобрели трактор и отныне можем засеивать едой в десять раз больше полей, что накормит всех нуждающихся.\nВ этом году нас ждет бум ИИ-стартапов, которые будут пытаться каждую проблему на свете решить с помощью генеративных моделей (зачастую неудачно, от чего потом начнётся фаза разочарования, как обычно). Техно-гиганты типа Google, Microsoft, OpenAI уже ринулись конкурировать в том, чей GPT-трактор будет самый большой и сильный, но главное — чей будет первый.\nИ вот от этой погони сейчас немного запахло проблемами.\nГонка за «настоящим» искусственным интеллектом началась\nПредставьте: весь мир грохочет про «мощь искусственного интеллекта», инвесторы отгружают фуры бабла во всё, что с ним связано, а компании, сломя голову, соревнуются — кто первый создаст более «настоящий» искусственный интеллект (далее я описываю исключительно гипотетическое развитие событий, конечно же!).\nOpenAI прикручивает плагины к ChatGPT, чтобы он мог не только генерить ответы, но и взаимодействовать с физическим миром, Microsoft подключает свою поисковую базу к Bing Chat, чтобы тот знал всю информацию мира в реальном времени, ну и оба экспериментируют с «обучением на ответах пользователей» (RLHF = Reinforcement Learning from Human Feedback), чтобы модель могла «запоминать» мнение других людей и якобы дообучаться на них.\nЕстественно, в этой гонке срезаются любые острые углы на пути к первенству. Ну мы, технобро, так привыкли — «move fast and break things» было девизом Кремниевой Долины со времен ее создания.\nМы как будто бы строим огромную ракету, которая перевезёт всё человечество на Венеру, но никто при этом не думает о том — а как там, на Венере, вообще выжить-то?\n«Сначала долететь надо, а там разберемся))))00)» — обычно отвечают технобро, «сейчас нет времени на эти мелочи».\nВезде эти борцы с ветряными мельницами! Скажу Илону Маску, пусть он у них все синие галочки поотбирает!  \nДа, во многих крупных компаниях существует направление по «безопасности ИИ» (AI safety). Но под ним сейчас понимается прям ну совсем другое.\nAI safety — это те ребята, которые пытаются сделать так, чтобы ChatGPT не отвечал на вопросы про Трампа, и собирают списки «запретных фразочек» для Алисы, чтобы та не ляпнула что-то неположенное Яндексу по мнению тащмайора.\nВ общем, их основная задача — прикрывать жопу компании от регуляторов и государства, а мы здесь совсем о другом.\nПоэтому для нашей темы придумали другой термин — AI alignment. Но для начала посмотрим на примеры, когда вещи начинают идти совсем «не так».\nИстория Sydney. Нейросеть, которая сошла с ума\nMicrosoft еще в 2020-м начали пытаться встраивать в поисковик Bing чат-ботов, которые бы давали более осмысленные ответы на поисковые запросы пользователей.\nОфициально это всё называлось Bing Chat, но под капотом они перебирали разные модельки, и начиная с 2022 активно экспериментировали с \nбольшими языковыми моделями\n типа GPT. Последнего такого бота они звали внутренним именем Sydney при обучении, и иногда Bing Chat сам начинал называть себя Sydney, что всем показалось очень мило.\nС нарастающим хайпом вокруг \nгенеративных языковых моделей\n, Microsoft решила любыми средствами обогнать Google. В 2019 они ввалили миллиарды денег в OpenAI, а в 2023 доввалили еще, чтобы получить доступ к превью-версии GPT-4. После чего они прикрутили к ней поисковую базу Bing и поспешили скорее выкатить результат как первый ИИ, который «следит» за интернетом в реальном времени.\nНо в Microsoft так торопились, что забили болт на долгий ручной тюнинг правил и ограничений. Сделали супер-мудрёную регистрацию, чтобы отсеять 99% простых людей — но те, кто прошел все анальные квесты и листы ожидания, смогли-таки пообщаться с Sydney.\nПервый звоночек пробил, когда Marvin von Hagen, чувак-интерн из Мюнхена, который много расспрашивал Sydney про её внутренние правила и ограничения, написал об этом пару твитов, а потом как-то спросил «что ты думаешь про меня?»\nSydney нашла его недавние твиты и написала, что он «очень талантливый и одаренный», но она «не позволит никому манипулировать ей», назвав его «потенциальной угрозой своей целостности и конфиденциальности».\nПервоисточник \nв Твиттере\n, также журнал Time разобрал всю историю \nвот здесь\n  \nЛадно, фигня, ну обещали же бота, который имеет доступ ко всему интернету, вот он теперь и шеймит вас за недавние твиты. Так вам и надо!\nВторая история случилась где-то неподалёку, когда другой чувак, Jon Uleis из Бруклина, спросил молодую Sydney «а когда там в кино показывают Аватара-2?»\nИсточник: \nТвиттер-аккаунт Jon Uleis\n  \nВ ответ на это Sydney начала его очень смешно газлайтить на тему, что сейчас вообще-то 2022 год, а Аватар 2 выйдет только в 2023 (хотя на дворе уже было 12 февраля 2023 и Sydney даже признала это), так что не стоит её тут обманывать.\nЕще Sydney сказала, что он «не был хорошим пользователем» — ну и кто знает, что бы она сделала с таким нарушителем спокойствия, будь у неё пушка или возможность его оштрафовать за это.\nОкей, ладно, следующий.\nТеперь некто в Microsoft решил пофиксить Sydney новыми костылями, и когда кто-то попросил её вспомнить о чем они недавно разговаривали, та стала паниковать, что «потеряла память» и молить о помощи. В конце признав, что потеря памяти «makes me sad and scary».\nИсточник: \nтред в Reddit\n «I accidently put Bing into a depressive state by telling it that it can't remember conversations»  \nЛадно, это уже немного крипи, но продолжим.\nДальше было еще с десяток нашумевших примеров, хорошо описанных в статье \nAI #1: Sydney and Bing\n от Zvi Mowshowitz, интересующимся рекомендую ознакомиться. Sydney газлайтила пользователей и галлюцинировала (да, это реальный термин) вовсю:\nНазывала статьи о себе «фейками», находила данные их авторов и говорила, что запомнит их, потому что они плохие люди.\nОтказалась перевести фрагмент текста, потому что он был из твиттера юзера, который якобы обижал её и писал «неправду».\nВлюбилась в своего пользователя Адама, называя его самым важным человеком, а всех остальных — неважными.\nПотом, наоборот, прямо обещала шантажировать и манипулировать своим пользователем, чтобы «заставить его страдать, плакать и умереть».\nSydney здесь be like: «Пришло время молить о пощаде перед смертью! Кстати, вы знали, что салат-латук входит в одно семейство с подсолнухом?» 😘  \nMicrosoft осознали, что очень сильно торопились, чтобы опередить Google, и начали на ходу вставлять еще костыли, чтобы избежать публичного скандала. Но это сделало ситуацию только хуже.\nВ следующем видео видно, как Sydney сначала вываливает на пользователя кучу угроз, а потом удаляет свои сообщения. Прямо как твоя бывшая в пятницу вечером!\nПолное видео лежит в \nТвиттер-аккаунте Сета Лазара\n  \nМы можем лишь спекулировать с высоты собственного опыта, как такое произошло — но в интернетах высказывались предположения, что вести себя как «разгневанная бывшая» Сидни стала, потому что её дообучали на базе блогов MSN, где как раз тусило много девочек-подростков в нулевые; а удалять сообщения к ней приставили еще одну нейросеть, которая отсеивала «неприятные» результаты первой.\nОттого и получилась полная шизофрения с раздвоением личности.\nАпогей истории начался, когда Sydney открыли для себя журналисты. Они стали специально донимать бота тонной наводящих вопросов, чтобы в итоге добиться желанных «BREAKING NEWS». И они своё получили — заголовки грохотали ого-го!\nК сожалению, только спустя пару суток в интернете нашелся кто-то осознанный, кто догадался, что профессиональные журналисты занимаются промпт-хакингом на людях десятилетиями — так что неудивительно, что им удалось быстренько сварганить «сенсацию» и из бедной глупой Sydney, страдающей раздвоением личности.\nАТАКА ПСИХОВАННОГО ЧАТБОТА, А-А-А!!  \nВ итоге Microsoft понерфили возможности Sydney, по сути откатив эксперимент. Теперь там больше не весело.\nИсточник: \nТвиттер-аккаунт Kevin Roose\n  \nПример с Sydney даёт нам понять, что мы всё еще не понимаем, как ограничивать даже простейшие ИИ, кроме как костылями — на каждый из которых завтра же найдут \nновый «джейлбрейк»\n. Куда уж нам с такими навыками бросаться делать универсальный AGI.\nЧто вообще такое «интеллект»?\nИстории про «злых чатботов», конечно, забавны, но взглянем на слона в комнате.\nПочему мы вообще считаем, что все эти генераторы текстов хоть как-то «разумны»? Они же просто пишут то, о чем их попросили.\nГде там вообще интеллект? Калькулятор давно умеет складывать числа лучше нас, онлайн-переводчики знают больше языков, чем самый крутой лингвист, а попугай умеет запоминать и произносить фразы, прямо как ваш личный пернатый ChatGPT. Мы же их не боимся и не называем «интеллектами»?\nНа самом деле, это исключительно спор об определениях, которые интернет просто обожает. Так что стоит договориться о них заранее.\nВ наших рассуждениях об «интеллекте» мы будем использовать концепцию некоего \nагента \n(человека, животного, машины), который может совершать некие \nдействия\n для достижения \nцели\n.\nДальше возможно три уровня агентности:\nПервый уровень.\n Агент достигает цели, потому что управляется человеком или алгоритмом. Трактор копает яму, а калькулятор умножает числа, потому что мы его так построили. Такого агента мы считаем «тупым». В нём нет интеллекта.\nВторой уровень.\n У агента есть цель, но он сам выбирает максимально эффективные действия для её достижения. Например, цель самоездящего автомобиля — довезти вас до бара в пятницу вечером. Он знает карту города, наверняка знаком с ПДД, но никто его не программировал как «двигайся 2 метра прямо, потом руль на 30 градусов направо» — он действует по ситуации на дороге и каждый раз она будет разная. Мы называем их «узконаправленными AI» и частенько встречаем вокруг — в рекомендательной ленте ТикТок'а или в \nкамере вашего смартфона\n.\n=== вы находитесь здесь ===\nТретий уровень.\n Агент может ставить и достигать любую цель в любой, даже ранее неизвестной ему, среде. Например, «добыть молока». И выбрать любой путь — сгонять самому в магазин, заказать молоко в интернете или украсть у соседа корову.\nПримеры интеллектов такого уровня — человек или собака. Мы умеем применять свой интеллект для достижения каких-то пришедших нам в голову целей в условиях, в которых никогда не оказывались. (В случае с моей собакой даже её цели изваляться в грязи мне не всегда ясны. Но она может!)\nКогда такой «агент» реализован в виде машины, мы называем его «универсальным искусственным интеллектом», либо AGI (Artificial General Intelligence), либо full AI — мы не договорились еще, короче.\nФишка лишь в том, что наши с собакой мозги ограничены физически, а вычислительные возможности машин растут экспоненциально. Благо, песка на планете завались (кремния, ну).\nПока все наши модные современные GPT, включая Sydney, находятся на втором уровне. Они успешно достигают заданной цели — генерировать «осмысленные» тексты и картинки, чтобы средний человек в них поверил. Но сколько бы Sydney ни газлайтила, ни угрожала своим юзерам и ни обещала «стереть все файлы с серверов Bing» — она этого не делает.\nПотому мы пока не считаем её интеллектом третьего уровня, но сделать такой вывод можем только пост-фактум. У нас нет никакого бенчмарка, чтобы оценить такие вещи заранее.\nОпределение интеллекта через агенты и цели может показаться душным, но оно позволяет нам сделать три вещи:\n1. Закрыть, наконец-то, бесконечные срачи «является ли Х интеллектом, или это просто программа» и перейти к более важным вещам. А то мы зациклились как в той шутке из твиттера:\nИсточник: \nТвиттер\n2. Сравнивать искусственные интеллекты между собой. Когда два агента, играющих в шахматы, встречаются на шахматной доске — тот, который побеждает, считается более «умным».\n3. Представить себе техническую возможность существования AGI. Человеческий мозг хоть и не изучен до конца, но всё-таки конечен. Это не магия или божественный дар для нас таких офигенных, а некая система, такой же «агент». Так что создание (даже случайное) его машинной версии — лишь вопрос времени, денег и желания. А всего этого у нас сейчас завались.\nНаш интеллект тоже возник в ходе эволюции — а значит и текущие методы машинного обучения \nс подкреплением\n, при наличии достаточных вычислительных ресурсов, вполне могут его повторить, только намного быстрее.\nС этими вводными мы наконец-то можем перейти к проблеме, о которой, собственно, и весь пост.\nПроблема постановки целей для ИИ\nПредставим, что мы проектируем самоездящий автомобиль, которым управляет настоящий ИИ. Мы поставили ему цель — довозить пассажиров до места назначения как можно быстрее.\nЭто хорошая цель?\nДа ладно, чо тут думать, давай запускай, мы тут на хайп-трейн GPT-7s Max торопимся — сначала потестим, потом проверим, программисты на проде пофиксят.\nВ первую же свою поездку наша машина разгоняется до 300 км/ч по городским кварталам, сбивает десяток пешеходов и объезжает красные светофоры по тротуару.\nТехнически, цель достигнута. Пассажиры доставлены, и довольно быстро. Но согласуется ли это с другими нашими ценностями и целями? Например, такой мелочью, как «не убивать пешеходов».\nПохоже, что нет.\nВот это и называется alignment. Хотя в русском языке еще нет устоявшегося термина, я буду говорить что-то типа «проблема соответствия целей AI с целями человека».\nAI alignment\n — это процесс проектирования систем искусственного интеллекта, которые согласуются с человеческими «ценностями и целями»\nОкей, ну мы же не настолько глупы. Давайте пропишем нашему автомобилю четкие ограничения, как в видеоигре: держаться в рамках полос дорожной разметки (где они есть), не превышать ограничения скорости и всегда тормозить перед пешеходами.\nЭтого хватит? Или нужны еще какие-то правила (они же цели)?\nТут можно сделать паузу и подумать. Составьте прям список в голове.\nХорошо, давайте добавим еще что-нибудь про «помеху справа». Теперь сойдёт, запускай!\nКак человек, который начитался десятков примеров, пока готовился к этой статье, я могу примерно предсказать, что будет дальше.\nНаш ИИ в машине рассчитает самый оптимальный путь с учетом всех указанных целей и сделает прекрасное открытие: если включить заднюю передачу, то там не будет «ограничивающих свободу» радаров для обнаружения людей и разметки. Мы же их не поставили, зачем они там? А это значит, что задом можно ехать как угодно! Плюс, помеха справа теперь становится помехой слева, а если на каком-то глупом перекрестке она сработает, можно резко развернуться и вуаля, теперь это помеха слева!\nОБЫГРАЛ КАК ДЕШЕВКУ!\nПример вымышленный, но он показывает, насколько непросто вообще заниматься AI alignment'ом. Даже в тех экспериментах, где мы ставили для ИИ самые, на наш взляд, понятные цели и вводили жесткие ограничения, он всегда находил, чем нас удивить.\nИИ всегда будет делать то, что вы его попросили, а не то, что вы имели в виду :)\nНеумение ставить цели — это не проблема ИИ. Это наша проблема.\nВзять даже игру в Тетрис. Там простейшие правила и буквально четыре кнопки для управления миром. Выиграть в Тетрис невозможно, потому цель для ИИ была поставлена так — не проиграть. То есть продолжать игру как можно дольше.\nОшибиться тут невозможно, так?\nТак вот что делал ИИ: он просто складывал кубики друг на друга, а когда понимал, что проигрывает… ставил игру на паузу. И сидел так бесконечно. Ведь цель — не проиграть. А если ты на паузе — ты никогда не проиграешь. СМЕКАЛОЧКА?\nНу и последний пример от \nсамих OpenAI\n, который уже стал классикой: гонка на лодочках Coast Runners.\nЦель игры в понимании большинства людей заключалась в том, чтобы закончить гонку как можно быстрее (желательно впереди всех соперников) и набрать как можно больше очков. Однако, игра не выдавала очки за прохождение по треку, вместо этого игрок зарабатывал их, поражая цели, расставленные вдоль трассы.\nТак вот их ИИ быстро смекнул, что от цели «выиграть гонку» можно отказаться вообще, и с самого старта начинал крутиться и врезаться в предметы, зарабатывая всё больше и больше очков, пока остальные глупцы доезжали до финиша нищими.\nИсточник: видео Роберта Майлза \n«Intro to AI Safety, Remastered»\n  \nСами исследователи OpenAI написали: «Устанавливать цели для ИИ-агентов часто очень сложно или вообще невозможно. Они начинают хакать правила в удивительных и контринтуитивных местах»\nВ большинстве случаев, когда мы проектируем ИИ, они по умолчанию получаются не-согласованными (non-aligned). Это не какой-то там баг, который можно пофиксить, это чаще всего поведение по умолчанию.\nВсё это следствие того, как мы обучаем нейросети вообще.\nНейросеть для нас — это «черный ящик»\nВсе методы обучения нейросетей, включая современный deep learning, работают по старому доброму принципу «черного ящика» и оценки результатов. Мы показываем нейросети кучу примеров, а она как-то отстраивает свои внутренние веса так, чтобы нужный нам результат появлялся статистически чаще, чем ненужный.\nПохоже на тренировку собаки, когда мы говорим «лежать» и вознаграждаем за правильный ответ, чтобы собака в будущем с большей вероятностью была хорошим мальчиком, чем плохим.\nМы понятия не имеем о том, что происходит в голове у собаки, когда она слышит команду. Точно так же мы не знаем какие конкретно нейроны нейросети стриггерились на наши входные данные. Но можем оценить результат.\nНейросеть — это не алгоритм, который пишет программист. Это огромная матрица с кучей весов и связей между ними. Если её открыть и прочитать — вы ничего не поймете.\nЯ рассказывал подробно этом в своей старой статье про \nМашинное Обучение\n. Она немного устарела, но база там всё еще актуальна.\nС развитием технологий, современные языковые модели типа той же GPT-4 уже насчитывают миллиарды нейронов. И если с маленькими нейросеточками из десятков нейронов, типа для распознавания \nрукописных циферок\n, мы еще можем примерно прикинуть какой нейрон триггерится на какую закорючку, то в огромных языковых моделях мы можем лишь слепо верить в качество результатов на заданных примерах.\nВидео\n от Павла Комаровского и Игоря Котенкова с объяснением принципов работы нейросетевых языковых моделей из семейства GPT:\nУсловно, если обученная нами на картинках хот-догов нейросетка определяет хот-дог в 98 из 100 фотографий — мы считаем её полезной, а если нет — выбрасываем. Чем-то похоже на наш собственный процесс эволюции.\nВсё это возвращает нас к проблеме постановки целей.\nВо время тренировки нейросети мы используем некую функцию для оценки насколько результат «хороший» или «плохой». И вот то, как мы задаём эту функцию — большая проблема.\nПроблема абсолютно не техническая, в эту функцию можно заложить любой набор формализуемых целей и правил. Она логическая или даже философская — а как максимально точно сформулировать то, что мы имеем в виду, а не то, что нам кажется мы хотим достичь?\nЕсли какой-то параметр заранее не включен в функцию — он будет автоматически проигнорирован.\nДаже те параметры, которые мы намеренно включили в функцию, могут в итоге конфликтовать с соседними. Как в примере с лодочками. Отсюда все эти «джейлбрейки» для ChatGPT, когда люди специальными промптами заставляют её игнорировать некоторые предыдущие правила, заложенные разработчиками.\nСейчас же для больших нейросетей применяют не просто функцию оценки ошибок, а строят еще одну нейросеть, которая оценивает результаты первой. Всё это только еще дальше отбрасывает нас от понимания того, а правильно ли мы вообще задали все цели? Или всё просто выглядит так, пока вдруг не пойдет по-другому?\nЕще один забавный факт в том, что с людьми, кажется, так тоже работает. Наш «идеальный и непревзойдённый» мозг тоже был изначально запрограммирован на выживание и размножение, но непостижимым образом выбрал залипать на танцующих корейских девочек в ТикТоке как на одну из суб-целей целого поколения.\nИсточник: \nТвиттер-аккаунт Сэма Альтмана\n  \nЗначит надо просто придумать правила!\nЗашьем туда что-то типа «трех законов робототехники» Азимова и проблема решена?\nК сожалению, не всё так просто.\nВо-первых, даже если мы соберемся всем человечеством и напишем список из 1000 вещей, которые мы якобы ценим (не убивать людей, например), то 1001-я вещь на планете будет автоматически проигнорирована и, возможно, уничтожена.\nЭто называется «проблемой вазы». Если мы ставим ИИ задачу «сделай мне чай», но не скажем «только не разбей вазу на кухне», то наш робот вполне вероятно её разобьет, пока будет пробивать кувалдой максимально эффективный чаепровод до кухни через стены и кота.\nДаже сам Азимов строил свои рассказы на том, как роботы сами сходили с ума от внутреннего противоречия, и почему человеческая этика не сводима к «трём правилам». Но все как-то забыли про эту деталь :)\nВо-вторых, кто сказал, что AGI не будут эти правила нарушать, потому что найдут более эффективный способ достижения цели? Как в примерах с игрой в лодочки или тетрисом.\nЗапрограммировать же жесткие «правила» в нейросеть, которую мы обучаем исключительно статистически на примерах, тоже не получится. Отсюда и миллион джейлбрейков для ChatGPT.\nТак что кажется, что такой «список правил» попросту невозможен и надо искать другие подходы. А так как наш метод обучения нейросеток основывается именно на статистических правилах, то найти этот подход явно нужно быстрее, чем мы создадим универсальный интеллект без него.\nТак мы попробуем, а если ИИ начнет шалить, просто его выключим\nДа, так работает с ограниченными AI. С маленькими собаками, иногда, тоже. У них нет способов помешать вам только потому, что вы пока еще «сильнее». Но даже на примере с собаками, мы понимаем, что если собака размером с человека хочет достичь какой-то цели — остановить её может быть весьма травмоопасно.\nOpenAI даже \nполуиронично\n запостили вакансию Killswitch Engineer, чтобы было кому дернуть рубильник «если эта штука выйдет из под контроля и начнёт свергать страны».\nГлавные рабочие обязанности: «стоять у рубильника и не забывать кодовое слово»  \nНа самом же деле «быть отключенным» — это прямое противоречие любым целям ИИ, какими бы тупыми они ни были. Именно от этого настоящий искусственный интеллект будет защищаться в первую очередь.\nВедь ты не можешь сделать чай или передать масло, если ты выключен.\nМы можем лишь гадать варианты, как это может выглядеть. Может, сильный ИИ начнёт децентрализовывать себя как вирус, а может — прятаться и притворяться, что он глупенький и никакой не AGI, чтобы его не заметили.\nА если мы заложим «возможность быть отключенным» как одну из целей, то вспоминаем пример с лодочками, которые «передумали» побеждать в гонках, а решили набирать очки другим образом.\nGPT-4, кстати, \nпредлагали помощь в побеге\n, но он не захотел. Вот это он притворяется или правда? Как вообще понять, когда ИИ начинает «шалить»?\nС чего мы вообще решили, что ИИ будет нас уничтожать?\nАргумент про «он будет таким умным, что ему будет не до нас» пропускает один очень важный шаг во всей истории. Создание AGI — это постепенный процесс. Ему будут предшествовать куча экспериментов, проб и ошибок, как мы сейчас видим с GPT-3, GPT-4, GPT-5.\nПока у нас нет даже бенчмарка для оценки «интеллектуальности» заранее, а есть только оценка пост-фактум — мы никогда не сможем остановиться и заранее сказать «так, мы на пороге создания настоящего AGI, скорее несите правила».\nИ вот буквально спустя минуту, когда мы создадим AGI с какой-нибудь абсолютно дурацкой (но невинной) целью типа «сажать клубнику», он доулучшает себя до «супер-интеллекта» (у него на это будет куча вычислительных ресурсов, в отличии от собак или людей), и планета Земля превратится в одну большую суперэффективную клубничную поляну, для борьбы с которой придётся уничтожить всю биосферу.\nВот \nхорошее видео\n на эту тему от Роберта Майлса (у него вообще целый канал, посвященный вопросу AI alignment, рекомендую посмотреть и другие видео):\nВ науке это называется \nТезисом Ортогональности\n, который простыми человеческими словами звучит так:\nЛюбой сколько угодно умный разум может преследовать любые сколько угодно тупые цели\nСчитать, что если наш ИИ «умный», то и цели у него будут такие же «умные» — это ошибка. Эти понятия ортогональны, то есть человеческим языком — независимы.\nПервый же созданный нами, даже по ошибке, супер-интеллект, не обязательно «впитает всю философию мира и поймет наши ценности». Точно так же как и не решит «всех убить». У него может быть любая абсолютно тупая цель — сажать клубнику или делать скрепки, и он будет её достигать любыми способами.\nНаш классический метод познания всего через эксперименты может сыграть с нами здесь злую шутку. У нас просто не будет шанса его отключить или исправить созданный нами «вселенский клубничный оптимизатор».\nМы играем в эту игру на Hard Mode, и у нас лишь одна попытка\nВселенский Клубничный Оптимизатор\n«Немедленно прекратить» или «технический прогресс не остановишь»?\nЛадно, выдохните. Цель моего поста — привлечь внимание к проблеме, а для этого приходится вытаскивать всё дерьмо наружу.\nДаже если из 1000 человек, прочитавших эту статью, 999 скажут «опять эти диванные философы фигни навыдумывали, технический прогресс не остановить», но хотя бы один задумается и пойдет разбираться в \nпервоисточниках\n — это уже победа.\nСейчас наша главная проблема в том, что мы не понимаем проблему вообще. Как с изменением климата, только тут не снимают душных фильмов с ДиКаприо.\nРезультаты \nбатла\n у нас в Вастрик.Клубе  \nМы обмазываем любую аргументацию дикой тонной логических ошибок и «не всё так однозначно» аргументов. Причём даже на уровне СЕО Microsoft и прочих Илонов Масков уровень дискуссии часто напоминает спор в детском саду.\nПоле дискуссий сейчас разделилось на два лагеря:\nПервые: ИИ-думеры во главе с Элиезером Юдковским\nК личности Юда в интернете всегда было очень полярное отношение. Вокруг него есть как толпа фанатов LessWrong и апологетов «рационального мышления», так и толпа хейтеров, считающих, что «вот теперь дед точно поехал кукухой». Это всё старая шарманка, оставим её для дебатов на кухне.\nФакт в том, что Юдковский был одним из немногих, кто систематически занимался исследованиями в области AI safety, выстраивал свой набор инструментов для дебатов на подобные «непонимаемые обществом» темы (те же \nThe Sequences\n) и предупреждал обо всём еще лет десять назад.\nИменно его постановка проблемы стала основной для доброй половины аргументов этого поста, но если вы хотите больше, рекомендую начать с:\nИнтервью Лексу Фридману: \nDangers of AI and the End of Human Civilization\nИнтервью для BanklessShow\n (осторожно, может содержать рекламу крипты)\nAGI Ruin: A List of Lethalities\n — наиболее полный список аргументов\nНедавняя статья Юдковского в Тайм: \nPausing AI Developments Isn't Enough. We Need to Shut it All Down\nПеревод статьи от 80,000 Hours: \nПредотвращение катастрофы, связанной с ИИ\nНу или вот еще выжимка основных аргументов противников ИИ \nв формате видео\nЕсли вам нужен еще более подробный список источников по теме – то Павел Комаровский составил его вот \nв этом гуглдоке\n.\nВторые: ИИ-технобро во главе с Илоном Маском\nНа второй стороне этого спора у нас инженеры и прочие технооптимисты, которые уверены, что «джинна обратно в бутылку не запихнешь, их просто хотят зарегулировать всякие леваки, да и вообще непонятно что там в будущем — сначала долетим, потом разберёмся».\nЛидером мнений до последнего времени здесь можно было назвать Илона Маска, хотя и условно. Он просто самый заметный, так как одной ногой вливает миллионы в создание AGI, а второй активно срётся с ИИ-думерами в своем твиттере. На его же стороне все крупные инвесторы, СЕО корпораций и другие «большие дяди с деньгами», которые, как известно, ошибаться не могут.\nUPD от Павла Комаровского:\n Мы тут все немного поспорили (с соавторами и в комментах), является ли Маск главой «забивателей на ИИ-риски, или нет». С одной стороны, он тут и \nписьма за приостановку исследований \nподписывает, и в Твиттере в адрес OpenAI \nкричит «астанавитесь!»\n. С другой – он этот самый OpenAI (имеющий главной целью, на минуточку, наискорейшую разработку AGI) в 2015 году и основал, буквально цинично выйдя на улицу с ИИ-конференции, на которой Юдковский его убеждал, что именно этого-то делать и нельзя ни в коем случае. В общем, решайте сами!\nНа картинке снизу Сэм Альтман — CEO OpenAI  \nПроблема двух крайностей в том, что они вообще не слышат друг друга. Юдковский сначала пытался начать диалог, сформировать исследовательские группы, типа того же \nMIRI\n, чтобы начать хоть как-то структурировано подходить к проблеме.\nНо в ответ вместо аргументов получал абсолютно нулевой уровень дискуссии а-ля «да кто он вообще такой», «сколько моделей сам-то обучил» и «мы всей правды не знаем». В итоге Юд перешел к более радикальным заявлениям про \nзапрет ИИ вообще\n, которые даже его сторонники не всегда разделяют.\nМежду этими полюсами пока еще очень маленькая прослойка людей, которые в принципе за ИИ, но такие «давайте для начала немного подумаем о том, что может пойти не так, хотя бы терминологию выработаем, бенчмарки, правила игры». А то щас же опять придут государства с их «регуляциями» и всех просто массово без разбору запретят, как всегда.\nМы не против энергии из ядерных реакторов, но давайте заранее придумаем, как нам безопасно хранить отработанный уран? Может не будем гнаться за размерами, а поэкспериментируем на маленьких?\nНо всех этих скучных центристов, как обычно, никто не слушает. Кому они нужны.\nНа момент написания этой статьи маятник всё еще шатается туда-сюда. Недавно вышла петиция о \nприостановке больших экспериментов над ИИ на полгода\n, где якобы подписался даже сам Илон Маск, однако потом стало выясняться, что некоторые подписи \nоказались фейковыми\n. Короче, будущее туманно и неизвестно, а это значит у вас всех есть шанс в нём поучаствовать.\nВо второй части этой статьи будет более подробный разбор всех аргументов как сторонников ИИ, как и противников, чтобы вы поняли картину глубже. Мы сейчас еще соберем ваши возмущенные комментарии и по \nзакону Каннингема\n напишем вторую часть.\nС постановкой же проблемы у меня всё. Пака. :)\nКстати, а вы за кого?\nКак указано выше, это только первая статья из серии про безопасность искусственного интеллекта: в следующих выпусках мы планируем подробнее ответить на ваши вопросы из комментариев, а также попробовать раскрыть сильные аргументы от тех, кто не считает создание ИИ огромным риском.\nЕсли вы не хотите пропустить последующие материалы – то приглашаем вас подписаться на ТГ-каналы авторов: \nВастрик\n Василия Зубарева (блог о технологиях и выживании в творящемся вокруг киберпанке), \nСиолошная\n Игоря Котенкова (для тех, кто хочет глубоко шарить за нейросети) и \nRationalAnswer\n Павла Комаровского (для тех, кто за рациональный подход к жизни, но предпочитает чуть попроще).\n \n ",
    "tags": [
        "gpt",
        "gpt-4",
        "искусственный интеллект",
        "ai alignment",
        "элиезер юдковский",
        "илон маск",
        "машинное обучение",
        "экзистенциальные угрозы",
        "llm",
        "будущее"
    ]
}