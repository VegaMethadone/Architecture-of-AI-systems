{
    "article_id": "725706",
    "article_name": "Разворачиваем Kubernetes-кластер на bare metal под управлением Deckhouse и создаем в нем виртуальные машины",
    "content": "В новом релизе Kubernetes-платформы Deckhouse v1.43 \nпоявилась система виртуализации\n, основанная на современных технологиях: KubeVirt, Cilium, LINSTOR. Она позволяет в удобном и привычном для пользователя платформы режиме запускать виртуальные машины и управлять их жизненным циклом.\nВ статье мы рассмотрим это на практике: развернем Deckhouse на bare-metal-сервере, запустим в нем виртуальную машину и протестируем, как обеспечивается связь между компонентами кластера с помощью магии Cilium.\nВиртуализация в кластере\nС новой системной виртуализации появилась возможность запускать привычные виртуальные машины (ВМ) прямо в Kubernetes и работать с ними как с ресурсами кластера. \nЭто может быть полезно, если для работы приложений требуются какие-то специфические инструменты, работа которых невозможна в K8s. А также в случае перехода от старой инфраструктуры с ВМ к контейнеризации приложений. В этом случае можно развернуть одну часть приложения в кластере, а другую поднять рядом в отдельной ВМ, созданной средствами Deckhouse и объединенной с кластером одной сетью.\nПодготовка сервера\nДля установки нам понадобится сервер, соответствующий минимальным требованиям:\nне менее 4 ядер CPU;\nне менее 8 Гб RAM;\nне менее 40 Гб дискового пространства;\nHTTPS-доступ к хранилищу образов контейнеров registry.deckhouse.io;\nна сервере не должно быть установлено пакетов container runtime, например containerd или Docker.\nТакже понадобится персональный компьютер, с которого будет производиться установка. Он должен соответствовать следующим требованиям:\nОС: Windows 10+, macOS 10.15+, Linux (Ubuntu 18.04+, Fedora 35+);\nустановленный Docker для запуска инсталлятора Deckhouse (инструкции по установке для \nUbuntu\n, \nmacOS\n, \nWindows\n);\nHTTPS-доступ к хранилищу образов контейнеров registry.deckhouse.io;\nSSH-доступ по ключу к серверу, который будет master-узлом будущего кластера.\nВ качестве сервера для тестов у нас будет 1U-сервер Supermicro со следующими характеристиками:\n2 процессора Intel(R) Xeon(R) CPU E5620 @ 2.40GHz, 16 ядер;\n40 Гб оперативной памяти;\n256 Гб SSD под корень ОС;\n320 Гб на отдельном диске для хранения данных виртуальных машин.\nВ первую очередь установим ОС, выбрав подходящую из списка поддерживаемых:\nРЕД ОС 7.3;\nAlterOS 7;\nAstra Linux Special Edition 1.7.2;\nCentOS 7, 8, 9;\nDebian 9, 10, 11;\nRocky Linux 8, 9;\nUbuntu 18.04, 20.04, 22.04.\nВ нашем случае это будет \nUbuntu Server 22.04.2 LTS\n. Устанавливаем ее в обычном режиме, не разбивая диск на разделы и не выбирая в конце установки никакого дополнительного софта, кроме OpenSSH-сервера.\nЗагружаемся в свежую ОС и настраиваем доступ по ключу с основного ПК:\n$ ssh-copy-id <IP-адрес сервера>\nКоманда не сработает, если ключ не был заранее сгенерирован. Сгенерировать его можно командой \nssh-keygen -t rsa\n.\nПодключимся к серверу, чтобы убедиться, что все настроено как нужно:\n$ ssh 192.168.2.38\nWelcome to Ubuntu 22.04.2 LTS (GNU/Linux 5.15.0-60-generic x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n\n  System information as of Wed Mar  1 11:23:13 AM UTC 2023\n\n  System load:  0.0                Temperature:             46.0 C\n  Usage of /:   2.7% of 292.35GB   Processes:               135\n  Memory usage: 2%                 Users logged in:         0\n  Swap usage:   0%                 IPv4 address for enp3s0: 192.168.2.38\n\n\n * Introducing Expanded Security Maintenance for Applications.\n   Receive updates to over 25,000 software packages with your\n   Ubuntu Pro subscription. Free for personal use.\n\n     https://ubuntu.com/pro\n\nExpanded Security Maintenance for Applications is not enabled.\n\n0 updates can be applied immediately.\n\nEnable ESM Apps to receive additional future security updates.\nSee https://ubuntu.com/esm or run: sudo pro status\n\n\nLast login: Wed Mar  1 10:34:01 2023 from 192.168.2.35\nВсе работает. Отключаемся от сервера, выполнив команду \nexit\n или нажав сочетание клавиш \nCtrl + D\n.\nУстановка Deckhouse на сервер\nКонфигурация будущего кластера\nПерейдем \nна страницу конфигурации\n будущего кластера в разделе Getting Started официального сайта Deckhouse. Здесь нужно указать шаблон доменных имен для веб-интерфейсов будущих компонентов кластера, таких как Grafana или Kubernetes Dashboard:\nШаблон имеет формат \n%s.example.com\n, где \n%s\n — имена доменов системных приложений кластера.\nНажмем кнопку \nДалее: Установка\n.\nНа следующей странице отобразится сгенерированное содержимое для файла конфигурации \nconfig.yml\n. Введенный ранее шаблон доменных имен появится в секции \npublicDomainTemplate\n:\n# Cекция с общими параметрами кластера (ClusterConfiguration).\n# Используемая версия API Deckhouse Platform.\napiVersion: deckhouse.io/v1\n# Тип секции конфигурации.\nkind: ClusterConfiguration\n# Тип инфраструктуры: bare metal (Static) или облако (Cloud).\nclusterType: Static\n# Адресное пространство Pod'ов кластера.\npodSubnetCIDR: 10.111.0.0/16\n# Адресное пространство для Service'ов кластера.\nserviceSubnetCIDR: 10.222.0.0/16\n# Устанавливаемая версия Kubernetes.\nkubernetesVersion: \"1.23\"\n# Домен кластера.\nclusterDomain: \"cluster.local\"\n---\n# Секция первичной инициализации кластера Deckhouse (InitConfiguration).\n# Используемая версия API Deckhouse.\napiVersion: deckhouse.io/v1\n# Тип секции конфигурации.\nkind: InitConfiguration\n# Секция с параметрами Deckhouse.\ndeckhouse:\n  # Используемый канал обновлений.\n  releaseChannel: Stable\n  configOverrides:\n    global:\n      modules:\n        # Шаблон, который будет использоваться для составления адресов системных приложений в кластере.\n        # Например, Grafana для %s.example.com будет доступна на домене grafana.example.com.\n        publicDomainTemplate: \"%s.example.com\"\n    # Включить модуль cni-cilium.\n    cniCiliumEnabled: true\n    # Конфигурация модуля\n    cniCilium:\n      # Режим работы туннеля.\n      tunnelMode: VXLAN\n---\n# Cекция с параметрами bare metal кластера (StaticClusterConfiguration).\n# Используемая версия API Deckhouse.\napiVersion: deckhouse.io/v1\n# Тип секции конфигурации.\nkind: StaticClusterConfiguration\n# Список внутренних сетей узлов кластера (например, '10.0.4.0/24'), который\n# используется для связи компонентов Kubernetes (kube-apiserver, kubelet...) между собой.\n# Если каждый узел в кластере имеет только один сетевой интерфейс,\n# ресурс StaticClusterConfiguration можно не создавать.\ninternalNetworkCIDRs:\n- '192.168.2.0/24'\nЗдесь нужно обратить внимание на два момента:\nВ последней секции сгенерированного конфигурационного файла — \nStaticClusterConfiguration\n — нужно указать сеть, в которую смотрит основной сетевой интерфейс сервера, т. к. на борту их несколько. \n☝️ \nЕсли на вашей машине один сетевой интерфейс — эту секцию можно удалить\n.\nМодуль, реализующий CNI, заменен с \ncni-flannel\n на \ncni-cilium\n. Это необходимо для работы виртуальных машин — их сетевое взаимодействие организовано на базе \nCilium\n. Также у него указан параметр работы туннелей \ntunnelMode: VXLAN\n.\nСохраним полученное содержимое в файле \nconfig.yml\n, положив его в любой отдельный каталог.\nУстановка Deckhouse\nДля установки платформы используется специальный подготовленный Docker-образ, в который необходимо передать созданный конфигурационный файл и SSH-ключи доступа на master-узел.\nПерейдем в каталог с созданным файлом и выполним команду:\ndocker run --pull=always -it -v \"$PWD/config.yml:/config.yml\" -v \"$HOME/.ssh/:/tmp/.ssh/\" registry.deckhouse.io/deckhouse/ce/install:stable bash\nЧерез некоторое время образ скачается из хранилища, запустится, и в терминале будет отображено приглашение командной строки внутри созданного контейнера:\n[deckhouse] root@9134e1cd790b / #\nВыполним команду:\ndhctl bootstrap --ssh-user=<username> --ssh-host=<master_ip> --ssh-agent-private-keys=/tmp/.ssh/id_rsa \\\n  --config=/config.yml \\\n  --ask-become-pass\nНа запрос пароля sudo введем пароль пользователя на сервере.\nЕсли sudo на сервере настроен таким образом, чтобы не запрашивать пароль, не вводите параметр \n--ask-become-pass\n.\nУстановка может занять от 15 до 30 минут, в процессе будет отображаться подробный лог. По завершении мы снова увидим приглашение командной строки контейнера.\nФинальная настройка кластера\nОсталось выполнить несколько шагов, чтобы кластер был готов к работе.\nДобавим конфигурацию kubectl для обычного пользователя, чтобы не вызывать каждый раз sudo:\n$ mkdir ~/.kube\n$ sudo cat /etc/kubernetes/admin.conf >> ~/.kube/config\nУбедимся, что теперь вызов команды доступен без повышенных привилегий:\n$ kubectl get no\nNAME                  STATUS   ROLES                  AGE     VERSION\nvirtualization-habr   Ready    control-plane,master   3d20h   v1.23.15\nТак как наш кластер состоит из одного узла, нужно снять с него taint. Для этого выполним на сервере команду:\nkubectl patch nodegroup master --type json -p '[{\"op\": \"remove\", \"path\": \"/spec/nodeTemplate/taints\"}]'\nЕсли ваш кластер будет состоять из нескольких узлов, вместо снятия taint'а \nдобавьте в кластер дополнительные узлы\n.\nПосле этого нужно подождать несколько минут, пока Deckhouse переконфигурируется. Проверить статус можно командой:\nkubectl -n d8-system exec deploy/deckhouse -- deckhouse-controller queue list\nВ ответ будет отображено много текста с описанием модулей, но нас интересует последняя часть:\nSummary:\n- 'main' queue: empty.\n- 68 other queues (1 active, 67 empty): 1 task.\n- total 1 task to handle.\nЕсли в строке \nqueue\n значение не \nempty\n, значит процесс еще не завершен.\nУстановка Ingress-контроллера\nСоздадим на master-узле файл \ningress-nginx-controller.yml\n в отдельном каталоге:\n$ mkdir templates\n$ cd templates\n~/templates$ vim ingress-nginx-controller.yml\nСкопируем в файл конфигурацию контроллера:\n# Cекция, описывающая параметры nginx ingress controller.\n# Используемая версия API Deckhouse.\napiVersion: deckhouse.io/v1\nkind: IngressNginxController\nmetadata:\n  name: nginx\nspec:\n  # Имя Ingress-класса для обслуживания Ingress NGINX controller.\n  ingressClass: nginx\n  # Версия Ingress-контроллера (используйте версию 1.1 с Kubernetes 1.23+).\n  controllerVersion: \"1.1\"\n  # Способ поступления трафика из внешнего мира.\n  inlet: HostPort\n  hostPort:\n    httpPort: 80\n    httpsPort: 443\n  # Описывает, на каких узлах будет находиться компонент.\n  # Возможно, захотите изменить.\n  nodeSelector:\n    node-role.kubernetes.io/control-plane: \"\"\n  tolerations:\n  - operator: Exists\nПрименим его, выполнив команду:\nkubectl create -f ingress-nginx-controller.yml\nЕсли все прошло успешно, в ответ отобразится сообщение:\ningressnginxcontroller.deckhouse.io/nginx created\nСоздание пользователя для доступа в веб-интерфейсы кластера\nЧтобы получить доступ в веб-интерфейсы кластера, необходимо создать пользователя, под которым будет выполняться вход. Скопируем содержимое в файл \nuser.yml\n:\napiVersion: deckhouse.io/v1\nkind: ClusterAuthorizationRule\nmetadata:\n  name: admin\nspec:\n  # Список учётных записей Kubernetes RBAC.\n  subjects:\n  - kind: User\n    name: admin@deckhouse.io\n  # Предустановленный шаблон уровня доступа.\n  accessLevel: SuperAdmin\n  # Разрешить пользователю делать kubectl port-forward.\n  portForwarding: true\n---\n# Секция, описывающая параметры статического пользователя.\n# Используемая версия API Deckhouse.\napiVersion: deckhouse.io/v1\nkind: User\nmetadata:\n  name: admin\nspec:\n  # E-mail пользователя\n  email: admin@deckhouse.io\n  # Это хэш пароля byul2sbqfb, сгенерированного сейчас.\n  # Сгенерируйте свой или используйте этот, но только для тестирования.\n  # echo \"byul2sbqfb\" | htpasswd -BinC 10 \"\" | cut -d: -f2\n  # Возможно, захотите изменить.\n  password: '$2a$10$pnQvwsl.gibQ1oM/z3D9g.g6o8v7dkbyqT0wFh7MsA7sUu3GhRwl6'\nОбратите внимание на раздел \npassword\n: в комментариях показан пример генерации нового пароля. Обязательно измените пароль, если готовите кластер не для ознакомительных целей!\nПрименим созданный файл в кластере:\nkubectl create -f user.yml\nВ случае успешного создания пользователя отобразится сообщение:\nclusterauthorizationrule.deckhouse.io/admin created\nuser.deckhouse.io/admin created\nПодготовка DNS-записей\nДля доступа к веб-интерфейсам кластера необходимо настроить DNS-записи, ведущие на IP-адрес master-узла кластера. Сделать это можно различными способами: настроить DNS-сервер, прописать соответствие в админ-панели хостера, предоставляющего доменное имя, или же указать нужные записи в файле \nhosts\n.\nДобавим следующие строки в \n/etc/hosts\n на рабочей машине, с которой велась установка:\napi.example.com\nargocd.example.com\ndashboard.example.com\ndeckhouse.example.com\ndex.example.com\ngrafana.example.com\nhubble.example.com\nistio.example.com\nistio-api-proxy.example.com\nkubeconfig.example.com\nopenvpn-admin.example.com\nprometheus.example.com\nstatus.example.com\nupmeter.example.com\nНе забудьте поменять \nexample.com\n на адрес, указанный в \nconfig.yml\n.\nПроверка работоспособности\nТеперь убедимся, что кластер развернут и готов к работе. Зайдем в веб-интерфейс Grafana по адресу grafana.example.com. Нас перебросит на страницу входа \nDex\n, где нужно ввести данные созданного ранее пользователя:\nПосле ввода учетных данных откроется главный дашборд Grafana:\nЗдесь отображается основная информация о компонентах кластера, а также его нагрузка.\nКластер работает. Настало время развернуть в нем виртуальные машины!\nРазвертывание виртуальных машин\nВключение LINSTOR\nДля работы с виртуальными жесткими дисками используется модуль \nlinstor\n, реализующий в кластере поддержку \nLINSTOR\n — системы для оркестрации блочных устройств хранения данных в Kubernetes. \nСоздадим файл \nlinstor.yml\n:\napiVersion: deckhouse.io/v1alpha1\nkind: ModuleConfig\nmetadata:\n  name: linstor\nspec:\n  enabled: true\nИ применим его в кластере:\n$ kubectl create -f linstor.yml\nmoduleconfig.deckhouse.io/linstor created\nПодождем пару минут и убедимся, что модуль запущен и работает:\n$ kubectl get moduleconfigs\nNAME             STATE     VERSION   AGE     STATUS\ncni-cilium       Enabled   1         3d22h\ndeckhouse        Enabled   1         3d22h\nglobal           Enabled   1         3d22h\nlinstor          Enabled   1         96s\nКонфигурация хранилища\nТеперь нужно сконфигурировать хранилище.\nДобавим alias для удобного управления LINSTOR:\nalias linstor='kubectl exec -n d8-linstor deploy/linstor-controller -- linstor'\nДля сохранения настроек пропишите эту команду в ~/.bashrc — и в новом сеансе SSH команда станет доступна сразу после входа.\nПосмотрим список всех доступных для организации хранилища дисковых устройств:\n$ linstor physical-storage list\n+----------------------------------------------------+\n| Size         | Rotational | Nodes                  |\n|====================================================|\n| 500107862016 | True       | officeserver[/dev/sda] |\n+----------------------------------------------------+\nОбратите внимание, что здесь отображаются только \nпустые\n диски!\nДля некоторых действий с хранилищем может понадобиться использование мастер-пароля, поэтому создадим Secret с ним (файл \npass.yml\n):\napiVersion: v1\nkind: Secret\nmetadata:\n  name: linstor-passphrase\n  namespace: d8-system\nimmutable: true\nstringData:\n  MASTER_PASSPHRASE: *!пароль* # Мастер-пароль для LINSTOR\nИ применим его в кластере:\n$ kubectl create -f pass.yml\nsecret/linstor-passphrase created\nТеперь создадим новый пул LVM на отдельном диске \n/dev/sda\n, найденном выше:\n$ sudo vgcreate vg0 /dev/sda --add-tag linstor-data\n  Physical volume \"/dev/sda\" successfully created.\n  Volume group \"vg0\" successfully created\nПодождем несколько минут пока пул будет создан, и убедимся, что следом за ним создался и StorageClass, в котором будут храниться диски будущих виртуальных машин:\n$ kubectl get storageclass\nNAME              PROVISIONER              RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE\nlinstor-data-r1   linstor.csi.linbit.com   Delete          Immediate           true                   2m46s\nХранилище готово! Можно создавать виртуальные машины.\nВключение модуля виртуализации\nПо умолчанию \nмодуль виртуализации\n отключен.\nСоздадим файл \nvirtualization.yml\n со следующим содержимым:\napiVersion: deckhouse.io/v1alpha1\nkind: ModuleConfig\nmetadata:\n  name: virtualization\nspec:\n  version: 1\n  enabled: true\n  settings:\n    vmCIDRs:\n    - 10.10.10.0/24\nВ параметре \nvmCIDRs\n мы указали подсеть, из которой будут выделяться IP-адреса виртуальным машинам.\nПрименим модуль на master-узле:\n$ kubectl create -f virtualization.yml\nmoduleconfig.deckhouse.io/virtualization created\nПодождем пару минут и проверим, что модуль запустился:\n$ kubectl get moduleconfigs\nNAME             STATE     VERSION   AGE     STATUS\ncni-cilium       Enabled   1         80m\ndeckhouse        Enabled   1         80m\nglobal           Enabled   1         80m\nlinstor          Enabled   1         40m\nvirtualization   Enabled   1         3m26s\nВ последней строчке состояние модуля должно быть \nEnabled\n.\nСоздание виртуальных машин\nПосмотрим перечень ОС, доступных для развертывания на создаваемых виртуальных машинах:\n$ kubectl get cvmi\nNAME           AGE\nubuntu-18.04   80s\nubuntu-20.04   80s\nubuntu-22.04   80s\nПри первом вызове команда может показать ошибку \nerror: the server doesn't have a resource type \"cvmi\"\n — нужно просто подождать и еще раз выполнить запрос.\nСоздадим файл \nhabr-vm.yml\n с описанием будущей виртуальной машины:\napiVersion: deckhouse.io/v1alpha1\nkind: VirtualMachine\nmetadata:\n  name: habr-vm\n  namespace: default\nspec:\n  running: true\n  resources:\n    memory: 8192M\n    cpu: \"4\"\n  userName: ubuntu\n  sshPublicKey: \"ssh-rsa AAAAB3NzaC1yc2EAAAADA ...\"\n  bootDisk:\n    source:\n      kind: ClusterVirtualMachineImage\n      name: ubuntu-22.04\n    size: 50Gi\n    storageClassName: linstor-data-r1\n    autoDelete: true\nЗдесь нужно указать параметры будущей виртуальной машины: количество оперативной памяти, имя пользователя, количество ядер CPU и ключ SSH для аутентификации. А также в разделе \nstorageClassName\n указать созданный ранее StorageClass.\nПрименим файл на master-узле:\n$ kubectl create -f habr-vm.yml\nvirtualmachine.deckhouse.io/habr-vm created\nПодождем некоторое время, пока машина развернется, и убедимся, что она запущена:\n$ kubectl get virtualmachines\nNAME        IPADDRESS    STATUS         AGE\nhabr-vm   10.10.10.0   Provisioning   40s\nСтатус \nProvisioning\n говорит о том, что машина успешно создана и в данный момент разворачивается. Как только процесс завершится, статус изменится на \nRunning\n.\nЕсли вы обнаружили ошибку в конфиге и хотите пересоздать ВМ, сначала удалите развернутую машину, а затем исправьте конфиг и создайте ВМ заново:\n$ kubectl delete virtualmachines master-vm\nvirtualmachine.deckhouse.io \"master-vm\" deleted\nПроверка работоспособности\nИтак, виртуальная машина развернута и успешно работает. Cilium обеспечивает сетевую связность элементов кластера таким образом, что получить доступ к созданной ВМ можно из любого места, например из Pod'а, развернутого в кластере непосредственно рядом с ВМ. \nСоздадим демо-Pod для проверки:\n$ kubectl run demo --image=ubuntu --rm -it --command -- bash\nIf you don't see a command prompt, try pressing enter.\nroot@demo:/#\nИ попробуем «достучаться» до развернутой рядом виртуальной машины. Для этого сначала установим утилиту ping, которой по умолчанию нет в образе Ubuntu, а затем «постучимся» по IP-адресу ВМ:\n/# apt update\n/# apt install iputils-ping\n\n/# ping 10.10.10.0\nPING 10.10.10.0 (10.10.10.0) 56(84) bytes of data.\n64 bytes from 10.10.10.0: icmp_seq=1 ttl=63 time=0.310 ms\n64 bytes from 10.10.10.0: icmp_seq=2 ttl=63 time=0.356 ms\n64 bytes from 10.10.10.0: icmp_seq=3 ttl=63 time=0.410 ms\n64 bytes from 10.10.10.0: icmp_seq=4 ttl=63 time=0.384 ms\n^C\n--- 10.10.10.0 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3063ms\nКак видно из лога, прямой доступ из Pod'а в кластере к ВМ есть.\nПолучение доступа к виртуальной машине извне\nСейчас виртуальная машина доступна только внутри сервера. В таком режиме можно работать, если она требуется только для развертывания отдельного сервиса. Но иногда нужен доступ к ней или ее сервисам снаружи. Предоставить его можно с помощью Service с настроенными \nexternalIPs\n:\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    run: master-service\n  name: master-service\n  namespace: default\nspec:\n  ports:\n  - port: 15022\n    protocol: TCP\n    targetPort: 22\n  selector:\n    vm.kubevirt.io/name: habr-vm\n  externalIPs:\n  - 192.168.2.45\nЗдесь мы пробрасываем порт \n15022\n снаружи кластера на порт \n22\n ВМ с именем \nhabr-vm\n, чтобы получить SSH-доступ к машине извне кластера.\nСохраним его в файл \nexternal-ip.yml\n и применим:\n$ kubectl create -f external-ip.yml\nservice/master-service created\nУбедимся, что Service создан и работает:\n$ kubectl get service\nNAME             TYPE        CLUSTER-IP      EXTERNAL-IP    PORT(S)     AGE\nkubernetes       ClusterIP   10.222.0.1      <none>         443/TCP     3h50m\nmaster-service   ClusterIP   10.222.158.22   192.168.2.45   15022/TCP   6s\nЗдесь мы воспользовались простым пробросом порта снаружи на ресурс внутри кластера. Такой подход не очень удобен, т. к. требует явного указания порта для соединения по SSH с внутренней ВМ. Если обратиться на порт по умолчанию (22), то мы попадем на master-узел основного кластера.\nДля элегантного решения этой проблемы можно воспользоваться модулем \nmetallb\n Deckhouse, который доступен в Enterprise-версии платформы. С его помощью можно настроить доступ так, чтобы из сети казалось, что у master-узла кластера есть несколько IP-адресов. Это позволит получить доступ к ВМ напрямую по ее IP без проброса разных портов.\nПроверим, что теперь мы можем попасть на внутреннюю ВМ снаружи:\n$ ssh 192.168.2.45 -p 15022\nWelcome to Ubuntu 22.04.1 LTS (GNU/Linux 5.15.0-57-generic x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\n\n  System information as of Tue Mar 21 13:24:57 UTC 2023\n\n  System load:  0.0               Processes:               128\n  Usage of /:   2.9% of 48.28GB   Users logged in:         0\n  Memory usage: 3%                IPv4 address for enp1s0: 10.10.10.0\n  Swap usage:   0%\n\n\n0 updates can be applied immediately.\n\n\nThe list of available updates is more than a week old.\nTo check for new updates run: sudo apt update\n\nLast login: Tue Mar 21 13:16:49 2023 from 192.168.2.35\nTo run a command as administrator (user \"root\"), use \"sudo <command>\".\nSee \"man sudo_root\" for details.\n\nzhbert@habr-vm:~$\nЗаключение\nВ статье мы рассмотрели пример использования нового модуля виртуализации Deckhouse. С его помощью можно развернуть полноценную виртуальную машину и работать с ней как с ресурсом Kubernetes, используя привычные команды kubectl. \nМагия Cilium, основанного на eBPF проекта для реализации сетевого взаимодействия, позволяет обеспечить сетевую связность всех компонентов таким образом, чтобы доступ к виртуальной машине был возможен из любой точки кластера. Благодаря этому ВМ можно использовать для развертывания сервисов, которые по каким-либо причинам невозможно развернуть внутри кластера привычным способом. Более того, на созданных ВМ можно развернуть еще один кластер под управлением Deckhouse, а затем получить доступ к отдельной ВМ уже из него.\nС любыми вопросами и предложениями ждем вас в комментариях к статье, а также в Telegram-чате \ndeckhouse_ru\n, где всегда готовы помочь. Будем рады issues (и, конечно, звёздам) \nв GitHub-репозитории Deckhouse\n.\nP.S.\nЧитайте также в нашем блоге:\n«\nИсследование производительности свободных хранилищ LINSTOR, Ceph, Mayastor и Vitastor в Kubernetes\n»\n.\n«\nОбзор Harvester — гиперконвергентного Open Source-решения на базе Kubernetes\n»\n.\n«\nУстанавливаем Kubernetes-платформу Deckhouse в закрытом окружении. Пошаговая инструкция\n»\n.\n \n ",
    "tags": [
        "kubernetes",
        "виртуальная машина",
        "kubevirt",
        "виртуализация",
        "devops"
    ]
}