{
    "article_id": "727268",
    "article_name": "Оптимизируем обмен данными между службами SCADA",
    "content": "История началась с того, что потребовалось создать демонстрационный проект SCADA на 50 000 тегов (точек или каналов) для потенциального клиента. Целью проекта было показать, что программный продукт SCADA достаточно производителен и удовлетворяет требованиям заказчика.\nХороший повод попробовать для демопроекта новое поколение SCADA-системы, одним из разработчиков которой является автор. Когда до демонстрации оставалось несколько дней, и неизбежность выполнения задачи стала очевидной, создаём новый проект. Добавляем в проект 1000 виртуальных устройств-симуляторов, генерирующих данные, создаём 50К+ каналов для хранения значений, запускаем… и на веб-клиенте созерцаем данные, которые обновляются один раз в несколько минут в неочевидной последовательности. О том, что было дальше, написана эта статья.\nАббревиатура SCADA достаточно известна и не требует расшифровки, при необходимости справочную информацию можно найти по \nссылке\n.\nПоиск узкого места\nВ первую очередь, необходимо определить причину проблемы. Чтобы читателю был понятен дальнейший ход мысли, на следующей схеме показано, из каких основных служб состоит программный комплекс SCADA.\nОсновные службы SCADA\nКраткое описание служб SCADA:\nСлужба Коммуникатор опрашивает контроллеры. Передаёт данные измерений службе Сервер и принимает команды телеуправления.\nСлужба Сервер обеспечивает запись и чтение архивов, расчёт по формулам, выполнение скриптов. Сервер взаимодействует с клиентами (службами Коммуникатор и Веб-приложением), которые подключаются по бинарному протоколу приложений поверх TCP.\nВеб-приложение предоставляет интерфейс оператора через браузер.\nВсе приложения пишут подробную информацию в текстовые логи, поэтому есть возможность проследить прохождение информации по цепочке снизу вверх – от контроллера, в нашем случае виртуального, до веб-клиента. Проверим журнал одной из линий связи:\nЖурнал линии связи\nВидим, что опрос выполняется с требуемой скоростью, примерно 5 сеансов в секунду (в конфигурации установлена задержка 200 мс после каждого сеанса). По другим линиям связи лог аналогичен, то есть в норме.\nПримечание\n. Линия связи с программной точки зрения – это отдельный поток, взаимодействующий с приборами по выбранному каналу связи, например, последовательному порту или TCP.\nДалее Коммуникатор добавляет полученные текущие данные в очередь и передаёт на Сервер. За это отвечает так называемый «источник данных» Server Data Source.\nИсточник данных Server Data Source\nОчередь текущих данных начинает заполняться и чуть более, чем за 20 секунд переполняется, последующие данные теряются. Увеличение размера очереди не решает проблему, просто очередь большей длины переполняется медленнее. Похоже, причина проблемы обнаружена. Однако, стоит проверить оставшиеся элементы цепочки.\nКак сказано в начале статьи, проект настраивался на новом поколении ПО SCADA, и специальных диагностических инструментов на тот момент не существовало. Открыв веб-приложение и построив несколько трендов, бегло убедились, что интерфейс отзывчив, тренды формируются менее, чем за секунду. Использование CPU службами SCADA также находилось в пределах нескольких %.\nТаким образом, сделан предварительный вывод о том, что узким местом системы является передача текущих данных от службы Коммуникатор службе Сервер. Необходимо исследовать проблему подробнее и найти её решение, не забывая о том, что скоро демонстрация проекта.\nВременное решение\nХотелось бы показать проект потенциальному заказчику на новом перспективном поколении SCADA, не откатываясь на текущую версию. Но исследование и ликвидация бутылочного горлышка, скорее всего, потребует больше времени, чем оставалось до демонстрации. 5 минут расфокусированного взгляда на монитор привели к такой догадке – если один «источник данных» не успевает передать нужный объём информации, то почему бы не попробовать несколько «источников данных» параллельно. И вторая резервная мысль – если данные не получается передать, то нужно их сгенерировать непосредственно на Сервере. Постараемся реализовать 1-й более честный вариант.\nК счастью, настройка нашего ПО SCADA достаточно гибкая, что позволило создать следующую конфигурацию, возможность которой при проектировании ПО даже не предполагалась:\nИсточники данных, передающие информацию параллельно\nСкриншот выше означает, что в настройках Коммуникатора были созданы 10 «источников данных», каждый из которых отвечает за передачу данных от устройств одной линии связи на Сервер. Привязка к линии выполнена с помощью фильтра по номерам устройств.\nЗапускаем проект и убеждаемся в том, что идея сработала! Данные успевают передаваться на Сервер и оператор видит изменения значений на веб-странице – достаточно для демонстрации.\nИсследование проблемы\nДемонстрация завершена, заказчик отправился думать и сравнивать наш продукт с конкурентами. Пора решить проблему системно, тем более что пользователи из сообщества также обнаружили данную «особенность» опытным путём.\nСреда испытаний\nИспытания проводились на компьютере и ПО со следующими характеристиками:\nПроцессор\n12th Gen Intel(R) Core(TM) i9-12900K   3.19 GHz\nОперативная   память\n32,0 ГБ\nОС\nWindows 11 Pro, версия 21H2\nВерсия   испытуемой SCADA\n6.0.2\nДля проведения испытаний разработаны следующие инструменты:\nМодуль ModPerformanceMonitor, который позволяет измерить скорость приёма данных различных видов, поступающих на Сервер.\nКонсольное приложение DataWriter, отправляющее Серверу различные виды данных с заданным размером и скоростью.\nИспытание №1\nВместо службы Коммуникатор будем использовать утилиту DataWriter, которая подключается к службе Сервер по TCP и в цикле передаёт данные заданного размера. Данные передаются в виде среза. \nСрез\n – это объект, состоящий из метки времени и массива значений каналов на этот момент времени. Длина среза – количество каналов.\nКанал\n – это именованный элемент информации, который обрабатывается SCADA, \n имеющий уникальный номер. Синонимы канала в других SCADA-системах: тег, переменная или точка.\nРезультаты испытаний представлены ниже.\nДлина среза\nТекущие данные, срезов в секунду\nИсторические данные, срезов в секунду\n10\n9,2\n5,4\n100\n9,2\n1,6\n1000\n9,2\n0,2\nИз таблицы видно, что скорость передачи текущих данных не зависит от длины среза, а скорость передачи исторических данных существенно зависит от неё (оптимизация передачи исторических данных выходит за рамки статьи).\nТекущие и исторические данные – это разные виды информации в рамках SCADA-системы, которые обрабатываются по-разному. Текущие данные, имея метку времени, при получении Сервером помещаются в оперативную память. Каждый канал имеет только одно текущее значение. Исторические данные при получении Сервером записываются в архив, в файловую систему или в базу данных. Исторические данные одного канала за период времени образуют \nтренд\n канала.\nИспытание №2\nОпределим, каким образом количество параллельных клиентов влияют на скорость передачи текущих данных между клиентом и службой Сервер.\nВ таблице ниже содержатся результаты при фиксированной длине среза равной 100 каналам.\nКоличество клиентов\nТекущие данные, срезов в секунду\nСкорость основного цикла, итераций в секунду\n1\n9,2\n9,2\n2\n18,4\n9,2\n3\n27,6\n9,2\nНиже результаты аналогичного испытания при длине среза 1000. Результаты совпадают.\nКоличество клиентов\nТекущие данные, срезов в секунду\nСкорость основного цикла, итераций в секунду\n1\n9,2\n9,2\n2\n18,4\n9,2\n3\n27,6\n9,2\nСкорость основного цикла измеряется с помощью модуля ModPerformanceMonitor, работа которого показана на скриншоте ниже. В основном цикле Сервер производит обработку данных, например, расчёт по формулам, поэтому скорость основного цикла (или длительность его итерации) – важный параметр, показывающий не находится ли система в «ступоре».\nРабота модуля Performance Monitor\nТаким образом, скорость передачи данных возрастает кратно количеству клиентов. По крайней мере, данное утверждение верно для нескольких клиентов. Это означает, что обработка текущих данных не является узким местом, а \nкорректно наше начальное заключение, что узким местом является именно передача данных\n. \nСкорость основного цикла не снижается при увеличении длины среза и количества клиентов – здесь норма.\nПримечание\n. Тестирование системы на предельное количество одновременно подключенных клиентов является задачей другого исследования.\nОптимизация передачи данных\nПовторюсь, что Сервер принимает TCP подключения от клиентов, открывая TCP-порт на прослушивание. Клиент подключается, инициирует запрос и получает ответ от Сервера. Поверх TCP используется бинарный протокол обмена данными, имеющий пакеты следующего вида:\nProtocol Header\nProtocol Data Unit (PDU)\nTransaction ID\n2 bytes\nLength\n4 bytes\nSession ID\n8 bytes\nFunction ID\n2 bytes\nFunction Arguments\nN bytes\nНе буду расшифровывать поля пакета, чтобы не утомлять читателя.\nИсходный код проекта насчитывает несколько сотен тысяч строк, поэтому поиск частей кода, которые требуется оптимизировать, должен быть основан на общем знании кода и информации, полученной в результате испытаний. Мы обнаружили, что количество обрабатываемых запросов от TCP-клиента, по сути, фиксировано. Клиентское приложение DataWriter отправляет запросы с той максимальной частотой, которая ограничивается временем ожидания ответа от Сервера. Следовательно, начать проверку кода было бы разумно с класса, который обрабатывает запросы клиентов.\nБазовым классом, прослушивающим соединение и отвечающим на запросы клиентов, является ListenerBase (\nссылка\n на GitHub, если ссылка нарушает правила публикации, просьба модератору её удалить). Оказалось, что после приёма данных от клиента вызывается задержка потока равная 100 мс. То есть обработка следующего клиентского запроса начнётся не ранее, чем через 100 мс после завершения обработки предыдущего. С этим связана фиксированная скорость приёма текущих данных 9,2 запроса в секунду, не зависящая от длины среза, и кратное увеличение скорости при увеличении количества клиентов.\nУстановим задержку в потоке подключенного клиента равной 10 мс. При такой задержке Сервер максимально сможет обработать 100 запросов в секунду от 1 клиента.\nНиже приведены результаты испытаний, аналогичных предыдущим, после уменьшения задержки в потоке клиента до 10 мс. Длина среза 100.\nКоличество клиентов\nТекущие данные, срезов в секунду\nСкорость основного цикла, итераций в секунду\n1\n64,4\n9,2\n2\n128,7\n9,2\n3\n193,2\n9,2\nАналогичное испытание. Задержка 10 мс. Длина среза 1000.\nКоличество клиентов\nТекущие данные, срезов в секунду\nСкорость основного цикла, итераций в секунду\n1\n64,4\n9,2\n2\n128,6\n9,2\n3\n193,3\n9,2\nСуммарная скорость также кратна количеству клиентов и скорость основного цикла стабильна при небольшом количестве клиентов. Загрузка CPU службой ScadaServerWkr составляет менее 1%.\nПромежуточный вывод\n: используя 1 клиентское подключение, удалось передать 64,4 среза секунду или 64400 точек в секунду, что близко к 4 млн точек в минуту. Такая скорость устраивает, но требуется достичь сопоставимых показателей совместно с Коммуникатором, а не утилитой DataWriter.\nВ чём недостаток протокола обмена данными в той версии, которая была на момент обнаружения проблемы? Дело в том, что при передаче текущих данных один пакет протокола приложений содержал только один срез. Если срез имеет длину 1000 или выше, тогда мы достигаем показателя 64К точек в секунду. А если, как в исходном демопроекте, длина среза менее 10? Тогда и скорость на несколько порядков меньше.\nРешение достаточно очевидно – необходимо изменить протокол обмена данными между приложениями таким образом, чтобы один пакет протокола содержал переменное количество срезов. Источник данных Server Data Source, который упоминался в начале статьи, должен упаковывать срезы из своей очереди в пакеты, контролируя суммарную длину срезов в пакете.\nПодтверждением данного подхода служит клиент популярной базы данных временных рядов InfluxDB, который имеет следующие параметры (\nссылка\n):\nBatchSize – количество точек данных для сбора в пакете,\nFlushInterval – время до записи пакета.\nРезультаты оптимизации\nВ результате передача данных каналов была модернизирована следующим образом:\nКоманды записи текущих и исторических данных в протоколе приложений были объединены в одну команду записи данных каналов. Эта новая команда позволяет передавать на Сервер сразу список срезов.\nОбновлён драйвер «источника данных». Из очереди извлекаются срезы текущих данных, имеющие суммарное количество каналов до 5000, затем они передаются одним пакетом. За одну итерацию цикла драйвера передаётся 10 пакетов. Передача исторических данных и событий не менялась.\nПараметры суммарной длина срезов (5000) и количества пакетов за итерацию (10) были подобраны опытным путём, чтобы достичь целевого показателя – не менее 64 пакета в секунду. Цель – «хорошая» скорость передачи, а не максимально возможная. Под хорошей скоростью понимается скорость, достаточная для решения задач клиентов с некоторым запасом.\nОписанные изменения в совокупности с оптимизацией программных блокировок Сервера (о блокировках в другой раз) позволили передавать из Коммуникатора на Сервер около 80000 каналов в секунду, используя 1 клиентское подключение. Например, для проекта, содержащего 1 млн каналов, для обновления всех секущих данных потребуется 12,5 секунд, если контроллеры предоставят такой поток измерений.\nВ разделе Среда испытаний упоминается версия ПО SCADA, с которой начиналось тестирование, поэтому стоит отметить, что изменения вошли в новую версию программного продукта 6.1.0.\nПланы на будущее\nДля комплексной оценки быстродействия ПО SCADA будет полезно провести следующие испытания:\nСравнение быстродействия исторических архивов, использующих различные средства хранения данных – файловую систему, СУБД PostgreSQL и InfluxDB.\nНагрузочное тестирование REST API и веб-приложения.\nP.S.\n В комментариях было бы интересно узнать о Вашем опыте, связанном с производительностью SCADA-систем, систем мониторинга и IoT, и любых систем, к быстродействию которых Вы причастны. Конструктивная критика статьи принимается.\n \n ",
    "tags": [
        "SCADA",
        "iot",
        "оптимизация"
    ]
}