{
    "article_id": "725864",
    "article_name": "Отладка гейзенбагов: история о параллельной обработке",
    "content": "Гейзенбаг\n \n(существительное)\n\r\nБаг, исчезающий при попытке его отладить\n\r\n\nПролог\n\r\nНедавно мы занимались крупнейшей миграцией в кодовой базе Python Analyzer, стремясь её ускорить. Среди прочих изменений одно крупное улучшение заключалось в следующем: теперь анализатор стал использовать concurrent.futures.ProcessPoolExecutor для параллельного выполнения независимых задач – таким образом мы могли задействовать все ядра ЦП. \n\r\nИ всё сработало довольно хорошо – в больших репозиториях мы наблюдали существенное увеличение скорости, и со временем ситуация бы только улучшилась. Мы смогли бы лучше контролировать, как функционируют инспекторы ошибок.\n\r\n\n\r\n\nГлава 0: Первый камень преткновения\n\r\nВ течение первых двух недель после того, как было развёрнуто обновление, мы стали фиксировать аварийное завершение некоторых аналитических прогонов. Отчасти это было ожидаемо – при такой крупной миграции неизбежно возникнут пограничные случаи, и мы просто должны быть способны оперативно отлаживать любые возникающие проблемы.\n\r\n\n\r\n\nСимптомы\n\r\nЗаглядывая в логи этих прогонов, мы заметили, что наш пул процессов аварийно завершается с ошибкой BrokenProcessPool. Обычно такие аварийные ситуации происходили только в больших репозиториях при попытке проанализировать большое количество файлов.\n\r\n\n\r\nТак случалось не каждый раз. Иногда такой анализ проходил без проблем. То есть, прогоны были хрупкими.\n\r\n\n\r\n\nДиагноз\n\r\n\nЭтот ответ\n подсказывает, что подобное могло происходить, поскольку процесс израсходовал всю доступную память. Время присмотреться к мониторингу.\n\r\n\n\r\nНаши аналитические задания мы выполняем через Google Cloud, и они работают в контейнерах, мониторинг которых удобно осуществлять при помощи GCP Monitoring. Если перезапустить один из неудавшихся анализов и пронаблюдать, как используется RAM, видим следующее:\n\r\n\n\r\n\nГрафик использования памяти\n\r\n\n(к сожалению, это оригинальный размер изображения)\n\r\n\n\r\n\n\r\nОчень похоже, что в контейнере заканчивается память.\n\r\n\n\r\nМы убедились, что контейнер действительно примерно на 100% выгребает тот лимит памяти, который для него установлен.\n\r\n\n\r\n\nЗадача\n\r\nПроведя дополнительные исследования, мы стали понимать, что проблема, по-видимому, заключается в очереди задач.\n\r\n\n\r\nМы не совсем правильно задавали футуры. Вот, например, как бы мы поступили, чтобы параллельно применить процесс ко всем элементам в составе любого заданного набора данных:\n\r\n\n\r\n\ndef process(item):\n    ... # do something here\n\ndata = get_data()\nresults = []\n\nwith concurrent.futures.ProcessPoolExecutor(CPU_COUNT) as executor:\n    futures = []\n\n    for item in data:\n        futures.append(executor.submit(process, item))\n\n    for future in concurrent.futures.as_completed(futures):\n        result = future.result()\n        results.append(result)\n\n# Здесь что-то делаем с результатами\n\r\nНа первый взгляд код кажется прямолинейным и корректным, и в основном так и есть, но упускается из виду одна вещь: ограниченный объём памяти, имеющейся у нас в распоряжении.\n\r\nПри каждом вызове executor.submit создаётся футура. Теперь в этой футуре будет содержаться копия тех данных, что вы её передали. Затем приступаем к выполнению футур. В каждой футуре так или иначе выполняется выделение памяти, обработка, а затем возвращение результата. К моменту окончания вычислений у нас будет список футур, в котором будут содержаться все футуры до одной, а также их память.\n\r\n\n\r\nПричём, элементов очень много: могут быть созданы сотни и даже тысячи футур, и все они поедают память. В результате мы столкнёмся с отказами при попытках выделить память, что, соответственно, приведёт к ошибкам BrokenProcessPool.\n\r\n\n\r\n\nИсправление\n\r\nЕсть два способа это исправить. Во-первых, можно начать объединять задачи в пакеты, код которых будет выглядеть примерно так:\n\r\n\n\r\n\nwith concurrent.futures.ProcessPoolExecutor(CPU_COUNT) as executor:\n    while data:\n        futures = []\n\n        # Less jobs at a time\n        for item in data[:10]:\n            futures.append(executor.submit(process, item))\n\n        for future in concurrent.futures.as_completed(futures):\n            item, result = future.result()\n            results.append(result)\n            data.remove(item)\n\n# Здесь что-то делаем с результатами \n\r\nТеперь мы храним всего десять футур, а затем отбрасываем их, как только получим результат. Также мы удаляем те элементы, которые уже успели обработать.\n\r\n\n\r\nОднако, есть даже более простой способ решить эту задачу:\n\r\n\n\r\n\nwith concurrent.futures.ProcessPoolExecutor(CPU_COUNT) as executor:\n    futures = (executor.submit(process, item) for item in data)\n\n    for future in concurrent.futures.as_completed(futures):\n        result = future.result()\n        results.append(result)\n\n# Здесь что-то делаем с результатами\n\r\nЭто интересно: мы превратили футуры в генератор.\n\r\n\n\r\nТеперь мы уже не храним список футур, как ранее, и ситуация меняется: ведь генератор ленивый. Он создаст футуру, только, если мы об этом попросим. А as_completed запрашивает у него футуру только при условии, что имеется пространство, в котором её можно было бы запустить. Поскольку мы используем футуру только в цикле for, она попадает под сборку мусора на каждой итерации. \n\r\n\n\r\nТаким образом, мы получаем результат, а затем сразу же избавляемся от уже не нужной нами футуры. Память не пожирается, проблема решена! Верно? \n\r\n\n\r\n\nГлава 1: Фантомные задержки \n\r\nТеперь в большинстве случаев анализатор работал идеально, как и ожидалось. Но в некоторых редких случаях начинали наблюдаться какие-то задержки.\n\r\n\n\r\nМы установили пороговое значение в 25 минут – в качестве максимальной длительности анализа в одном репозитории. При превышении этого порога мы отменяем анализ и сбрасываем его с пометкой «timed out» (время истекло). 25 минут – это очень долго для анализа, и обычно такие задержки случаются только при наличии какого-либо бага в анализаторе.\n\r\n\n\r\n\nСимптомы\n\r\nТакое поведение казалось очень хаотичным и непредсказуемым.\n\r\n\n\r\nТо и дело в ходе анализа, примерно через 10 минут после пуска, вывод логов просто прекращался. Как будто контейнер переставал их печатать потому, что в нём прекращались всякие события.\n\r\n\n\r\nЧерез 25 минут контейнер приходилось убить. Поскольку никакого результата при этом не генерировалось, ситуация подавалась как задержка.\n\r\n\n\r\nНе было никакого реального способа подробнее выяснить причины такого зависания, поскольку извне зайти в продакшен-контейнеры невозможно.\n\r\n\n\r\n\nДиагноз\n\r\nНа первый взгляд, это классическая проблема из области многопроцессорной разработки. Казалось, что либо у нас взаимная блокировка, либо какой-то рабочий процесс, которого мы дожидаемся, застрял в бесконечном цикле. Поэтому мы принялись закапываться в исходный код:\n\r\n\n\r\n\nЕсть ли где-нибудь в базе кода бесконечные циклы, любые циклы while? Нет. \n\r\n\nЧрезвычайно длительная обработка? Для каждого процесса, который мы порождали, предусматривалась задержка, поэтому и здесь проблем не могло возникать.\n\r\n\nВзаимные блокировки? Возможно. Но в состоянии взаимной блокировки процессы не должны потреблять ЦП.\n\r\n\n\r\nВсё время продолжали поступать новые процессы с памятью. В качестве предохранительной меры против неуловимых ошибок BrokenProcessPool мы модифицировали код — так, чтобы воссоздавать пул, если такая ошибка произойдёт:\n\r\n\n\r\n\nexecutor = concurrent.futures.ProcessPoolExecutor(CPU_COUNT)\nprocess_pool_broke = False\n\nwhile data:\n    futures = ...\n\n    try:\n        for future in concurrent.futures.as_completed(futures):\n            item, result = future.result()\n            results.append(result)\n            data.remove(item)\n\n    except BrokenProcessPool:\n        executor.shutdown(wait=False)\n        logger.info(\"Re creating pool...\")\n        executor = concurrent.futures.ProcessPoolExecutor(CPU_COUNT)\n\nexecutor.shutdown()\n\n# Здесь что-то делаем с результатами\n\r\nВ логах мы также наблюдаем воссоздание пула. Соответственно, память в данном случае тоже съедалась. До того мы не уделяли этому пристального внимания, поскольку все признаки указывали на взаимную блокировку.\n\r\n\n\r\n\nИзыскания\n\r\nТеперь проблема заключается в том, что мы должны как-то воссоздать это поведение. Тот репозиторий, из-за которого подвисал анализ, оказался приватным, поэтому мы не могли воссоздать на локальной машине ровно то же окружение. Требовалось найти публичный репозиторий, при работе с которым провоцируется такая же проблема. Правда, была одна ниточка: представлялось, что в репозитории должно быть 800 или около того файлов на Python, то есть, речь шла о большом проекте.\n\r\n\n\r\nМы пробовали прогонять через продакшен-анализатор большие проекты, например, Django – и нам не везло. Всякий раз анализ проходил как по маслу. \n\r\n\n\r\nМы искали свидетельства о задержках в истории анализатора, чтобы найти другие крупные репозитории, в которых такие задержки ранее фиксировались, и нашли пару подобных репозиториев с открытым исходным кодом. Каким-то образом, попытав счастья примерно с дюжиной репозиториев и ни разу не сумев получить взаимную блокировку, мы нашли такой, в котором удалось воспроизвести проблему. Проект назывался weblate.\n\r\n\n\r\nМы попытались повторно запустить анализатор, чтобы посмотреть, можно ли уверенно воспроизвести баг в этом репозитории weblate. И… неудачно. \n\r\n\n\r\nПопробовали несколько раз – нет задержки. Действительно, самый настоящий гейзенбаг. Что ж, хотя бы в этом отношении поведение развивается последовательно: те задержки, которые побудили нас заняться этим расследованием, тоже исчезали при попытке повторно прогнать тот же самый анализ.\n\r\n\n\r\nПопытка проанализировать weblate на нашем внутреннем инстансе DeepSource нам также не помогла: никаких задержек, никаких зависаний.\n\r\n\n\r\n\nДокопаться до истины\n\r\nИтак, у нас был один случайным образом отказывающий репозиторий, а теперь их стало два. Может показаться, будто мы вдвое усугубили проблему, но на самом деле это был не тупик — теперь можно было приступать к проведению параллелей между двумя прогонами, искать, в чём же они схожи. Может быть, нам удалось бы точнее охарактеризовать те условия, которые должны быть соблюдены, чтобы этот баг проявился. \n\r\n\n\r\nПорывшись в логах двух проектов, мы нашли кое-что странное. В самом начале там есть одна строка: \n\r\n\n\r\n\nINFO: Running analysis in parallel mode, core count: 24\n\r\nЭто было чуть странно, поскольку, как мы заметили, у нас в кластере одновременно выполняется 24 процесса, а количество процессов, переданных исполнителю, вдвое превышает количество ядер:\n\r\n\n\r\n\nfor item in data[: CPU_COUNT * 2]:\n    futures.append(executor.submit(process, item))\n\r\nМы добавили дополнительное логирование, стараясь воспроизвести эту проблему – и, действительно, в ходе обоих неудавшихся анализах одновременно работало по 48 процессов, а не по 24, как ожидалось. \n\r\n\n\r\nМы проверили логи тех внутренних прогонов, которые не приводили к задержкам, и логи показали: \n\r\n\n\r\n\nINFO: Running analysis in parallel mode, core count: 12\n\r\n12 ядер, 24 процесса.\n\r\n\n\r\nАнализ проходит успешно, когда в деле 12 ядер, и неуспешно – когда 24. Мы нашли общий знаменатель. Теперь, чтобы быть на 100% уверенными, мы должны были надёжно воспроизвести этот баг.\n\r\n\n\r\nДавайте рассмотрим определение CPU_COUNT:\n\r\n\n\r\n\n# Откат к 4 ядрам, если заданное количество недоступно\nCPU_COUNT = os.cpu_count() or 4\n\r\nЗаменили это значение на жёстко запрограммированное:\n\r\n\n\r\n\nCPU_COUNT = 24\n\r\nСделали это, попытались проанализировать weblate на нашем внутреннем инстансе анализатора – и, вуаля! Воспроизводится. Анализ из раза в раз застревает спустя 10 минут работы.\n\r\n\n\r\nНаконец-то мы прижучили баг: Когда число os.cpu_count() велико, получаются задержки.\n\r\n\n\r\n\nРеальная проблема\n\r\nНаши контейнеры работают на Google Kubernetes Engine, представляющем собой управляемый многоядерный кластер k8s. Но у нас в аналитических кластерах лимитируются ресурсы, так, что анализ допускается проводить не более чем на 4 ядрах ЦП одновременно. \n\r\n\n\r\nТак вот в чём дело. Почитав \nдокументацию\n, удалось выяснить, что лимитирование ресурсов в Kubernetes обычно осуществляется путём ограничения «процессорного времени», а не количества задействованных ядер. \n\r\n\n\r\nТаким образом, если указать процессу, что он может «использовать 4 ядра ЦП», то на практике он будет использовать не 4 ядра, а все до одного ядра в кластере. Но планировщику будет приказано ограничить количество циклов ЦП, которые могут быть использованы данным кластером, так, чтобы это количество было эквивалентно эксплуатации 4 ядер.\n\r\n\n\r\nПока не укладывается в голове? Давайте объясню на примере:\n\r\nДопустим, у вас в кластере имеется 40 ядер. Если вы укажете, что каждый кластер должен использовать максимум 4 ядра, то процессор будет ограничен не 4 ядрами, а 10% процессорного времени. Если в настоящее время у вас фактически работает только один контейнер, то процесс будет потреблять 10% процессорного времени на каждом из ядер ЦП, работающих в этом кластере. \n\r\n\n\r\nЭто означает две вещи:\n\r\n1. os.cpu_count() будет видеть все ядра, присутствующие в кластере, даже если для ЦП будут заданы ограничения. А поскольку мы используем такое количество ядер для порождения некоторого числа процессов, мы будем порождать большее количество процессов на то же количество «процессорного времени». \n\r\n\n\r\nВ предыдущем примере, если мы порождали 5%, то каждый процесс получал 2% от общей доли в 10% процессорного времени. Но, если породить 10 процессов, то каждый из них вдруг замедляется вдвое, поскольку получает всего по 1% от общего количества ресурсов.\n\r\nИтак, если на той машине, где работает процесс, ядер ЦП больше, чем ожидалось, то каждый создаваемый нами параллельный процесс вдруг начинает работать вдвое медленнее без какой-либо причины. \n\r\n\n\r\n2. Чем больше процессов мы порождаем, тем больше памяти в итоге используем. Если вместо 12 ядер на машине окажется 24 ядра, то расход памяти у нас внезапно удвоится. А поскольку использование памяти у нас в контейнере никак не лимитировано, проблемы с памятью нам обеспечены.\n\r\n\n\r\nИтак, суть бага заключалась не в параллельной обработке, а в планировании. Это объясняет, почему он не воспроизводился на внутреннем инстансе: наш внутренний инстанс запускал в кластер более мелкие машины, тогда как в продакшен-кластере были более крупные машины, и ядер на них тоже было больше.\n\r\n\n\r\n\nИсправление\n\r\nПроблема исправлялась незамысловато: требовалось задать сократить максимальное количество процессов, остановиться, например, на 8:\n\r\n\n\r\n\nCPU_COUNT = os.cpu_count() or 4\nPROCESS_COUNT = min(2 * CPU_COUNT, 8)\n\n...\n\nwith executor:\n    for item in data[: PROCESS_COUNT]:  # Changed from CPU_COUNT * 2\n        futures.append(executor.submit(process, item))\n\n    # Оставшаяся часть кода – без изменений\n\r\nПосле того, как мы это проделали, все задержки исчезли.\n\r\n\n\r\nПроизводительность нашего анализатора от этого не снижается, поскольку пока ваши процессы используют 100% процессорного времени, выделяемого вашему контейнеру, вы работаете с максимальной достижимой скоростью.\n\r\n\n\r\nФактически, производительность в таком случае даже возрастает, поскольку при использовании меньшего объёма памяти меньше ресурсов тратится на подкачку страниц, пробуксовку памяти и т.д. – в конечном итоге, скорость работы у каждого отдельного процесса возрастает.\n\r\n\n\r\n\nЗаключение\n\r\nВся эта отладочная сага не научила нас чему-либо особенно новому. Но вот какие уроки мы извлекли из проделанной работы:\n\r\n\n\r\n\nПервым делом нужно воспроизвести проблему. Если не удаётся надёжно воспроизвести баг, то, каких бы исправлений вы ни вносили, вы не можете знать наверняка, сработают ли они.\n\r\n\nВторой шаг – это исключение и отыскание общих паттернов. Заметив схожести между двумя экземплярами бага, добавив логи и устранив возможные источники проблемы вы, в конце концов, доберётесь до корня проблем.\n\r\n\nСвою инфраструктуру нужно знать, даже, если вы работаете в контейнере. Docker решает множество проблем, изолируя ваше рабочее окружение, но не проблему ограничений, накладываемых на ресурсы.\n\r\n\n\r\n\n\r\n\nЭпилог\n\r\nК моменту, когда эти исправления были внесены, схожие проблемы с памятью стали просматриваться в нашем анализаторе Ruby – возникали исключения DeadWorker. Точно такое же исправление (ограничение, согласно которому анализатор может порождать максимум 8 процессов) позволило устранить задержки и в данном случае. \n\r\n\n\r\nНеделю спустя то же самое произошло с PHP, и наше исправление снова сработало same.\n\r\nТак мы расправились с одним гнусным гейзенбагом.\n\r\n\n\r\n\nP.S.\n\r\nНа сайте издательства продолжается \nвесенняя распродажа\n.\n \n ",
    "tags": [
        "программирование",
        "исследования и прогнозы в IT",
        "процессоры",
        "отладка"
    ]
}