{
    "article_id": "723792",
    "article_name": "Часть 2. Перевод нейронной сети на базе Keras LSTM на работу с матричными операциями",
    "content": "В \nпервой части \nчасти я перевел обученную модель полносвязной сети на базе Keras на работу с матричными вычислениями. Модель разработана для новостного агрегатора с целью фильтрации нежелательных новостей. \nНо если посмотреть \nстатью\n-руководство от tensorflow, можно увидеть, что одной из рекомендаций по классификации теста является использование сетей долгой краткосрочной памяти (LSTM). \nДля моей задачи сеть прямого распространения обладает достаточным качеством, предсказуемостью и стабильностью результатов (объяснимое переобучение, влияние архитектуры сети на качество и т.д.). Ну и немаловажно - быстро обучается, в отличие от LSTM. \nНо ради \"академического\" интереса обучим сеть c LSTM для бинароной классификации текста и переведем её также на работу только с матрицами и пакетом numpy. Это также наглядно покажет, как устроены ячейки LSTM. \nСеть LSTM\nИтак, tensorflow рассматривает следующую архитектуру сети:\nmodel = tf.keras.Sequential([\n  encoder, \n    tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=64,\n        # Use masking to handle the variable sequence lengths\n        mask_zero=True),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1)\n])\nВот как она выглядит в графическом виде:\nАрхитектура сети с сайта tensorflow \n(\nhttps://www.tensorflow.org/text/tutorials/text_classification_rnn\n)\nМодули TextVectorization и \ntf.keras.layers.Embedding\nте же, что были в первой части моей работы. Вкратце напомню, что TextVectorization преобразует слова в уникальные числовые индексы, а Embedding затем преобразует их в плотные вектора. \nДалее идет слой LSTM, который с помощью слоя \ntf.keras.layers.Bidirectional\n \"проходится\" по двум направлениям: от начала последовательности к концу и наоборот, а затем объединяет результаты. Но для начала надо смоделировать более простую архитектуру - без \ntf.keras.layers.Bidirectional\n, а зачем уже с ним. \nРассмотрим следующую модель:\nmodel = tf.keras.Sequential([\n   tf.keras.layers.Embedding(\n        input_dim=len(encoder.get_vocabulary()),\n        output_dim=64,\n        mask_zero=True),\n    tf.keras.layers.LSTM(64),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nДля визуального понимания кода модели \ntf.keras.layers.LSTM\nрассмотрим типовую схему ячейки LSTM. На ней я подписал все функции, которые мы должны вычислить. \nАрхитектура ячейки LSTM\nПолучить веса обученной модели для этого слоя не так просто как для слоев Dense и Embedding - это делается следующим образом (вот \nтут\n про это хорошо написано):\n# Количество ячеек памяти. Задается при объявлении слоя tf.keras.layers.LSTM(units)\nunits = int(int(model.layers[1].trainable_weights[0].shape[1])/4)\nprint(\"No units: \", units)\n\nW = model.layers[1].get_weights()[0]\nU = model.layers[1].get_weights()[1]\nb = model.layers[1].get_weights()[2]\n\nW_i = W[:, :units]\nW_f = W[:, units: units * 2]\nW_c = W[:, units * 2: units * 3]\nW_o = W[:, units * 3:]\n\nU_i = U[:, :units]\nU_f = U[:, units: units * 2]\nU_c = U[:, units * 2: units * 3]\nU_o = U[:, units * 3:]\n\nb_i = b[:units]\nb_f = b[units: units * 2]\nb_c = b[units * 2: units * 3]\nb_o = b[units * 3:]\nВеса \nW\n используются при математических операциях с входной последовательностью \nx\n (т.е. выходом слоя Embedding). Веса \nU\n - для преобразования состояния \nh \nпрошлой итерации. \nb \n- это смещение (bias).\nСимвол \n означает операцию \nnp.multiply.\n Символ \n обычное поэлементное суммирование векторов.  \nС учетом особенностей работы ячеек LSTM индексы с названиях коэффициентов обозначают \"функциональное назначение\" элементов:\nИндекс \"\nf\" — \nфункция забывания/forget gate\n. \nПо сути, тут с помощью умножения на коэффициент от 0 до 1 управляется значением состояния \nС\nt\n \nдля удаления информации о прошлых шагах обработки. Код для вычисления: \n# letter - это очередной символ из слоя Embedding\n# h_st - выход предыдущей ячейки\n\nself.f_t = self.sigmoid(np.dot(letter, self.W_f) \n                        + np.dot(self.h_st, self.U_f) \n                        + self.b_f)\nИндекс \"\ni \"— \nдобавление информации к состояния, «входной вентиль». Здесь\n \nна основе выхода предыдущей ячейки \nh\nt-1\n и ввода x\nt\n определяется, какие значения использовать из ввода (x) во внутреннем состоянии. Код:\n# letter - это очередной символ из слоя Embedding\n# h_st - выход предыдущей ячейки\n\nself.i_t = self.sigmoid(np.dot(letter, self.W_i) \n                        + np.dot(self.h_st, self.U_i) \n                        + self.b_i)\n Индекс \"\nс\" — \nподготовка функции \nĈ\nt\nself.Ct_t = np.tanh(np.dot(letter, self.W_c) \n                    + np.dot(self.h_st, self.U_c) \n                    + self.b_c)\nТеперь все готово для расчета нового клеточного состояния \nС\nt\n self.state = np.multiply(self.f_t, self.state) + np.multiply(self.i_t, self.Ct_t)\nИндекс \"\nо\n\" - расчет выходного значения. Применяем гиперболический тангенс к текущему состоянию ячейки и умножаем на преобразованное значение текущего входного символа: \nself.h_st = np.multiply(self.sigmoid(np.dot(letter, self.W_o) \n                                     + np.dot(self.h_st, self.U_o)\n                                     + self.b_o), \n                       np.tanh(self.state))\nПолный код ячейки LSTM выглядит следующим образом.\ndef lstm(self, data):\n  # инициализация начального состояния ячейки и выходного состояния для работы на первой итерации\n    self.state=np.zeros(self.units)\n    self.h_st=np.zeros(self.units)\n\n    # проходим по символам в прямом направлении. \n    for letter in data:\n        \n        self.f_t = self.sigmoid(np.dot(letter, self.W_f) + np.dot(self.h_st, self.U_f) + self.b_f)\n        \n        self.i_t = self.sigmoid(np.dot(letter, self.W_i) + np.dot(self.h_st, self.U_i) + self.b_i)\n        \n        self.Ct_t = np.tanh(np.dot(letter, self.W_c) + np.dot(self.h_st, self.U_c) + self.b_c)\n\n        self.state = np.multiply(self.f_t, self.state) + np.multiply(self.i_t, self.Ct_t)\n\n        self.h_st = np.multiply(self.sigmoid(np.dot(letter, self.W_o) + np.dot(self.h_st, self.U_o)+ self.b_o), np.tanh(self.state))\n        \n    return np.array(self.h_st)\nBidirectional LSTM\nВ случае использования \ntf.keras.layers.Bidirectional\n создается два слоя LSTM: один проходит цепочку слов в прямом направлении, второй - в обратном. Зачем результаты конкатенируются. \nВ результате обучения у нас получает две группы весов (для LSTM прямого и обратного направления соответственно) .  Веса получают из обученной модели следующим образом:\nself.vocal_dict = {vocal_dict[k]: k for k in range(len(vocal_dict))} \nself.units = units\n\n# слой прямого прохождения\nself.W_farward = lstm_weights[0]\nself.U_farward = lstm_weights[1]\nself.b_farward = lstm_weights[2]\nself.W_i_farward = self.W_farward[:, :self.units]\nself.W_f_farward = self.W_farward[:, self.units: self.units * 2]\nself.W_c_farward = self.W_farward[:, self.units * 2: self.units * 3]\nself.W_o_farward = self.W_farward[:, self.units * 3:]\n\nself.U_i_farward = self.U_farward[:, :self.units]\nself.U_f_farward = self.U_farward[:, self.units: self.units * 2]\nself.U_c_farward = self.U_farward[:, self.units * 2: self.units * 3]\nself.U_o_farward = self.U_farward[:, self.units * 3:]\n\nself.b_i_farward = self.b_farward[:self.units]\nself.b_f_farward = self.b_farward[self.units: self.units * 2]\nself.b_c_farward = self.b_farward[self.units * 2: self.units * 3]\nself.b_o_farward = self.b_farward[self.units * 3:]\n\n# слой обратного прохождения\nself.W_backward = lstm_weights[3]\nself.U_backward = lstm_weights[4]\nself.b_backward = lstm_weights[5]\nself.W_i_backward = self.W_backward[:, :self.units]\nself.W_f_backward = self.W_backward[:, self.units: self.units * 2]\nself.W_c_backward = self.W_backward[:, self.units * 2: self.units * 3]\nself.W_o_backward = self.W_backward[:, self.units * 3:]\n\nself.U_i_backward = self.U_backward[:, :self.units]\nself.U_f_backward = self.U_backward[:, self.units: self.units * 2]\nself.U_c_backward = self.U_backward[:, self.units * 2: self.units * 3]\nself.U_o_backward = self.U_backward[:, self.units * 3:]\n\nself.b_i_backward = self.b_backward[:self.units]\nself.b_f_backward = self.b_backward[self.units: self.units * 2]\nself.b_c_backward = self.b_backward[self.units * 2: self.units * 3]\nself.b_o_backward = self.b_backward[self.units * 3:]\n Реализация \ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units))\n таким образом, следующая:\ndef lstm_farward(self, data):\n      \n    self.state=np.zeros(self.units)\n    self.h_st=np.zeros(self.units)\n    for letter in data:\n          \n      self.f_t = self.sigmoid(np.dot(letter, self.W_f_farward) + np.dot(self.h_st, self.U_f_farward) + self.b_f_farward )\n      \n      self.i_t = self.sigmoid(np.dot(letter, self.W_i_farward) + np.dot(self.h_st, self.U_i_farward) + self.b_i_farward )\n      \n      self.Ct_t = np.tanh( np.dot(letter, self.W_c_farward) + np.dot(self.h_st, self.U_c_farward) + self.b_c_farward )\n      \n      self.state = np.multiply(self.f_t, self.state) + np.multiply(self.i_t, self.Ct_t)\n      \n      self.h_st = np.multiply(self.sigmoid(np.dot(letter, self.W_o_farward) + np.dot(self.h_st, self.U_o_farward)+ self.b_o_farward), np.tanh(self.state))\n\n    return np.array(self.h_st)\n\ndef lstm_backward(self, data):\n    \n    self.state=np.zeros(self.units)\n    self.h_st=np.zeros(self.units)\n    \n    for letter in data[::-1]:\n        \n        self.f_t = self.sigmoid(np.dot(letter, self.W_f_backward) + np.dot(self.h_st, self.U_f_backward) + self.b_f_backward )\n        \n        self.i_t = self.sigmoid(np.dot(letter, self.W_i_backward) + np.dot(self.h_st, self.U_i_backward) + self.b_i_backward )\n        \n        self.Ct_t = np.tanh( np.dot(letter, self.W_c_backward) + np.dot(self.h_st, self.U_c_backward) + self.b_c_backward )\n\n        self.state = np.multiply(self.f_t, self.state) + np.multiply(self.i_t, self.Ct_t)\n\n        self.h_st = np.multiply(self.sigmoid(np.dot(letter, self.W_o_backward) + np.dot(self.h_st, self.U_o_backward)+ self.b_o_backward), np.tanh(self.state))\n\n    return np.array(self.h_st) \n\nemb_out = self.embedding(sentanence)\nlstm_out_farward =  self.lstm_farward(emb_out)\nlstm_out_backward = self.lstm_backward(emb_out)\nlstm = np.concatenate((lstm_out_farward, lstm_out_backward))\nЗаключение\nВ данной статье приведено моделирования слоев \ntf.keras.layers.Bidirectional\n и \ntf.keras.layers.LSTM\n. Полученные модели могут использоваться для развертывания обученной модели на системах без необходимости установки пакета tensorflow, а также для изучения работы LSTM в образовательных целях.\n \n ",
    "tags": [
        "Keras",
        "матрицы",
        "ml"
    ]
}