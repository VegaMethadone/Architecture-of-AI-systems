{
    "article_id": "727804",
    "article_name": "Savant: новый высокопроизводительный фреймворк Python для видеоаналитики на оборудовании Nvidia",
    "content": "В статье рассматривается новый открытый фреймворк для потоковой видеоаналитики и демонстрируются его возможности на примере демонстрационного приложения, которое использует модель DeepStream’s PeopleNet для обнаружения людей и их лиц, размывает лица и отображает панель управления с помощью OpenCV CUDA.\nМы будем использовать Savant для обработки видео в реальном времени с протоколом RTSP и для обработки видеофайлов в пакетном режиме, чтобы продемонстрировать, как конвейер может достигать скорости 400 кадров в секунду на Nvidia Tesla T4.\nДля тех, кто хочет сначала попробовать без подробностей, мы подготовили скрипты для быстрого старта на основе Docker Compose (раздел \"Быстрый старт\").\nSavant на GitHub\nЯ директор продукта и представляю коллектив разработчиков фреймворка, буду рад ответить на любые вопросы. Статья является переводом моей же \nстатьи\n на английском на Medium, я постарался адаптировать текст, но заранее прошу прощения за возможные киноляпы.\nО Фреймворке Savant\nSavant (от французского — «ученый») — это высокоуровневый фреймворк для видеоаналитики, созданный на основе Nvidia DeepStream. Он позволяет создавать пайплайны компьютерного зрения для видеоданных любого типа (файлы, потоковые трансляции и наборы изображений) с помощью YAML и дополнительных функций обработки на языке Python.\nSavant предоставляет разработчикам несколько значимых функций, которых нет в базовом функционале Nvidia DeepStream; давайте кратко обсудим некоторые из них.\nАдаптеры\n. В базовом DeepStream данные потоков тесно связаны с конвейером. Это означает, что если источник или назначение выходят из строя, то и весь конвейер прекращает работу; после этого может потребоваться несколько секунд, чтобы снова восстановить работу конвейера, поскольку перезапуск тяжеловесных конвейеров с моделями занимает значительное количество времени, и данные теряются за это время. В Savant адаптеры общаются с фреймворком через открытый потоковый API на основе ZeroMQ и Avro. Это позволяет адаптерам выходить из строя без влияния на работу конвейера.\nМультиплексирование данных\n. Savant позволяет автоматически обрабатывать несколько видеопотоков, которые могут динамически появляться и исчезать: вы запускаете модуль обработки и отправляете данные в него, указывая уникальный идентификатор источника. Фреймворк правильно обрабатывает данные, разделяя их на отдельные потоки.\nНезависимость от типа потока данных\n. Вы можете одновременно подавать различные типы медиапотоков в конвейер — потоки RTSP, файлы, наборы изображений — и все будет работать прозрачно. Нет необходимости модифицировать конвейер для совместимости. Таким образом, вы можете разрабатывать и тестировать на своем рабочем месте, используя набор изображений, а после обрабатывать потоки RTSP в продуктовой среде.\nГотовность к использованию в продуктиве\n. Savant общается с адаптерами через открытое API, а сам фреймворк работает в контейнере Docker. Это означает, что вы можете легко развернуть его в K8s или Docker Compose и использовать адаптеры для подключения к вашей инфраструктуре управления данными. Мы предоставляем несколько готовых к использованию адаптеров, которые могут служить основой для реализации ваших собственных.\nСовместимость между Edge и серверным оборудованием\n. Мы преодолели ряд проблем, чтобы обеспечить совместимость с различным оборудованием Nvidia. Хотя Nvidia стремится предоставить единый интерфейс для всех устройств, значительные различия требуют адаптации под конкретные устройства. Мы предоставляем Docker‑контейнеры как для Jetson, так и для x86, которые уже совместимы с оборудованием.\nПоддержка OpenCV CUDA\n. С этой интеграцией вы можете эффективно работать с кадрами видео в памяти GPU на Python без передачи их в память CPU. Эта функция определенно порадует тех, кто выполняет нетривиальные видео преобразования, рисует дашборды и использует сложные модели, которые требуют преобразований объектов (например, обработка лиц с лэндмарками). В то время как Nvidia предоставляет байндинги Python для мэппинга кадров видео в память CPU, OpenCV CUDA часто работает намного быстрее во многих случаях, например, при размытии лиц или номерных знаков.\nSavant предлагает множество других полезных функций. Вы можете найти более подробную информацию о них на \nсайте\n проекта.\nПочему просто не использовать DeepStream?\nМы разрабатываем приложения для фреймворка DeepStream уже несколько лет и за это время столкнулись с рядом проблем при реализации решений, подходящих для использования в продуктиве: от разработчика требуется глубокое понимание GStreamer — фреймворка обработки мультимедиа с открытым исходным кодом. Тем временем, GStreamer является относительно низкоуровневым фреймворком, построенным на экосистеме GObject, и часто требует реализации функционала на C/C++, а также обработки множества событий и сигналов.\nТем не менее, GStreamer оптимально подходит для создания высокопроизводительных систем обработки видео. Об этом свидетельствует тот факт, что ведущие производители железа, такие как Nvidia, AMD (Xilinx) и Intel, разрабатывают свои фреймворки на основе GStreamer в качестве плагинов.\nВ итоге, барьер входа для DeepStream высок. От разработчика требуются навыки програмирования GStreamer, умение писать плагины на C/C++, знания о том, как обрабатывать сигналы конвейера и понимание архитектуры системы. Если к предыдущим сложностям добавить дополнительные плагины, которые Nvidia разработала для оптимизации инференса на их оборудовании, то вы столкнетесь со многими препятствиями на пути к готовому к применению в продуктиве пайплайну.\nАнализ видео с использованием PyTorch, OpenCV, TensorFlow\nМы знаем, что многие люди создают системы видеоаналитики с использованием других технологий. Они вычитывают видеофайлы или потоки с помощью OpenCV и передают растры изображений в PyTorch, и все как будто работает нормально. \nДействительно, в случае неторопливого поступления изолированных изображений, таких как сканы паспортов или фото растений, все может выглядеть хорошо; однако для интенсивных потоков изображений или видео транзакционные издержки в неоптимизированных фреймворках могут привести к уменьшению производительности в 10 раз и более.\nДругими словами, вы можете потратить существенно большее количество денег и использовать в несколько раз больше аппаратного обеспечения, чтобы достичь той же производительности, которую вы могли бы достичь с помощью DeepStream. Стоит отметить, что некоторые конвейеры не могут работать в принципе или демонстрируют плохую производительность на Nvidia Jetson - широко используемых edge-устройствах - при использовании PyTorch или других неоптимизированных технологий.\nВ общем, каждый раз, когда вы делаете видео-аналитику с помощью PyTorch, TensorFlow или другого подобного фреймворка,  где-то в мире плачет котик.\nПочему стоит рассмотреть Savant\nВам стоит попробовать Savant, потому что:\nс его помощью действительно легко создавать и запускать конвейеры обработки видео высокой производительности как для дискретных GPU, так и для периферийных устройств; \nлегко расширять и настраивать их под свои нужды; \nвы получаете приложения, готовые к использованию в производстве. \nМы переходим к нашему демонстрационному приложению, которое поможет вам познакомиться с технологией.\nДемонстрационное приложение\nТеперь, когда у вас есть общее представление о Savant, мы приглашаем вас исследовать приложение, которое мы разработали, чтобы продемонстрировать возможности фреймворка.\nПриложение с помощью модели \nPeopleNet\n в видеопотоке обнаруживает людей и их лиц, а затем эти лица размываются с помощью OpenCV CUDA. Дополнительно, чтобы продемонстрировать OpenCV CUDA, приложение отображает анимированную панель с количеством людей с обнаруженными лицами и без них. Кроме того, для снижения дрожания рамок используется трекер.  И вся эта красота вам доступна на скорости ~ 400 FPS и выше для HD-видео на современном железе. Демонстрационная схема конвейера представлена ниже.\nМы покажем вам, как использовать Savant в реальных сценариях с видеопотоками с камер, транслировать результаты конвейера через RTSP и продемонстрируем производительность конвейера в режиме пакетной обработки на локальных видеофайлах.\nКод для демонстрации находится в каталоге проекта с примерами на GitHub \nsamples/peoplenet_detector\n.\nТребования к рабочей среде\nХорошее интернет-соединение является важным: демонстрация воспроизводит видеофайлы непосредственно из Интернета, поэтому медленное интернет-соединение может вызвать залипание поткоа. Попробуйте открыть эту \nссылку\n в браузере, чтобы проверить, воспроизводится ли видео без проблем.\nДля запуска демонстрации вам понадобится корректно функционирующее окружение с поддержкой ускоренных вычислений на аппаратных средствах Nvidia. Пожалуйста, прочтите краткое \nруководство\n по настройке Ubuntu 22.04 для поддержки Savant.\nТребования к среде на базе x86\n: Nvidia dGPU (Volta, Turing, Ampere, Ada Lovelace), ОС Linux с драйвером 525+, Docker с установленным и настроенным плагином \nCompose\n и Nvidia Container \nRuntime\n.\nТребования к среде на базе Jetson\n: Nvidia Jetson (NX/AGX, Orin NX/Nano/AGX) с JetPack 5.1+ (фреймворк не поддерживает первое поколение Jetson Nano), Docker с установленным и настроенным плагином \nCompose\n и Nvidia Container \nRuntime\n.\nПроверка совместимости окружения\nSavant и адаптеры запускаются как контейнеры Docker. Поскольку мы используем ускоренные вычисления Nvidia, Nvidia Container Runtime должен быть настроен правильно. Устройства Jetson не должны иметь проблем, если вы установили и настроили последнюю операционную систему в соответствии с рекомендациями Nvidia.\nКлонируйте проект Savant, чтобы получить доступ к исходному коду:\ngit clone https://github.com/insight-platform/Savant.git\ncd Savant\ngit lfs pull\ncd ..\nЗапустите следующую команду, чтобы убедиться, что среда совместима:\n./Savant/utils/check-environment-compatible\nВы должны увидеть сообщение о том, что среда в порядке:\nEnvironment is OK\nЕсли вывод соответствует ожиданиям, можно продолжать; иначе необходимо корректно настроить среду перед продолжением.\nБыстрый старт\nЕсли вы хотите сначала попробовать Savant, прежде чем потратить на него свое время используйте \ndocker compose\n в директории \nsamples/peoplenet_detector\n, чтобы запустить конвейер все-в-одном:\ngit clone https://github.com/insight-platform/Savant.git\ncd Savant/samples/peoplenet_detector\ngit lfs pull\n\n# если хотите поделиться своей геопозицией с нами, запустите\n# следующую команду (это опционально), мы просто хотим узнать о \n# том, что вы попробовали Savant\n#\ncurl --silent -O -- https://hello.savant.video/peoplenet.html\n\n# if x86\n../../utils/check-environment-compatible && \\\n   docker compose -f docker-compose.x86.yml up\n\n# if Jetson\n../../utils/check-environment-compatible && \\\n   docker compose -f docker-compose.l4t.yml up\n\n# Ctrl+C to stop running the compose bundle\n\n# to get back to the project root\ncd ../..\nЗапуск конвейера в первый раз может занять несколько секунд из-за компиляции модели в формат TensorRT, что занимает время.\nТеперь откройте URL \nrtsp://127.0.0.1:8554/city-traffic-processed\n в вашем любимом медиаплеере (мы рекомендуем VLC), и вы должны увидеть результирующее видео как представлено ниже: \nВ качестве альтернативы вы можете использовать ваш любимый браузер для доступа к потоку, открыв \nhttp://127.0.0.1:8888/city-traffic-processed/\n. Однако трансляция HLS добавляет задержку в несколько секунд.\nЧтобы завершить демонстрацию, нажмите \nCtrl+C\n. Теперь вы готовы углубиться в подробности о том, как выглядят и работают модули и адаптеры Savant.\nКак модуль Savant взаимодействует с миром\nВ Savant, когда он используется в продуктиве, модуль аналитики взаимодействует с внешними системами, используя потоковый API, основанный на ZeroMQ и Apache Avro.\nПротокол передает видеоинформацию и набор метаданных, которые могут быть использованы в качестве входных конфигурационных параметров и накапливаются во время обработки видео. Протокол сообщений ввода и протокол сообщений вывода идентичны.\nТакая реализация позволяет каскадировать модули аналитики, например, распределить обработку между несколькими вычислительными устройствами с различной специализацией.\nАдаптер\n — это специальное приложение, которое отправляет данные в модуль или получает результаты из модуля, используя протокол Savant. Такая архитектура делает систему устойчивой к отказам источника или приемника: адаптеры могут иметь проблемы, но модуль аналитики будет продолжать работать. Кроме того, концепция адаптера позволяет абстрагировать обработку от типа источника данных — вы можете отправлять файлы, непрерывные видеопотоки (например, RTSP), наборы изображений и т. д. в модуль.\nВ этой статье мы будем использовать три адаптера, которые поставляются с фреймворком:\nVideo File Source\n для обработки видеофайлов и коллекций видеофайлов; \nRTSP Source\n для обработки входящих потоков RTSP; \nAlways-On RTSP Sink\n для трансляции результатов по RTSP.\nЗнакомство с адаптерами\n \nAlways‑On RTSP Sink\n — это выходной адаптер, который транслирует по RTSP видеопоток, состоящий либо из кадров, полученных из источника, либо из замещающего изображения с отметкой времени при ожидании новых кадров.\nТаким образом, этот адаптер генерирует непрерывный RTSP-поток независимо от наличия правильно работающего источника, переключаясь между отображением кадров и замещающим изображением на лету.\nАдаптер \nAlways-On RTSP Sink\n требует поддержку аппаратных энкодеров и декодеров Nvidia, что связано с тем, что он всегда выполняет транскодинг видео внутри себя.\nАрхитектура адаптера для любопытных\nМы создали этот адаптер, во многом, для демонстрационных целей, но поскольку на профессиональных GPU транскодирование по сути бесплатное и позволяет получить пару тысяч FPS, то его вполне можно использовать и в продуктиве.\nИдея на миллион\n: Часто, люди, которым надо передать RTSP куда‑то далеко по публичной сети не знаю как это сделать, чтобы картинка не сыпалась. Вы можете использовать пару адаптеров \nRTSP Source\n и \nAlways‑On RTSP Sink\n для передачи RTSP‑потоков по длинным публичным каналам с пляшущим джиттером — все будет проигрываться существенно лучше, чем получать доступ по RTSP напрямую — потоки не будут сыпаться, если будут задержки фреймов, будет показываться картинка.\nАдаптер работает по принципу RTSP push и поэтому требует RTSP-сервер для широковещательной трансляции видео конечным пользователям; мы будем использовать надежный и открытый медиа-сервер \nMediaMTX\n.\n# You are expected to be in Savant/ directory\n\ngit clone https://github.com/insight-platform/Fake-RTSP-Stream\ncd Fake-RTSP-Stream && docker compose up -d && cd ..\n\n# если хотите поделиться своей геопозицией с нами, запустите\n# следующую команду (это опционально), мы просто хотим узнать о \n# том, что вы попробовали Savant\n#\ncurl --silent -O -- https://hello.savant.video/peoplenet.html\n\nСейчас мы протестируем адаптер \nAlways-On RTSP\n в связке с адаптером \nVideo File Source\n, устанавливая прямое соединение между ними без промежуточного аналитического модуля (протокол Savant позволяет осуществить это):\nТеперь давайте запустим адаптер и убедимся, что он работает:\n# You are expected to be in Savant/ directory\n\ndocker run --gpus=all --rm -it \\\n--add-host=gw:host-gateway \\\n-e ZMQ_ENDPOINT=ipc:///tmp/zmq-sockets/video.ipc \\\n-e ZMQ_TYPE=ROUTER \\\n-e ZMQ_BIND=True \\\n-e SOURCE_ID=camera1 \\\n-e STUB_FILE_LOCATION=/stub_img/smpte100_1280x900.jpeg \\\n-e RTSP_URI=rtsp://gw:8554/stream1 \\\n-v `pwd`/samples/stub_imgs/:/stub_img/ \\\n-v /tmp/zmq-sockets:/tmp/zmq-sockets \\\nghcr.io/insight-platform/savant-adapters-deepstream:0.2.0-6.2 \\\npython -m adapters.ds.sinks.always_on_rtsp\nЗаметка для Jetson\n: Если вы запускаете код на Jetson, измените имя образа на “*-l4t”: “savant-adapters-deepstream” должен быть заменен на “savant-adapters-deepstream-l4t”.\nОбратите внимание на следующие переменные:\nZMQ_ENDPOINT\n, \nZMQ_TYPE\n и \nZMQ_BIND\n определяют сокет, используемый для соединения; значения, в сочетании с соответствующими параметрами входного адаптера, должны формировать допустимую схему подключения; \nSOURCE_ID\n определяет какой поток будет транслироваться RTSP; на практике, в некоторых случаях, кадры из разных источников могут приходить на сокет одновременно; \nSTUB_FILE_LOCATION\n задает путь к замещающему изображению;\nRTSP_URI\n указывает URI, по которому будет публиковаться видео-поток.\nПосле запуска адаптера можно уже наблюдать поток RTSP, открыв URL \nrtsp://127.0.0.1:8554/stream1\n. Так как в потоке пока еще нет реальных данных, вы должны увидеть замещающее изображение.\nВ качестве альтернативы доступа к потоку можно использовать браузер, открыв \nhttp://127.0.0.1:8888/stream1/\n; однако, HLS вносит задержку в несколько секунд - не рекомендуем.\nТеперь давайте отправим видеофайл в \nAlways-On RTSP Sink\n, используя адаптер \nVideo File Source\n, чтобы продемонстрировать, как \nAlways-On RTSP Sink\n отобразит его содержимое. \nПеред отправкой откройте плеер VLC (или браузер) на видимой части экрана, чтобы увидеть, как адаптер будет транслировать содержимое файла через RTSP. \nЗапустите трансляцию содержимого видео-файла с помощью адаптера \nVideo File Source\n командой:\n# You are expected to be in Savant/ directory\n\ndocker run --rm -it \\\n--entrypoint /opt/app/adapters/gst/sources/media_files.sh \\\n-e ZMQ_ENDPOINT=ipc:///tmp/zmq-sockets/video.ipc \\\n-e ZMQ_TYPE=DEALER \\\n-e ZMQ_BIND=False \\\n-e READ_METADATA=False \\\n-e SYNC_OUTPUT=True \\\n-e SOURCE_ID=camera1 \\\n-e LOCATION=https://eu-central-1.linodeobjects.com/savant-data/demo/Free_City_Street_Footage.mp4 \\\n-e FILE_TYPE=video \\\n-v /tmp/zmq-sockets:/tmp/zmq-sockets \\\nghcr.io/insight-platform/savant-adapters-gstreamer:0.2.0\nЗаметка для Jetson\n: Если вы работаете на устройстве Jetson, измените имя образа на \"*-l4t\": \"savant-adapters-gstreamer\" должен быть изменен на \"savant-adapters-gstreamer-l4t\".\nОбратите внимание на следующие переменные среды:\nSYNC_OUTPUT\n указывает, что необходимо синхронизировать скорость отправки данных со скоростью FPS источника; если для вывода используется адаптер \nAlways-On RTSP Sink\n, то без синхронизации адаптер просто \"проглотит\" часть фреймов, потому что они будут обрабатываться слишком быстро; чтобы этого не произошло, мы на стороне источника данных замедляем поток до требуемого FSP;\nSOURCE_ID\n идентификатор источника; в данном случае он должен соответствовать соответствующему параметру, заданному для \nAlways-On RTSP Sink\n; \nLOCATION\n URL видео; \nFILE_TYPE\n тип файлов, используя значения video или picture.\nПосле запуска вы увидите содержимое видеофайла, проигрываемое в плеере. Когда файл будет отправлен полностью, вы снова увидите заглушку с изображением. Вы можете повторять этот эксперимент несколько раз, отправляя разные файлы в кодеках H.264 или HEVC.\nТаким образом, адаптеры позволяют сосредоточиться на обработке данных при разработке конвейера без необходимости беспокоиться о источниках и потребителях. Вы можете прочитать больше о адаптерах Savant в \nдокументации\n. Теперь давайте перейдем к созданию основного конвейера.\nНа этом этапе, пожалуйста, остановите все запущенные адаптеры, потому что позже мы запустим их с немного измененными параметрами, чтобы учесть наличие модуля между ними.\nДалее мы рассмотрим структуру модуля обработки для Savant.\nСтруктура модуля обработки видео\nМодуль обработки видео в рамках фреймворка Savant описывается файлом конфигурации в формате YAML (можно и с помощью Python его сконструировать, но мы здесь это не рассматриваем). Модуль запускается в контейнере Docker и взаимодействует с внешним миром через адаптеры.\nВажной особенностью Savant является возможность динамической обработки нескольких потоков: вы можете отправлять данные из различных адаптеров в модуль одновременно, указывая уникальный идентификатор: фреймворк автоматически разберет входящие потоки и обработает данные соответственно. \nДавайте кратко рассмотрим содержание каждого раздела файла конфигурации.\nРаздел \"parameters\"\nКонфигурационный файл начинается с названия конвейера и общих параметров. Они определяют разрешение, до которого масштабируются обработываемые кадры из всех потоков, с возможностью расширения размера \"холста\" кадра путем указания отступов.\nname: demo\nparameters:\n  frame:\n    width: 1280\n    height: 720\n    padding:\n      keep: true\n      left: 0\n      right: 0\n      top: 180\n      bottom: 0\n  output_frame:\n    codec: raw-rgba\n  draw_func:\n    module: samples.peoplenet_detector.overlay\n    class_name: Overlay\n    kwargs:\n      # kwargs are omitted for the sake of briefness\nВ демонстрации в верхней части кадра отображается информационная панель-дашборд, для которой определен отступ сверху в 180 пикселей. В результате этой манипуляции выходной кадр будет иметь разрешение 1280x900.\nДалее, если требуется отобразить на фрейме типовые аннотации объектов такие как рамки, метки и другие стандартные элементы, определение модуля должно включать параметр \ndraw_func\n. Этот специальный элемент конвейера выполняется непосредственно перед отправкой кадров на выход.\nРисование в Savant не использует низкоуровневый API DeepStream, поддерживает больше примитивов (в том числе повернутые прямоугольные рамки и произвольные многоугольники) и, благодаря реализации через OpenCV CUDA, сохраняет эффективность, работая непосредственно с видеопамятью и обеспечивая доступ к эффективно реализованным операциям, таким как гауссово размытие. Кроме того, возможно расширение функции рисования, если требуются дополнительные операции, которые мы не предусмотрели.\nВ демонстрации дополнительно отображаются спрайты панели инструментов и размываются лица на кадре, поэтому будет использоваться собственная реализация функции рисования.\nКроме того, в этой секции задается кодек выходного кадра. Если он не задан, то кадры не отправляются на выход, только метаданные — это используется в пайплайнах, где конструирование выходного видео не требуется.\nВ продуктовой среде следует использовать тип кодирования \nhevc\n или \nh264\n, однако здесь мы используем \nraw-rgba\n для обеспечения совместимости с видеокартами GeForce, которые имеют ограничение на количество одновременно кодируемых видеопотоков равное трем. \nВ общем, у нас тут тоже плачет котенок, потому что передача \nraw-rgba\n из памяти GPU в CPU легко снижает скорость работы на 40-50%. Однако, поскольку у большинства людей на рабочих машинах GeForce, а не Quadro или Tesla, мы оставили \nraw-rgba\n. В бенчмарке производительности мы используем кодирование в \nh264\n.\nРаздел \"pipeline\"\nРаздел \"pipeline\" содержит элементы конвейера, включая источники данных, выходные элементы и элементы обработки. Если источник и выход не заданы, используются элементы по умолчанию на основе ZeroMQ и Apache Avro.\nДавайте рассмотрим следующую диаграмму, чтобы вспомнить структуру конвейера:\nФункциональные блоки (желтые и синие) задаются в разделе \npipeline.elements\n, как показано в следующем примере кода:\npipeline:\n  elements:\n    - element: nvinfer@detector\n      ...\n    - element: nvtracker\n      ...\n    - element: pyfunc\n      ...\nТаким образом, упомянутый выше конвейер включает в себя детектор людей и лиц, за которым следует трекер людей, а затем настраиваемая функция Python.\nКаждый элемент имеет свои специфические параметры конфигурации и отвечает за выполнение определенной задачи, такой как обнаружение объектов, отслеживание или обработка изображений. Для создания полного конвейера обработки видео вы задаете несколько элементов в разделе \npipeline.elements\n, а фреймворк обеспечивает продвижение элементов потоков данных между ними.\nДавайте более подробно рассмотрим эти элементы обработки.\nДетектор на основе NvInfer\nПервым в последовательности элементов является элемент Nvinfer из DeepStream, который обеспечивает детектцию лиц и людей с помощью модели PeopleNet. Параметры модели могут быть указаны непосредственно в файле YAML.\nДля краткости мы предоставим только ключевые параметры, необходимые для понимания работы блока, опуская вспомогательные, которые можно найти в полном \nфайле\n конфигурации:\n- element: nvinfer@detector\n  name: peoplenet\n  model:\n    format: etlt\n    remote:\n      # where to download model\n      ...\n    model_file: resnet34_peoplenet_pruned.etlt\nФреймворк поддерживает два способа работы с файлами моделей:\nЛокальная упаковка в образ во время сборки контейнера или мэппинг с помощью отображения каталогов хоста в образ;\nЗагрузка моделей удаленно при первом запуске контейнера (в этом случае требуется постоянный том Docker, чтобы загруженные модели не исчезали при перезапуске контейнера).\nВ демонстрации используется метод удаленной загрузки (AWS S3), поэтому настраивается раздел \nremote\n, где указываются параметры загрузки.\nТеперь настроим вход модели, задав параметры препроцессинга. Эти настройки напрямую соответствуют параметрам из файла конфигурации модели Deepstream:\n# continuing PeopleNet config\ninput:\n  layer_name: input_1\n  shape: [3, 544, 960]\n  scale_factor: 0.0039215697906911373\nНастройка детектора завершается определением выходных слоев модели, классов объектов и их названий, а также параметров фильтрации для каждого класса объектов. Если какие-либо из классов не требуются для работы конвейера, их можно исключить, опустив соответствующее описание:\n# continuing peoplenet config\noutput:\n  layer_names: [output_bbox/BiasAdd, output_cov/Sigmoid]\n  num_detected_classes: 3\n  objects:\n    - class_id: 0\n      label: person\n      selector:\n        kwargs:\n          min_width: 32\n          min_height: 32\n    - class_id: 2\n      label: face\n      selector:\n        kwargs:\n          confidence_threshold: 0.1\nЭлемент \nselector\n выполняет дополнительную фильтрацию внутри класса: для класса \nperson\n исключаются объекты с высотой или шириной менее 32 пикселей.\nРазработчик конвейера может переопределить \nselector\n с помощью функции Python. Реализация по умолчанию доступна по \nссылке\n.\nОбнаруженные объекты передаются следующему элементу в конвейере, в нашем случае, трекеру людей.\nЭлемент трекера на основе NvTracker\nSavant поддерживает стандартные трекеры DeepStream, но также возможно реализовать свой трекер с помощью \npyfunc\n(обратите внимание на наш фреймворк трекеров \nSimilari\n). В этом приложении мы будем использовать стандартный \nтрекер\n Nvidia из DeepStream:\n- element: nvtracker\n  properties:\n    tracker-width: 640\n    tracker-height: 384\n    ll-lib-file: /opt/nvidia/deepstream/deepstream/lib/libnvds_nvmultiobjecttracker.so\n    ll-config-file: ${oc.env:APP_PATH}/samples/peoplenet_detector/config_tracker_NvSORT.yml\nДалее, конвейер выполняет аналитические операции на Python, а именно сопоставление лиц с человеческими телами и подсчет количества людей с обнаруженными лицами и без для панели инструментов. Для реализации этой функциональности будет использован элемент \npyfunc\n.\nЭлемент Pyfunc\nPyfunc\n - это класс Python, который имеет доступ к метаданным кадра, накопленным конвейером на текущий момент, и к самому кадру:\n- element: pyfunc\n  module: samples.peoplenet_detector.analytics\n  class_name: Analytics\n  kwargs:\n    counters_smoothing_period: 0.25\nКак и \ndraw_func\n, \npyfunc\n является одним из существенных преимуществ Savant по сравнению с базовым Deepstream. Такие элементы позволяют вставлять пользовательский Python-код в конвейер без необходимости понимания архитектуры GStreamer и необходимости писать большое количество шаблонного кода.\nМы не приводим анализ \nкода\n функции для краткости, но вы можете изучить его самостоятельно для лучшего понимания.\nЗапуск модуля фреймворка\nПерейдем к запуску модуля вместе с адаптерами. Для начала работы запустим контейнер с аналитическим модулем. В отличие от адаптеров, модуль использует два сокета: один для связи с источником, а другой для связи с адаптером вывода.\nЗапустим модуль:\n# You are expected to be in Savant/ directory\n\ndocker run --rm -it --gpus=all \\\n-e ZMQ_SRC_ENDPOINT=ipc:///tmp/zmq-sockets/input-video.ipc \\\n-e ZMQ_SRC_TYPE=ROUTER \\\n-e ZMQ_SRC_BIND=True \\\n-e ZMQ_SINK_ENDPOINT=ipc:///tmp/zmq-sockets/output-video.ipc \\\n-e ZMQ_SINK_TYPE=PUB \\\n-e ZMQ_SINK_BIND=True \\\n-v /tmp/zmq-sockets:/tmp/zmq-sockets \\\n-v `pwd`/samples:/opt/app/samples \\\n-v `pwd`/downloads/peoplenet_detector:/downloads \\\n-v `pwd`/models/peoplenet_detector:/models \\\nghcr.io/insight-platform/savant-deepstream:0.2.0-6.2-samples \\\nsamples/peoplenet_detector/demo.yml\nПримечание для Jetson:\n Если вы запускаете на Jetson, измените имя образа на \"*-l4t\": \"savant-deepstream\" должен быть заменен на \"savant-deepstream-l4t\".\nПосле запуска модуль выведет на консоль сообщение, указывающее на успешный запуск (при первом запуске конвейера может потребоваться некоторое время — из‑за компиляции модели в engine, что может занять пару десятков секунд; в продуктиве, если есть возможность работать с готовыми engine, лучше так и делать):\n2023-03-30 16:41:38,867 [savant.gstreamer.runner] [INFO] Pipeline starting \nended after 0:00:02.267221.\nОбработка видеопотока из файла\nПосле запуска модуля вы можете запустить адаптер \nAlways-On RTSP Sink\n. Мы уже демонстрировали, как правильно запустить \nAlways-On RTSP Sink\n с MediaMTX. Теперь мы запустим его с немного другими параметрами для корректного взаимодействия с модулем:\n# You are expected to be in Savant/ directory\n\ndocker run --gpus=all --rm -it \\\n--add-host=gw:host-gateway \\\n-e ZMQ_ENDPOINT=ipc:///tmp/zmq-sockets/output-video.ipc \\\n-e ZMQ_TYPE=SUB \\\n-e ZMQ_BIND=False \\\n-e SOURCE_ID=camera1 \\\n-e STUB_FILE_LOCATION=/stub_img/smpte100_1280x900.jpeg \\\n-e RTSP_URI=rtsp://gw:8554/stream1 \\\n-v `pwd`/samples/stub_imgs/:/stub_img/ \\\n-v /tmp/zmq-sockets:/tmp/zmq-sockets \\\nghcr.io/insight-platform/savant-adapters-deepstream:0.2.0-6.2 \\\npython -m adapters.ds.sinks.always_on_rtsp\nП\nримечание для Jetson\n: Если вы запускаете на Jetson, измените имя образа на \"*-l4t\": \"savant-adapters-deepstream\" должен быть заменен на \"savant-adapters-deepstream-l4t\".\nПосле запуска адаптера проверьте доступность потока RTSP, открыв адрес \nrtsp://127.0.0.1:8554/stream1\n. Оставьте видеоплеер открытым на видном месте, чтобы увидеть воспроизведение потока после получения данных.\nТеперь, когда конвейер готов к получению данных, давайте отправим данные в модуль с помощью адаптера \nVideo File Source\n:\n# You are expected to be in Savant/ directory\n\ndocker run --rm -it \\\n--entrypoint /opt/app/adapters/gst/sources/media_files.sh \\\n-e ZMQ_ENDPOINT=ipc:///tmp/zmq-sockets/input-video.ipc \\\n-e ZMQ_TYPE=DEALER \\\n-e ZMQ_BIND=False \\\n-e READ_METADATA=False \\\n-e SYNC_OUTPUT=True \\\n-e SOURCE_ID=camera1 \\\n-e LOCATION=https://eu-central-1.linodeobjects.com/savant-data/demo/Free_City_Street_Footage.mp4 \\\n-e FILE_TYPE=video \\\n-v /tmp/zmq-sockets:/tmp/zmq-sockets \\\nghcr.io/insight-platform/savant-adapters-gstreamer:0.2.0\nПримечание для Jetson\n: Если вы запускаете на Jetson, измените имя образа на \"*-l4t\": \"savant-adapters-gstreamer\" должен быть заменен на \"savant-adapters-gstreamer-l4t\".\nВ плеере вы должны увидеть результаты работы аналитического модуля, как показано в видео ниже:\nЖелаемый результат достигнут!\nОбработка потока RTSP\nКогда модуль был протестирован на видеофайле, давайте подключим его к потоку RTSP от IP-камеры. Для этого изменим адаптер источника.\nВ качестве замены IP-камеры будем использовать поток RTSP, созданный из зацикленного видеофайла с помощью репозитория Fake-RTSP-Stream:\nУбедитесь, что вы можете получить доступ к зацикленному потоку \nrtsp://127.0.0.1:8554/city-traffic\n с помощью VLC. Затем запустите адаптер \nRTSP Source,\n передав ему URI данного RTSP-потока:\n# you are expected to be in Savant/ directory\n\ndocker run --rm -it \\\n--add-host=gw:host-gateway \\\n--entrypoint /opt/app/adapters/gst/sources/rtsp.sh \\\n-e SOURCE_ID=camera1 \\\n-e RTSP_URI=rtsp://gw:8554/city-traffic \\\n-e SYNC_OUTPUT=True \\\n-e ZMQ_ENDPOINT=ipc:///tmp/zmq-sockets/input-video.ipc \\\n-e ZMQ_TYPE=DEALER \\\n-e ZMQ_BIND=False \\\n-v /tmp/zmq-sockets:/tmp/zmq-sockets \\\nghcr.io/insight-platform/savant-adapters-gstreamer:0.2.0\nПримечание для Jetson\n: если вы запускаете на Jetson, измените имя образа на \"*-l4t\": \"savant-adapters-gstreamer\" должен быть заменен на \"savant-adapters-gstreamer-l4t\".\nСейчас кадры из потока RTSP \ncity-traffic\n отправляются в модуль и должны отображаться в плейере по адресу \nrtsp://127.0.0.1:8554/stream1\n.\nНесколько потоков RTSP\nНаконец, стоит отметить, как легко масштабировать построенный конвейер до нескольких, например, трех источников. Для этого запустите еще два контейнера \nAlways-On RTSP Sink\n и два контейнера RTSP Source, присвоив каждому источнику свой собственный \nSOURCE_ID\n и URI потока. В самом модуле ничего менять не надо - обработка будет выполняться параллельно для трех источников.\nУ GPU GeForce есть аппаратное ограничение на количество одновременно кодируемых потоков, равное трём, поэтому нельзя запустить более трёх независимых потоков на этих картах с использованием адаптера \nAlways-On RTSP Sink\n. \nЕсли вам нужно передавать более трех различных потоков, вам нужно использовать профессиональные карты Nvidia, карты для центров обработки данных или устройства Jetson Edge, поскольку на них нет таких ограничений.\nЗавершение работы демонстрации\nДля освобождения ресурсов необходимо завершить все запущенные контейнеры (Ctrl+C, если контейнер запущен в интерактивном режиме) и вспомогательные контейнеры, которые использовались для эмуляции потоковой передачи RTSP:\n# You are expected to be in Savant/ directory\n\ncd Fake-RTSP-Stream\ndocker compose down \ncd ..\nИзмерение производительности\nДавайте оценим какую пиковую производительность конвейера мы сможем получить. Для повышения производительности мы включем кодирование выходных фреймов в \nh.264\n , так как передавать кадры \nraw-rgba\n из GPU в CPU и далее в 0MQ из-за их большого размера не эффективно  - это существенно снижает производительность обруботки данных.\noutput_frame:\n  codec: h264\nКроме того, мы исключим адаптеры из тестирования, чтобы измерить только производительность модуля: заменим источник на локальный файл и выход на \ndevnull_sink\n. Для этого мы добавим раздел источника в блок конвейера:\npipeline:\n  source:\n    element: uridecodebin\n    properties:\n      uri: file:///data/Free_City_Street_Footage.mp4\nА раздел вывода конвейера станет таким:\npipeline:\n  sink:\n    - element: devnull_sink\nВ результате вы получите конфигурацию, которую можно найти в файле \ndemo_performance.yml\n.\nДавайте загрузим видеофайл, который мы будем использовать для тестирования скорости работы модуля, на локальный компьютер:\n# you are expected to be in Savant/ directory\nmkdir -p var/data_in/demo\n\ncurl -o var/data_in/demo/Free_City_Street_Footage.mp4 \\\n   https://eu-central-1.linodeobjects.com/savant-data/demo/Free_City_Street_Footage.mp4\nВсе готово к запуску теста производительности с помощью следующей команды:\n# You are expected to be in Savant/ directory\n\ndocker run --rm -it --gpus=all \\\n-v `pwd`/samples:/opt/app/samples \\\n-v `pwd`/downloads/peoplenet_detector:/downloads \\\n-v `pwd`/models/peoplenet_detector:/models \\\n-v `pwd`/var/data_in/demo:/data:ro \\\nghcr.io/insight-platform/savant-deepstream:0.2.0-6.2-samples \\\nsamples/peoplenet_detector/demo_performance.yml\nПримечание для Jetson\n: Если вы запускаете на Jetson, измените имя образа на \"*-l4t\": \"savant-deepstream\" должен быть заменен на \"savant-deepstream-l4t\".\nПосле завершения обработки модуль выведет количество обработанных кадров и FPS в журнале. На виртуальной машине AWS c 4мя ядрами Xeon Platinum 8259CL и Nvidia Tesla T4 результат выглядит следующим образом:\n2023-04-08 14:31:04,755 [savant.demo_performance] [INFO] Processed 3478 frames, \n359.75 FPS.\nПользуясь случаем передаю привет GIL!\nДанный пайплайн требователен к производительности ядра CPU, потому что в функции на Python выполняется существенная для Python вычислительная нагрузка, что на слабых CPU может влиять на снижение производительности пайплайна и слабой загрузке GPU (наш инженер реализовал обработку данных с помощью Scikit и NumPy, чтобы здоровья читателей меньше потратить - можно упороться и сделать быстрее, конечно). \nДля оптимальной работы стоит предпочесть процессор с малым количеством быстрых ядрер, нежели с их большим количеством.\nТаким образом, в режиме обработки потокового видео в реальном времени конвейер с одной картой уровня Tesla T4 может обрабатывать до 14 камер с частотой кадров 25 FPS.\nЗаключение\nМы познакомились с фреймворком Savant и исследовали конвейер, который включает в себя обнаружение и отслеживание объектов, отображение видеоаналитики, такой как подсчет объектов и отрисовка рамок, текста, статических и анимированных спрайтов на кадре, а также применение гауссовского размытия к определенным областям кадра.\nРешение требует минимального количества кода и имеет высокую производительность благодаря использованию стека NVIDIA Deepstream; оно обладает устойчивостью к сбоям источников данных и масштабируемостью для параллельной обработки нескольких потоков одновременно. Конвейер может быть легко развернут на аппаратном обеспечении NVIDIA Edge, таком как Jetson, без каких-либо модификаций.\nЭто была первая демонстрация фреймворка Savant, сфокусированная на поведении и взаимодействии, а не на внутренних API. Мы будем благодарны за ваш интерес к нашим будущим публикациям. У нас есть \nTelegram канал\n GitHub \nDiscussions\n и \nDiscord\n. Если вам понравилось, поставьте \n🌠\n на GitHub.\n \n ",
    "tags": [
        "opencv",
        "deepstream",
        "nvidia",
        "neural networks",
        "computer vision",
        "deep learning",
        "cuda"
    ]
}