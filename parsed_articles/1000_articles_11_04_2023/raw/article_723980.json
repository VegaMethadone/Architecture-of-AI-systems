{
    "article_id": "723980",
    "article_name": "Тонкости настройки Grafana Loki",
    "content": "Привет! Меня зовут Игорь, я управляющий партнёр и системный архитектор в \nKTS\n. Мы занимаемся разными проектами, от создания корпоративных систем до нестандартных спецпроектов, мобильной разработкой и DevOps. Накопленный опыт позволяет помогать нашим клиентам справляться с инфраструктурой и её проблемными местами с помощью разных инструментов. \nВ этой статье, подготовленной по мотивам \nмоего доклада\n в «Школе мониторинга» Slurm, хочу поделиться своим набором best practices «Как лучше всего настроить Grafana Loki для сбора логов в инфраструктуре». \nНа мой взгляд, порог входа в Loki достаточно низкий, и в Интернете много туториалов. Поэтому я расскажу о более сложных и не совсем очевидных настройках, с которыми не раз сталкивался при работе с Grafana Loki. \nЧто будет в статье: \nЗадача сбора логов\nСпособы запуска Loki\nSingle-binary\nSimple scalable deployment\nMicroservices mode\nКак устроена архитектура Grafana Loki\nМинимальная конфигурация Loki (filesystem)\nS3 в качестве storage\nКонфигурация кластерных и High Availability решений\nТайм-ауты\nРазмеры сообщений\nЧанки\nПараллелизм\nОптимизация Write Path\nОбъемы данных в Grafana Cloud\nЧто вам с этого?\nС чего начинать учить DevOps?\nЗадача сбора логов\nЧетыре основных вопроса, которые нужно себе задать перед тем, как пытаться интегрировать какую-либо систему сбора логов:\nКак собрать логи?\nКак извлечь из них нужные метаданные, чтобы в будущем было легче идентифицировать логи?\nКак хранить эти данные, чтобы быстрее их записывать и находить? (самый сложный вопрос, пожалуй)\nКак найти логи?\nКаждая система, будь то syslog, Elasticsearch, или системы, которые построены на ClickHouse — даже сама Grafana Loki — отвечают на эти вопросы по-разному.\nПоэтому когда мы будем обсуждать архитектуру, то вернёмся к тому, чем концептуально отличается Grafana Loki от Elasticsearch, и почему она выигрывает в стоимости хранения логов.\nПайплайн сбора логов обычно выглядит просто и понятно.\nИтак, у нас есть большое количество разнообразных источников данных, откуда к нам прилетают логи: Kubernetes-кластеры, виртуальные машины, Docker-контейнеры и другие. Они проходят фазу сбора, процессинга, фильтрации. \nЗатем сохраняются в определенном виде, в зависимости от того, какое хранилище вы используете. Например, в виде базы данных, если работаете с ClickHouse, или в S3 Bucket в Grafana Loki. Но обратите внимание, что у каждого пользователя, который извлекает данные с другой стороны, могут быть разные сценарии действий. Например: извлечь логи за год или за последние 10 минут, чтобы отфильтровать данные по ним. \nСпособы запуска Loki\nСуществуют три способа запуска, которые, по большому счету, отличаются масштабированием. \nSingle-binary\nЭтот способ самый простой и используется в основном в первичных туториалах по Loki. Логика такая: мы берем бинарь, запускаем его, подключаем к storage.\nРоль Storage может исполнять как файловая система, на которой запущен наш процесс, так и удалённый S3 Bucket. Здесь это значения не имеет. Такой подход имеет свои плюсы. Например, лёгкость запуска — потребуется минимум конфигурации, которую мы рассмотрим ниже. Минус — плохая отказоустойчивость: если машина выпадает, то логи не пишутся вообще.\nСитуацию можно улучшить так: \nНа двух разных виртуальных машинах запустить один и тот же процесс Loki с одинаковым конфигом\nОбъединить их в кластер с помощью секции \nmemberlist\nПеред этим поставить любой прокси —например Nginx или Haproxy\nГлавное — подключить всё к одному Storage\nСоответственно, можно масштабировать процесс дальше, то есть запустить три, четыре, пять узлов.\nПолучится примерно такая схема:\nОбратите внимание, что Loki занимается как записью, так и чтением. Поэтому нагрузка равномерно распределяется по всем инстансам. Но по факту она совсем неравномерная, потому что в одни моменты времени бывает много чтений, а в другие — много записей. \nSSD - Simple Scalable Deployment\nВторой способ следует из первого и позволяет разделить процессы чтения и записи, чтобы мы могли запустить, например, более дискозависимые процессы на одном «железе», а менее дискозависимые на другом.\nДля запуска вам необходимо передать флажок \n-target=write\n или \n-target=read\n, и в каждом из этих процессов запускаются те сущности, которые отвечают за конкретный путь запроса: write или read. Точно так же нужно поставить прокси перед всеми инстансами, который будет проксировать:\nзапросы на запись → на узлы записи\nостальные запросы →  на узлы с чтением\nЭтот способ запуска Grafana считает наиболее рекомендуемым в плане работы и Grafana активно развивает именно его.\nMicroservices mode\nMicroservices mode — более развёрнутый путь, когда мы каждый компонент Loki запускаем самостоятельно.\nКомпонентов очень много, но они легко отделяются друг от друга, и их можно распределить на две группы или даже три.\nГруппа компонентов на запись: \nдистрибьюторы и инджесторы, которые пишут в Storage.\nГруппа компонентов на чтение:\n query-frontend, querier, index gateway. Это те компоненты, которые занимаются исполнением запросов.\nВсе остальные утилитарные компоненты\n: например, кеши, compactor и другие.\nКак устроена архитектура Grafana Loki\nОстановимся чуть подробнее на архитектуре, чтобы в дальнейшем понимать, что вообще конфигурируется в той или иной секции. \nДля начала — как индексируются данные в блоке. В отличие от Elasticsearch, который по дефолту индексирует все документы полнотекстово и целиком, Grafana Loki идет по другому пути - он индексирует не содержимое логов, а только их метаданные, то есть время и лейблы.\nЭти лейблы очень похожи на Prometheus-лейблы. Я думаю, многие из вас с ними знакомы. \nВ итоге в Grafana мы храним очень маленький индекс данных, потому что данных в нем очень мало. Здесь хочу заметить, что у Elasticsearch индекс раздувается зачастую больше, чем сами данные.\nНеиндексируемые данные мы храним как они есть в порядке появления. Если их нужно отфильтровать, пользуемся \"grep\", своего рода встроенным в Loki. \nStream\n — уникальный набор лейблов, несмотря на то, что логи могут идти из одного источника.\nВ данном случае лейбл \ncomponent=\"supplier\"\n порождает новый стрим. Нам это понадобится в дальнейшем, т.к. настройки, которые связаны с рейт-лимитами и ограничениями, зачастую распространяются именно на стрим.\nЧанки\n — набор из нескольких строчек логов. Вы взяли строчки лога, поместили их в одну сущность, назвали ее chunk, сжали и положили в Storage.\nТеперь вернёмся к архитектуре и подробнее рассмотрим write path и read path и  их различия.\nWrite path.\n Точкой входа для записи логов в Loki является сущность под названием «Дистрибьютор». Это stateless-компонент. Его задача — распределить запрос на один или несколько инджесторов.\nИнджесторы — это уже stateful-компоненты. Они объединены в так называемый \nhash ring\n — систему консистентного хеширования. Она позволяет проще добавлять и удалять инджесторы из этого кластера. Все они подключены к одному Storage. \nRead path \nустроен сложнее, но принцип похожий. Есть stateless-компоненты — query frontend и querier. Query frontend — компонент, который помогает разделить запрос, чтобы быстрее его выполнить. \nНапример, нужно запросить данные за месяц. С помощью query frontend делим запрос на более мелкие интервалы и направляем в параллель на несколько querier, а потом объединяем результат. \nQuerier\n — компонент, который запрашивает логи из Storage. Если есть новые данные из инжесторов, которые в Storage еще не записаны, querier запрашивает их тоже.\nМинимальная конфигурация Loki (filesystem)\nЭта конфигурация нацелена на работу с файловой системой, когда Loki запущен в single instance режиме. Остановлюсь на более важных конфигурационных секциях.\nLoki \n— это инструмент, который постоянно развивается, чтобы упростить и улучшить работу с конфигурацией. Пару версий назад появилось одно из самых лучших изменений — секция common. Когда в конфигурации есть несколько повторяющихся элементов, common объединяет их в разных частях конфига. То есть, если раньше приходилось настраивать \nring\n, \nstorage\n и другие элементы для инджестора, querier, дистрибьютора отдельно, то сейчас это все можно сделать в одном месте. \nauth_enabled: false\n\nserver:\n  http_listen_port: 3100\n\ncommon:\n  path_prefix: /tmp/loki\n  storage:\n    filesystem:\n      chunks_directory: /tmp/loki/chunks\n      rules_directory: /tmp/loki/rules\n  \n  replication_factor: 1\n\n  ring:\n    instance_addr: 127.0.0.1\n    kvstore:\n      store: inmemory\n\nschema_config:\n  configs:\n    - from: 2020-09-07\n      store: boltdb-shipper\n      object_store: filesystem\n      schema: v12\n      index:\n        prefix: loki_index_\n        period: 24h\nЗдесь видно, что \nstorage\n настроен в виде файловой системы, и указано, где хранятся чанки. Там же можно указать, где хранить индекс, правила для алертинга и т.д.\nschema_config\n — это конфиг для указания, как хранятся данные: чанки, индекс. Здесь в целом ничего не меняется с давних времен, но иногда появляются новые версии схемы. Поэтому рекомендую периодически читать чендж-логи, чтобы вовремя обновлять схемы в Loki и иметь последние улучшения.\nКак только появляется несколько инстансов, необходимо объединить их в кластер, который работает на протоколе memberlist (также можно использовать сторонние системы, такие как etcd или consul). Это \nGossip\n-протокол. Он автоматически находит узлы Loki по определенному принципу:\nauth_enabled: false\n\nserver:\n  http_listen_port: 3100\n\ncommon:\n  path_prefix: /tmp/loki\n  storage:\n    filesystem:\n      chunks_directory: /tmp/loki/chunks\n      rules_directory: /tmp/loki/rules\n  \n  replication_factor: 1\n\n  ring:\n    kvstore:\n      store: memberlist\n\nschema_config:\n  configs:\n    - from: 2020-09-07\n      store: boltdb-shipper\n      object_store: filesystem\n      schema: v12\n      index:\n        prefix: loki_index_\n        period: 24h\n\nmemberlist:\n  join_members:\n    - loki:7946\nЕсть множество разных способов автоматической конфигурации кластера, которые отлично работают. Например, в секции \nmemberlist.join_members\n можно указать разные настройки:\nАдрес одного хоста\nСписок адресов\ndns+loki.local:7946\n — Loki сделает A/AAAA DNS запрос для получения списка хостов\n \ndnssrv+_loki._tcp.loki.local\n — Loki сделает SRV DNS запрос для получения не только списка хостов, но и портов\n \ndnssrvnoa+_loki._tcp.loki.local\n — SRV DNS \n— \nзапрос без A/AAAA запроса\nЗачем это нужно?\n Внутри Loki есть компоненты, которые должны знать друг о друге. Например, дистрибьюторы должны знать об инджесторах. Поэтому они регистрируются в одном кольце. После этого дистрибьюторы знают, на какие инджесторы отправить запрос на запись. Еще в пример можно привести компакторы, которые должны работать в единственном экземпляре в кластере. \nS3 в качестве storage\nS3 — наиболее рекомендованный способ хранения логов в Loki, особенно, если вы деплоите Loki в Kubernetes. Когда мы используем S3 в качестве storage, немного меняется конфигурация:\nauth_enabled: false\n\nserver:\n  http_listen_port: 3100\n\ncommon:\n  path_prefix: /tmp/loki\n  storage:\n    \n    s3:  # Секция filesystem меняется на s3\n      \n      s3: https://storage.yandexcloud.net\n      bucketnames: loki-logs\n      region: ru-central1\n      access_key_id:\n      secret_access_key:\n  \n  replication_factor: 1\n\n  ring:\n    kvstore:\n      store: memberlist\n\nschema_config:\n  configs:\n    - from: 2020-09-07\n      store: boltdb-shipper\n      object_store: filesystem\n      schema: v12\n      index:\n        prefix: loki_index_\n        period: 24h\n\nmemberlist:\n  join_members:\n    - loki:7946\nНесколько советов:\nИспользуйте \nhttps://\n вместо \ns3://\n - так вы гарантируете использование шифрованного соединения\nbucketnames\n можно указать несколько для распределения хранения\nМожно использовать \nACCESS_KEY_ID\n и \nSECRET_ACCESS_KEY\n переменные окружения для конфигурации ключей доступа к S3\nМы меняем \nstorage\n на S3, указываем \nendpoint\n, \nbucketnames\n, и другие конфигурации, которые относятся к S3.\nОбратите внимание, что \nbucketnames\n во множественном числе, а значит, их можно указать сразу несколько. Тогда Loki начнёт равномерно распределять чанки по всем указанным бакетам, чтобы снизить нагрузку на один бакет. Например, это нужно, когда в вашем хостере есть ограничения по RPS на бакет.\nКонфигурация кластерных и High Availability решений\nДопустим, мы записали логи в несколько узлов. Один из них отказал и запросить данные из него нельзя, потому что он не успел записать их в storage. \nHigh Availability в Loki обеспечивается через опцию \nreplication_factor\n. Благодаря этой настройке дистрибьютор отправляет запрос на запись логов не в одну реплику, а сразу в несколько.\nauth_enabled: false\n\nserver:\n  http_listen_port: 3100\n\ncommon:\n  path_prefix: /tmp/loki\n  storage:\n    s3:\n      s3: https://storage.yandexcloud.net\n      bucketnames: loki-logs\n      region: ru-central1\n      access_key_id:\n      secret_access_key:\n  \n  replication_factor: 3  # Обратите внимание на это поле\n\n  ring:\n    kvstore:\n      store: memberlist\n\nschema_config:\n  configs:\n    - from: 2020-09-07\n      store: boltdb-shipper\n      object_store: filesystem\n      schema: v12\n      index:\n        prefix: loki_index_\n        period: 24h\n\nmemberlist:\n  join_members:\n    - loki:7946\nreplication_factor:\nDistributor отправляет чанки в несколько Ingester\nМинимум – 3 для 3х нод\nПозволяет не работать 1 из 3 нод\nmaxFailure = (replication_factor / 2) +1\nДистрибьютор отправляет чанки сразу в несколько инджесторов. Теперь в storage будет храниться в три раза больше данных. \nДа, это больше, чем нужно. Проблему можно решить через дедупликацию данных. Для этого есть компонент-компактор. Работает так:\nМы пишем с тремя инджесторами одни и те же данные;\nОбеспечиваем этим отказоустойчивость;\nПотом подчищаем с помощью компактора ненужные данные, которые лежат в дуплицированном виде в сторадже.\nТайм-ауты\nДостаточно тяжелая тема, потому что очень часто при неправильной настройке Loki можно встретить ошибки типа 502, 504. \nЧтобы лучше разобраться в ошибках, нужно, во-первых, увеличить таймауты до достаточных значений в вашем проекте, а во-вторых — правильно сконфигурировать несколько видов таймаутов. \nТаймауты \nhttp_server_{write,read}_timeout\n настраивают базовый таймаут на время ответа веб сервера\nquerier.query_timeout\n и \nquerier.engine.timeout\n настраивают максимальное время работы движка, непосредственно исполняющего запросы на чтение\nserver:\n  http_listen_port: 3100\n  http_server_write_timeout: 310s\n  http_server_read_timeout: 310s\n\nquerier:\n  query_timeout: 300s\n  engine:\n    timeout: 300s\nВ случае использования прокси перед Loki, например NGINX, следует увеличить таймауты и в нём (\nproxy_read_timeout\n и \nproxy_write_timeout\n)\nserver {\n  proxy_read_timeout = 310s;\n  proxy_send_timeout = 310s;\n}\nТакже необходимо увеличить таймаут на стороне Grafana. Это настраивается в секции \n[dataproxy]\n в конфиге.\n[dataproxy]\ntimeout = 310\nЛучше всего, если вы поставите минимальный из всех 4-х видов таймаутов у querier (в примере – 300s). Так он завершится первым, а все следующие — например НТТР-серверы, Nginx или Grafana — чуть дольше.\nДефолтный тайм-аут очень маленький, поэтому я рекомендую увеличивать эти значения.\nРазмеры сообщений\nТема может показаться сложной, потому что природа происхождения некоторых ошибок неочевидна. \nРазмеры сообщений, \ngrpc_server_max_{recv,send}_msg_size\n — это ограничения на возможный размер логов. При этом дефолтные значения очень маленькие. Например, если есть большой stack trace и в одной лого-линии отправляются логи размером 20 Мб, то он в принципе не влезет в этот лимит. Значит, его нужно увеличивать.\nserver:\n  http_listen_port: 3100\n  grpc_server_max_recv_msg_size: 104857600  # 100 Mb\n  grpc_server_max_send_msg_size: 104857600  # 100 Mb\n\ningester_client:\n  grpc_client_config:\n    max_recv_msg_size: 104857600  # 100 Mb\n    max_send_msg_size: 104857600  # 100 Mb\nДефолт 4Mb\nНепосредственно влияет на размер логов обрабатываемых Loki\nЭнкодинг чанков тоже нельзя обойти стороной. Дефолтное значение — gzip, то есть максимальное сжатие. Grafana рекомендует переключиться на snappy — и я по опыту с ними согласен. Тогда логи может и занимают чуть-чуть больше места в сторадже, но становятся более производительными чтения и записи данных.\ningester:\n  chunk_encoding: snappy\nДефолт — gzip\nЛучшее сжатие\nМедленнее запросы\nРекомендуем snappy\nСжатие чуть хуже\nОднако очень быстрое кодирование/декодирование\nЧанки\nС чанками связано много настроек относительно их размеров и периодов времени жизни. Рекомендую их сильно не трогать. Но при этом нужно понимать, что вы делаете, когда меняете значения.\ningester:\n  chunk_idle_period: 2h\n  chunk_target_size: 1536000\n  max_chunk_age: 2h\nДефолты достаточно хорошие:\nchunk_block_size\n и \nchunk_retain_period\n не рекомендуется менять совсем.\nсhunk_target_size\n можно увеличить, если чанки в основном полные. Это даст им больше пространства.\nсhunk_idle_period\n означает, сколько чанк будет жить в памяти инджестора, если в нём нет вообще никаких записей. Так вот, если ваши стримы в основном медленные и полупустые, лучше увеличить период. По дефолту — 30 минут.\nПараллелизм \nЕще один важный вопрос связан с конкурентностью. \nquerier:\n  max_concurrent: 8\n\nlimits_config:\n  max_query_parallelism: 24\n  split_queries_by_interval: 15m\n\nfrontend_worker:\n  match_max_concurrent: true\nquerier.max_concurrent\n показывает, сколько запросов в параллель может обрабатывать один querier. Рекомендовано ставить примерно удвоенное количество CPU, дефолт = 10 (будьте внимательны к этим цифрам).\nlimits_config.max_query_parallelism\n показывает, сколько максимум параллельности есть у тенанта. Значения querier.max_concurrent должны матчится с max query parallelism по формуле:  \n[queriers count] * [max_concurrent] >= [max_query_parallelism]\nВ нашем примере должны быть запущены минимум 3 \nquerier\n, чтобы обеспечить параллелизм 24.\nОптимизация Write Path\nЗдесь есть несколько настроек, связанных с записью — \ningestion_write_mb, ingestion_burst_size_mb\n.\nlimits_config:\n  ingestion_rate_mb: 20\n  ingestion_burst_size_mb: 30\nЕсли здесь стоят достаточно низкие дефолты, рекомендую увеличить их. Это позволит гораздо больше и чаще писать логи.  Остальные значения относятся к tenant, поэтому с ними нужно быть аккуратнее.\nДля стримов есть отдельная настройка — \nper_stream_rate_limit\n.\nlimits_config:\n  per_stream_rate_limit: \"3MB\"\n  per_stream_rate_limit_burst: \"10MB\"\nНа примере показаны более-менее нормальные дефолты. Но если вы начинаете в них упираться, то рекомендую разбить стрим на несколько — добавить лейбл. Это уменьшит rate-limit стрима. В обратной ситуации можно пробовать увеличивать лимиты. \nОбъемы данных в Grafana Cloud\nЭти данные я вытащил из одной их презентации.\nGrafana Loki обрабатывает:\n500 МБ логов в секунду\n43 ТВ в день;\n1,25 ПВ в месяц.\nОни стремятся обеспечивать нагрузку примерно 10 МВ в секунду на инджестор. При этом использование памяти всего лишь около 10 GB.\nЭти данные позволяют пользователям примерно представить, какую инфраструктуру им запускать. Для примера: у нас на одном из проектов rate гораздо ниже, около 20 или 30 GB в день. Тем не менее три инджестора справляются с таким потоком данных с большим запасом. \nИтоги\nВ статье и докладе, по которому она написана, я постарался подробно описать работу с Loki в контексте конфигурирования:\nзадача сборов логов и её 4 основных вопроса\nархитектура Loki\n3 вида запуска Loki\nHA в Loki с помощью replication factor\nконфигурация тайм-аутов и размера сообщений\nпараллелизм — следите за этими настройками очень внимательно! \nоптимизация write path — в целом несложный процесс. Вам нужно просто посмотреть на показатели на графике и оптимизировать соответствующим образом\nДругие статьи про DevOps для начинающих:\nСравнение Kubernetes с другими оркестраторами: Docker Swarm, Nomad, Openshift\nКак изучать Kubernetes джуну — и зачем\nКак деплоить приложение на Django в Kubernetes с нуля\nДругие статьи про DevOps для продолжающих:\nЗачем мы сделали собственный контроллер для копирования секретов в Kubernetes\nКак экономить ресурсы, затрачиваемые Kubernetes\nЗа последние годы де-факто стандартом оркестрации и запуска приложений стал Kubernetes. Поэтому умение управлять Kubernetes-кластерами является особенно важным в работе любого современного DevOps-инженера. \nПорог входа может казаться достаточно высоким из-за большого числа компонентов и связей между ними внутри системы. На курсе \n«Деплой приложений в Kubernetes»\n. мы рассмотрим самые важные концепции, необходимые для управления кластерами любой сложности и научим применять эти знания на практике.\n👉 \nПочитать про курс подробнее можно здесь:\n \nhttps://vk.cc/cn0ty6\n \n ",
    "tags": [
        "grafana",
        "loki",
        "prometheus",
        "devops",
        "kubernetes"
    ]
}