{
    "article_id": "727906",
    "article_name": "Лучшая нейросеть в генерации видео GEN-1: хардкорный архитектурный обзор",
    "content": "Видео - это один из самых популярных и влиятельных видов медиа в современном мире. Однако создание качественного и интересного видео требует много времени, ресурсов и навыков. Как было бы здорово, если бы можно было просто написать или нарисовать, что вы хотите видеть на экране, и получить готовое видео без лишних усилий? Это именно то, что обещает Gen-1 - новая технология для генерации видео с помощью слов и изображений от компании RunwayML. В этой статье вы узнаете что такое Gen-1, как она работает и как ее использовать для создания потрясающих видео на любую тему.\nВведение\nGen-1 - это технология генерации видео с помощью искусственного интеллекта, разработанная компанией Runway Research\n. Она позволяет использовать слова и изображения для создания новых видео из существующих, изменяя их содержание и стиль. Gen-1 основана на диффузионных моделях, которые могут эффективно и реалистично синтезировать видео с высоким разрешением. Gen-1 поддерживает различные режимы работы, такие как стилизация, создание сюжетов, маскирование, рендеринг и настройка. Gen-1 может быть полезна для творческих профессионалов, которые хотят экспериментировать с будущим кинематографии. Gen-1 является результатом научных исследований в области мультимодального синтеза изображений и видео.\nАрхитектура\nАрхитектура модели была описана в научной статье  \"Structure and Content-Guided Video Synthesis with Diffusion Models\".\nСтатья предлагает метод синтеза видео с помощью диффузионных моделей, которые могут изменять содержание существующего видео, сохраняя его структуру. Для этого авторы используют два компонента: структурный гид и контентный гид. Структурный гид - это диффузионная модель, которая обучается на видео без текстовых подсказок и генерирует видео с похожей структурой на исходное. Контентный гид - это диффузионная модель, которая обучается на изображениях с текстовыми подсказками и генерирует изображения с заданным содержанием. Для синтеза видео с новым содержанием авторы комбинируют выходы обоих гидов с помощью адаптивного взвешивания и усреднения. Это позволяет контролировать баланс между структурой и содержанием в сгенерированном видео.\nСтруктурный гид  \nСтруктурный гид - это диффузионная модель, которая обучается на видео без текстовых подсказок и генерирует видео с похожей структурой на исходное. Структура видео определяется как пространственное и временное расположение объектов и сцен. Для того, чтобы модель могла изучить структуру видео, авторы используют два типа данных: монокулярные оценки глубины и оптические потоки. Монокулярные оценки глубины показывают расстояние от камеры до каждого пикселя в кадре, а оптические потоки показывают направление и скорость движения каждого пикселя между кадрами. Авторы показывают, что использование этих данных улучшает качество и разнообразие сгенерированных видео.\nСтруктурный гид состоит из трех частей: энкодера, декодера и предиктора. Энкодер принимает на вход видео X и выдает зашумленное видео Z. Декодер обратно восстанавливает видео X из Z посредством диффузионного процесса, который состоит из T шагов. На каждом шаге t декодер вычисляет вероятность\n, где x_t и z_t - это частично зашумленные версии X и Z соответственно. Предиктор принимает на вход зашумленный кадр z_t и выдает оценки глубины d_t и оптических потоков f_t для следующего кадра. Эти оценки используются для регуляризации диффузионного процесса и повышения его стабильности. Модель обучается максимизировать правдоподобие \n на тренировочных данных.\nКонтентный гид  \nКонтентный гид - это диффузионная модель, которая обучается на изображениях с текстовыми подсказками и генерирует изображения с заданным содержанием. Содержание изображения определяется как семантические атрибуты объектов и сцен. Для того, чтобы модель могла изучить содержание изображения, авторы используют два типа данных: текстовые подсказки и маски сегментации. Текстовые подсказки описывают желаемое содержание изображения в естественном языке, а маски сегментации показывают границы объектов и сцен в изображении. Авторы показывают, что использование этих данных улучшает качество и разнообразие сгенерированных изображений.\nКонтентный гид также состоит из трех частей: энкодера, декодера и предиктора. Энкодер принимает на вход изображение Y и текстовую подсказку S и выдает зашумленное изображение W. Декодер обратно восстанавливает изображение Y из W посредством диффузионного процесса, который также состоит из T шагов. На каждом шаге t декодер вычисляет вероятность \n, где y_t и w_t - это частично зашумленные версии Y и W соответственно, а s - это эмбеддинг текстовой подсказки S. Предиктор принимает на вход зашумленный кадр w_t и выдает маску сегментации m_t для следующего кадра. Эта маска используется для регуляризации диффузионного процесса и повышения его стабильности. Модель обучается максимизировать правдоподобие \n на тренировочных данных.\nАдаптивное взвешивание и усреднение  \nЭто процедура, которая комбинирует выходы структурного и контентного гидов для получения нового видео X. Для этого она использует два параметра: коэффициент\n, который определяет степень изменения содержания видео, и функцию f(z_t), которая определяет степень изменения каждого кадра z_t. На каждом шаге t диффузионного процесса процедура вычисляет новый кадр x_t по формуле:\nгде y_t - это кадр, сгенерированный контентным гидом из w_t и s. Функция f(z_t) вычисляется как:\nгде x_t - это кадр, восстановленный структурным гидом из\n - это коэффициент, который зависит от дисперсии шума на шаге t. Таким образом, функция f(z_t) смешивает исходный кадр z_t и восстановленный кадр x_t в соответствии с уровнем шума. Процедура адаптивного взвешивания и усреднения позволяет гладко переходить от исходного видео к новому видео с заданным содержанием.\nВывод  \nGen-1 открывает новую эру для ИИ-видео. Она демонстрирует, что искусственный интеллект может быть мощным инструментом для творчества и рассказывания историй. Однако это только начало пути. В будущем мы можем ожидать еще более выразительных, реалистичных и управляемых систем генеративного ИИ, таких как Gen-2 от RunwayML, которая может не только генерировать новые видео с помощью одного лишь текста, но и расширяет существующие возможности GEN-1 . Это будет революционным переходом от первой версии, который расширит возможности для создания уникального и захватывающего видео-контента.\nПопробовать GEN-1 уже сейчас: \nhttps://runwayml.com/#generate-videos\nПочитать: \nhttps://arxiv.org/abs/2302.03011\n \n \n ",
    "tags": [
        "генерация изображений",
        "генерация видео",
        "stablediffusion",
        "dalle",
        "midjourney"
    ]
}