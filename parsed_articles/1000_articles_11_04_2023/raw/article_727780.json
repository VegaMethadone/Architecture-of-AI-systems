{
    "article_id": "727780",
    "article_name": "Как мы внедрили GPT-4 в несколько сфер компании и научили его отвечать на специфичные вопросы",
    "content": "Меня зовут Дмитрий Дударев, я технический директор в компании, которая занимается разработкой медицинских VR симуляторов.\nСтатья написана кожаным мешком, желающим поделиться опытом автоматизации некоторых сфер компании с помощью ИИ.\nПереводы с контекстом\nВнутри симуляторов происходят обследования, диагностика и лечение виртуальных пациентов. Все это сопровождается тоннами медицинских текстов с диалогами, историями болезней, результатами анализов и тд. Для управления этими данными у нас есть свой веб сервис, где медицинские эксперты могут создавать новые сценарии и редактировать контент. Тексты должны быть переведены на восемь разных языков и все это осложняется постоянно обновляющимися данными.\nРаньше для переводов мы использовали аутсорс переводчиков и системы краудтранслейтинга, где множество неизвестных людей в специальном онлайн сервисе накидывалось на куски текстов и переводили это за большие деньги и с кучей ошибок (после переводов данные проверяюстся мед экспертами - носителями языков).\nА потом появился chatGPT, поразивший меня своими возможностями. Он сходу умеет переводить тексты лучше существовавших до него специализированных нейросетей - переводчиков, и, в отличие от них, способен еще и учитывать контекст.\nЯ сразу купил API и сделал телеграм бота для предварительного тестирования (API OpenAI в России пока работает без VPN). Результаты превзошли все ожидания. Мы использовали бота не только для переводов, но и для придумывания новых текстов вроде имен пациентов и их ответов на некоторые вопросы.\nСледующим шагом мы встроили интерфейс нейронки в наш веб сервис и теперь можем нажатием одной кнопки перевести все десятки тысяч строк текста на 8 языков.\nКонечно, задача переводов текстов для медицинских симуляторов очень ответственная. Мы не хотим, чтобы запись в виртуальную карту пациента из \"неоформленный стул\" превратилась в \"undecorated chair\", поэтому для каждого типа данных мы указываем контекст. Для диалогов, например, в начало каждого запроса добавляется текст:\n\"Переведи следующе ответы пациента на вопросы доктора на арабский язык. Каждая строка - новый ответ. Сохраняй форматирование.\"\nКонечно, критические вещи проверяются медицинскими экспертами, но от огромной части работы нейронка нас освободила.\nОператор техподдержки\nВ целом, с переводами все просто. Гораздо более интересно применение нейронки в качестве оператора техподдержки, способного работать 24/7 на любых языках, отвечать мгновенно и знать все тонкости наших проектов и способы разрешения технических проблем.\nДля того, чтобы GPT мог отвечать на вопросы, специфичные для наших продуктов, нужно ему как-то скормить тонну текстов документаций, инструкций и траблшутингов. Для этого есть три способа:\nPrompt-engineering. \nЗаключается, как и в задаче с переводами, в добавлении в начало каждого запроса все необходимые текстовые данные. Очевидно, при таком способе мы сразу упремся в максимальную длину контекста (обычно это 4096 токенов) или в 0 на банковском счету.\nЧто такое токен\nFine-tuning. \nЭто механизм дообучения нейронки, в котором меняются сами веса модели. Вы предоставляете ей множество примеров \"запрос - ответ\" и она учится их использовать уже без добавления контекста в каждый запрос. Для этого у OpenAI есть специальный API. Такой подход первым бросился мне в глаза, но у него есть недостатки:\nЕсть сложности с дообучением при изменениях в документах.\nПредназначен не столько для накопления базы знаний, сколько для приобретения навыков общения.\nТоже стоит денег.\nИспользование LlamaIndex.\nЭто нечто среднее между первыми двумя вариантами. LlamaIndex - это система, которая может прочитать один раз кучу ваших документов и с помощью OpeanAI API создать индекс файл, содержащий ваши данные в виде векторов.  Этот файл создается один раз и позволяет по запросу на естественном языке вытащить необходимый кусок текста, который далее можно скормить GPT в качестве начальной информации. Т.е. вместо того, чтобы в начало каждого сообщения от клиента добавлять все гигабайты ваших документов, вы можете добавить только маленький нужный кусок данных, связанный с вопросом клиента. Ну и плюс еще предыдущие сообщения для удерживания контекста.\nПример использования LlamaIndex:\npip install llama-index\nСоздание Index файла из файлов:\nimport os\nos.environ[\"OPENAI_API_KEY\"] = 'YOUR_OPENAI_API_KEY'\nfrom llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\ndocuments = SimpleDirectoryReader('data').load_data()\nindex = GPTSimpleVectorIndex.from_documents(documents)\nСохраниение файла и использование для запроса данных:\n# save to disk\nindex.save_to_disk('index.json')\n# load from disk\nindex = GPTSimpleVectorIndex.load_from_disk('index.json')\nindex.query(\"<question_text>?\")  \nИтого после прихода сообщения от пользователя можно сформировать запрос нейронке, соединив несколько частей:\n\"Ты специалист техподдержки в компании ***. Компания занимается разработкой медицинских VR симуляторов. Вежливо отвечай на вопрос клиента на том же языке, на котором он говорит. Отвечай точно, не придумывай лишнего. В случае затруднения отвечай что позовешь специалиста.\nНиже представлена информация о наших продуктах:\"\nИнформация, полученная из индекс файла в ответ на сумму последних нескольких сообщений в чате\nСумма последних нескольких сообщений в чате\nПодход оказался удивительно эффективным. Бот отвечает корректно на нужном языке и в полном соответствии с нашими документами. В случае сомнений нейронка говорит, что позовет оператора, что по ключевому слову \"оператор\" отслеживается ботом и передается уведомление уже нашим биологическим операторам.\nОбщие мысли о GPT\nGPT - всего лишь языковая модель, обученная продолжать текст, но для успешного продолжения текста нужно иметь в голове глубокое понимание взаимосвязей между сущностями. Да, у нейронки пока нет визуальных, аудиальных, тактильных и других дополнительных ассоциаций со словами, но это не мешает ей уже сейчас уметь прикидываться человком с ограниченным объемом знаний и индивидуальными чертами характера.\nGPT показал удивительную эмерджентность, когда количество переходит в качество. С повышением количества параметров умения росли скачкообразно:\nСами разработчики уже во всю \nзаявляют \nо скором выходе действительно сильного искусственного интеллекта. Не знаю пока радоваться этому событию или сожалеть о грядущей утере какой либо ценности человеческих мозгов, но нас определенно ждет новый мир!\n \n ",
    "tags": [
        "gpt",
        "gpt-3",
        "gpt-4",
        "искусственный интеллект",
        "техподдержка",
        "перевод",
        "медицина",
        "стартапы",
        "vr"
    ]
}