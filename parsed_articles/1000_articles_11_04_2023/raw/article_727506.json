{
    "article_id": "727506",
    "article_name": "Выбор слоя активации в нейронных сетях: как правильно выбрать для вашей задачи",
    "content": "В машинном обучении и нейронных сетях слои активации играют очень важную роль в процессе обработки данных. В этой статье мы рассмотрим, что такое слои активации, как они работают и как выбрать наиболее подходящий слой для вашей задачи.\nЧто такое слои активации?\nСлои активации - это один из основных типов слоев, которые используются в нейронных сетях. Они представляют собой функцию, которая добавляет нелинейность к выходу предыдущего слоя. Это позволяет нейронной сети лучше моделировать сложные функции и более точно предсказывать результаты.\nКак работают слои активации?\nСлои активации принимают на вход результаты предыдущего слоя, называемые входом, и преобразуют их в выходное значение, которое передается следующему слою. Для этого они используют функцию активации, которая определяет, каким образом данные будут преобразованы.\nСигмойда\nКод создания графика сигмойды на matplot\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Задаем параметры сигмоиды\nx = np.linspace(-100, 100, 1000)\ny = 1 / (1 + np.exp(-x))\n\n# Строим график\nplt.plot(x, y)\n\n# Настраиваем оси координат и заголовок\nplt.xlabel('x')\nplt.ylabel('sigmoid(x)')\nplt.title('График сигмоиды')\n\n# Настраиваем значения осей координат\nplt.xlim(-10, 10)\nplt.ylim(0, 1)\n\n# Строим график, настраиваем ширину линии и добавляем сетку\nplt.plot(x, y, linewidth=5)\nplt.grid(True)\n\n# Отображаем график\nplt.show()\nСигмоидная функция активации - это нелинейная функция, которая преобразует входное значение в диапазоне от отрицательной бесконечности до положительной бесконечности в значение от 0 до 1. Эта функция активации часто используется в нейронных сетях для задач бинарной классификации.\nМатематически сигмоидная функция активации определяется следующим образом:\nГрафически сигмоидная функция активации выглядит как S-образная кривая, которая монотонно возрастает и имеет асимптоты на 0 и 1. В частности, если \nx\n > 0, то \nf(x)\n > 0.5, а если \nx\n < 0, то \nf(x)\n < 0.5. Значение 0.5 достигается при \nx\n = 0.\nСигмоидная функция активации используется для преобразования выходного значения нейрона в вероятность, т.е. вероятность того, что входное значение относится к классу 1, если мы работаем с задачей бинарной классификации. Если значение сигмоидной функции близко к 1, то вероятность того, что входное значение относится к классу 1, высока. Если значение близко к 0, то вероятность того, что входное значение относится к классу 1, низкая.\nОднако сигмоидная функция активации имеет недостаток, который называется проблемой затухания градиента (vanishing gradient problem). Это означает, что при использовании сигмоидной функции активации в глубоких нейронных сетях градиенты могут становиться очень маленькими, что затрудняет обучение. В таких случаях часто используется другая функция активации, например, ReLU (Rectified Linear Unit).\nReLU\nКод создания графика ReLU на matplot\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Задаем параметры сигмоиды\nx = np.linspace(-100, 100, 1000)\ny = np.maximum(x, 0)\n\n# Строим график\nplt.plot(x, y)\n\n# Настраиваем оси координат и заголовок\nplt.xlabel('x')\nplt.ylabel('ReLU(x)')\nplt.title('График функции ReLU')\n\n# Настраиваем значения осей координат\nplt.xlim(-5, 5)\nplt.ylim(-0.5, 5)\n\n# Строим график, настраиваем ширину линии и добавляем сетку\nplt.plot(x, y, linewidth=5)\nplt.grid(True)\n\n# Отображаем график\nplt.show()\nReLU (Rectified Linear Unit) - это нелинейная функция активации, которая широко используется в глубоком обучении. Она преобразует входное значение в значение от 0 до положительной бесконечности. Если входное значение меньше или равно нулю, то ReLU выдает ноль, в противном случае - входное значение.\nМатематически ReLU определяется следующим образом:\nгде \nmax\n - функция, возвращающая максимальное значение из двух.\nГрафически ReLU выглядит как линейная функция с нулевым отсечением на оси абсцисс в точке 0. Это значит, что функция имеет постоянный наклон во всех точках, кроме точки 0, где происходит отсечение.\nReLU имеет несколько преимуществ по сравнению со сигмоидной функцией активации. Во-первых, ReLU более вычислительно эффективна, поскольку она является простой и быстрой операцией, которая не требует вычисления экспоненты. Во-вторых, ReLU решает проблему затухания градиента, так как она не вызывает затухания градиента при обратном распространении ошибки, как это происходит в случае с сигмоидной функцией активации.\nОднако, ReLU имеет некоторые недостатки. Во-первых, при использовании ReLU, некоторые нейроны могут \"умереть\" (dead neurons), т.е. они могут получить отрицательное значение и оставаться неактивными на всем протяжении обучения. Во-вторых, ReLU несимметрична относительно нуля, поэтому может возникнуть проблема \"расслоения\" (clustering), когда нейроны могут выдавать только положительные значения. Для решения этих проблем могут быть использованы другие функции активации, такие как Leaky ReLU или ELU.\nLeaky ReLU\nКод создания графика Leaky ReLU на matplot\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Задаем параметры функции Leaky ReLU\nx = np.linspace(-10, 10, 1000)\nalpha = 0.1\ny = np.where(x > 0, x, alpha*x)\n\n# Строим график\nplt.plot(x, y)\n\n# Настраиваем оси координат и заголовок\nplt.xlabel('x')\nplt.ylabel('Leaky ReLU(x)')\nplt.title('График функции Leaky ReLU')\n\n# Настраиваем значения осей координат\nplt.xlim(-5, 5)\nplt.ylim(-0.5, 5)\n\n# Строим график, настраиваем ширину линии и добавляем сетку\nplt.plot(x, y, linewidth=5)\nplt.grid(True)\n\n# Отображаем график\nplt.show()\nLeaky ReLU (Rectified Linear Unit) - это функция активации, которая используется в нейронных сетях для введения нелинейности в выходные данные каждого нейрона.\nОбычный ReLU принимает входные значения и преобразует их, оставляя только положительные значения без изменения, а все отрицательные значения заменяет на 0. Однако у этого метода есть один недостаток, а именно \"умирание ReLU\". Это происходит в том случае, если входное значение отрицательное, то нейрон не будет активироваться и не будет вносить вклад в выходную функцию.\nДля решения этой проблемы был разработан Leaky ReLU. В отличие от ReLU, Leaky ReLU возвращает само значение при положительном входном значении, а при отрицательных значениях возвращает линейную функцию от входа, умноженную на небольшой коэффициент, называемый отрицательным уклоном (leak). Таким образом, у нейрона всегда есть возможность вносить вклад в выходную функцию, даже если входные данные отрицательны.\nФормула для Leaky ReLU выглядит следующим образом:\nгде \na (alpha)\n - отрицательный уклон, который является маленьким положительным числом, например, 0,01.\nПреимуществом Leaky ReLU является устойчивость к \"умиранию\" нейронов и лучшая сходимость в процессе обучения, что приводит к более быстрому и точному обучению нейронных сетей.\nELU\nКод создания графика ELU на matplot\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Задаем параметры функции ELU\nx = np.linspace(-10, 10, 1000)\nalpha = 1.0\ny = np.where(x > 0, x, alpha * (np.exp(x) - 1))\n\n# Строим график\nplt.plot(x, y)\n\n# Настраиваем оси координат и заголовок\nplt.xlabel('x')\nplt.ylabel('ELU(x)')\nplt.title('График функции ELU')\n\n# Настраиваем значения осей координат\nplt.xlim(-5, 5)\nplt.ylim(-2, 5)\n\n# Строим график, настраиваем ширину линии и добавляем сетку\nplt.plot(x, y, linewidth=5)\nplt.grid(True)\n\n# Отображаем график\nplt.show()\nELU (Exponential Linear Unit) - это функция активации, которая была предложена в 2015 году в статье \"\nFast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)\n\". Она представляет собой измененную версию ReLU (Rectified Linear Unit), которая помогает ускорить обучение глубоких нейронных сетей и справляется с проблемой \"мертвых нейронов\" (dead neurons).\nELU определяется следующим образом:\nгде \na (alpha)\n - это параметр, который может быть установлен в значение 1 по умолчанию.\nELU работает так же, как и ReLU, возвращая исходное значение входа, если он больше нуля. Однако, если значение входа меньше или равно нулю, то ELU использует экспоненциальную функцию, чтобы получить значение, которое ближе к нулю, чем значение, возвращаемое ReLU. Это позволяет избежать \"мертвых нейронов\" и ускорить обучение глубоких нейронных сетей.\nКроме того, ELU имеет свойство гладкости, которое так же помогает избежать проблемы \"взрывающегося градиента\" (exploding gradient), которая может возникать при использовании других функций активации, таких как ReLU. Это делает ELU более стабильной и более эффективной функцией активации для обучения глубоких нейронных сетей.\nОднако, как и любая другая функция активации, ELU не подходит для всех задач и может давать неоптимальные результаты в некоторых случаях. Поэтому при выборе функции активации необходимо учитывать особенности конкретной задачи и проводить эксперименты для определения оптимальной функции.\nSiLU\nКод создания графика SiLU на matplot\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Задаем параметры функции SiLU\nx = np.linspace(-10, 10, 1000)\ny = x * (1 / (1 + np.exp(-x)))\n\n# Строим график\nplt.plot(x, y)\n\n# Настраиваем оси координат и заголовок\nplt.xlabel('x')\nplt.ylabel('SiLU(x)')\nplt.title('График функции SiLU')\n\n# Настраиваем значения осей координат\nplt.xlim(-5, 5)\nplt.ylim(-1.5, 1.5)\n\n# Строим график, настраиваем ширину линии и добавляем сетку\nplt.plot(x, y, linewidth=2)\nplt.grid(True)\n\n# Отображаем график\nplt.show()\nSiLU (Sigmoid-weighted Linear Unit) - это нелинейная функция активации, которая была предложена в 2017 году в статье \"\nSigmoid-Weighted Linear Units for Neural Network Function Approximation in Reinforcement Learning\n\". SiLU сочетает в себе линейные и нелинейные свойства и имеет ряд преимуществ по сравнению с другими функциями активации.\nФункция активации - это нелинейная функция, которая применяется к выходу каждого нейрона в нейронной сети. Она используется для добавления нелинейности в вычисления нейрона и позволяет модели учиться более сложным функциям. Различные функции активации могут влиять на скорость обучения модели, точность и стабильность её предсказаний.\nОдной из самых распространенных функций активации является сигмоидная функция, которая представляет собой \"сжимающую\" функцию и применяется для преобразования значений в диапазон от 0 до 1. Однако, сигмоидная функция имеет некоторые недостатки, включая \"затухание градиентов\" и \"эффект насыщения\", что может затруднять обучение нейронных сетей.\nSiLU - это функция активации, которая решает проблемы \"затухание градиентов\" и \"эффект насыщения\". Она является гладкой, монотонно возрастающей и не имеет \"эффекта насыщения\" как у сигмойдной функции, что позволяет модели обучаться более эффективно и быстро сходиться к оптимальному решению.\nгде: \nσ\n(\nx\n) - функция сигмойды, формула написанная ниже, и подробнее объяснена в пункте выше \"Сигмойда\".\nВ области компьютерного зрения SiLU часто используется в сверточных нейронных сетях (CNN), где она может помочь увеличить точность и скорость обучения моделей. Например, её используют в модели YOLOv8. Но, к сожалению, этой модели нет в библиотеки TensorFlow, и использовать её просто так, у вас не получиться.\nГиперболический тангенс\nКод создания графика гиперболический тангенс на matplot\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Задаем параметры сигмоиды\nx = np.linspace(-100, 100, 1000)\ny = np.tanh(x)\n\n# Строим график\nplt.plot(x, y)\n\n# Настраиваем оси координат и заголовок\nplt.xlabel('x')\nplt.ylabel('tanh(x)')\nplt.title('График гиперболического тангенса')\n\n# Настраиваем значения осей координат\nplt.xlim(-10, 10)\nplt.ylim(-1, 1)\n\n# Строим график, настраиваем ширину линии и добавляем сетку\nplt.plot(x, y, linewidth=5)\nplt.grid(True)\n\n# Отображаем график\nplt.show()\nГиперболический тангенс (tanh) является одной из наиболее распространенных функций активации в нейронных сетях. Он используется как для классификации, так и для регрессии, а также для обработки изображений и других типов данных. \nЭто функция активации, которая преобразует входные значения в диапазоне от -1 до 1. Формула для вычисления гиперболического тангенса выглядит следующим образом  \nГиперболический тангенс очень похож на сигмоидальную функцию, которая также используется в нейронных сетях. Он принимает входные значения и преобразует их в диапазон от -1 до 1, что может использоваться для задач регрессии. Значения, близкие к -1, интерпретируются как отрицательные значения, а значения, близкие к 1, как положительные значения. Значения, близкие к нулю, обрабатываются как нейтральные.\nПо сравнению со сигмоидальной функцией, гиперболический тангенс имеет более пологую кривую, что позволяет сети лучше распознавать сложные зависимости в данных. Также гиперболический тангенс имеет гладкую производную, что позволяет использовать алгоритмы оптимизации, которые требуют вычисления градиента.\nSoftmax\nК сожалению, функция Softmax визуально представляется в виде кривой, что затрудняет ее графическое отображение на графике. Поэтому будет демонстрация на примере заданного вектора.\nКод создания функции Softmax с заданным вектором на matplot\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef softmax(z):\n    # Преобразуем вектор в массив, чтобы избежать ошибок типа \"integer division or modulo by zero\"\n    z = np.array(z)\n    # Вычисляем экспоненты каждого элемента вектора\n    exp_z = np.exp(z)\n    # Вычисляем сумму экспонент всех элементов вектора\n    sum_exp_z = np.sum(exp_z)\n    # Вычисляем вероятности для каждого элемента вектора\n    softmax_z = exp_z / sum_exp_z\n    return softmax_z\n\n# Задаем входной вектор\nz = [1, 2, 3, 4, 1, 2, 3]\n# Вычисляем значения Softmax\nsoftmax_z = softmax(z)\n\n# Выводим значения на экран\nprint(\"Softmax(z) =\", softmax_z)\n\n# Строим график вероятностного распределения\nplt.bar(range(len(z)), softmax_z)\nplt.title(\"Softmax Distribution\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Probability\")\nplt.show()\nВ этом примере мы задаем входной вектор \nz\n, затем вычисляем значения с помощью функции \nsoftmax()\n. Затем мы выводим значения на экран и строим график вероятностного распределения. График отображает вероятности для каждого элемента входного вектора в виде столбчатой диаграммы. Более детальные значения, вы можете увидеть ниже: \nSoftmax(z) = [0.02364054 0.06426166 0.1746813  0.474833   0.02364054 0.06426166  0.1746813 ]\n  \nФункция Softmax используется для преобразования вектора значений в вероятностное распределение, которое суммируется до 1. Она особенно полезна в многоклассовой классификации, где необходимо определить вероятности для каждого класса.\nФормула функции Softmax выглядит следующим образом:\nгде \nz_i\n - это элемент входного вектора, а \nk\n - это общее число элементов в векторе.\nГрафик функции Softmax представляет собой гладкую кривую, начинающуюся от 0 и заканчивающуюся на 1, что соответствует сумме вероятностей всех элементов вектора. Кривая функции Softmax имеет свойство, что вероятность любого элемента вектора увеличивается, если значения других элементов уменьшаются, что позволяет использовать эту функцию для многоклассовой классификации.\nХотя функция Softmax имеет множество применений в машинном обучении, она также может иметь недостатки, такие как чувствительность к выбросам и несбалансированным данным, что может приводить к неверным вероятностным оценкам.\nКак выбрать подходящий слой активации?\nВыбор подходящего слоя активации зависит от задачи машинного обучения, типа данных и модели, которую вы хотите создать. Вот несколько рекомендаций, которые могут помочь в выборе подходящего слоя активации:\nДля задач классификации, используйте Softmax, если вы хотите получить вероятности классов в качестве выходных данных. Используйте Sigmoid или Tanh, если вы хотите получить двоичный вывод.\nДля задач регрессии, используйте ReLU или его модификации, такие как LeakyReLU или ELU. Эти функции обычно дают лучшую производительность в задачах регрессии.\nДля моделей глубокого обучения, ReLU является общим выбором для скрытых слоев, так как она может ускорить обучение, но можно также использовать другие функции, например, PReLU или Swish.\nДля рекуррентных нейронных сетей, обычно используются функции активации Tanh.\nЕсли вы не уверены, какую функцию активации использовать, попробуйте использовать несколько функций активации и сравните их производительность на валидационном наборе данных.\nКроме того, при выборе функции активации необходимо учитывать свойства функции, такие как производная, способность обеспечивать нелинейность и способность предотвращать затухание градиента.\nВ целом, выбор подходящего слоя активации зависит от конкретной задачи и экспериментальных результатов. Необходимо тщательно подбирать функцию активации и изменять ее, чтобы получить оптимальные результаты для вашей модели.\nВажно также помнить, что выбор подходящего слоя активации может зависеть от структуры и архитектуры вашей нейронной сети, а также от данных, на которых вы обучаете модель. Поэтому важно экспериментировать с разными функциями активации и выбирать ту, которая работает лучше всего для вашей конкретной задачи.\nВ заключение, слои активации являются одним из ключевых элементов в нейронных сетях, которые позволяют моделировать сложные функции и более точно предсказывать результаты. В этой статье были перечислены далеко не все виды слоев активации, а только те, что наиболее популярны, на слуху, или я сам лично использовал их в своей работе.\nСпасибо за прочтение! \n \n ",
    "tags": [
        "искуственный интеллект",
        "нейронные сети",
        "функция активации",
        "машинное обучение",
        "градиентный спуск",
        "затухание нейронов",
        "мертвые нейроны",
        "взрыв градиента"
    ]
}