{
    "article_id": "725960",
    "article_name": "NER: Как мы обучали собственную модель для определения брендов. Часть 2",
    "content": "Привет всем! Сегодня продолжим рассказ о том, как наша команда Data Science из \nCleverData\n начала выделять бренды в строках онлайн-чеков. Цель такого упражнения — построение отчета для бренд-анализа, о котором мы подробно рассказали в \nпервой статье на эту тему.\n Из второй части вы узнаете, как на базе пайплайна (сводки с данными) для получения разметки по брендам мы обучили собственную NER-модель. \nПочему решили развивать собственную NER-модель\nПосле полученного воодушевляющего отчета на основе пайплайна, о котором мы говорили \nв прошлой статье\n (напомним,  использовали предобученные NER-модели на тэг Organization: Spacy, Natasha, NER Rus_Bert torch (DeepPavlov), NER Ontonotes Bert mult torch (DeepPavlov)), стало понятно, что игра стоит свеч и что имеет смысл развивать собственную NER-модель. Частично разметка была получена как раз тем «многомодельным» пайплайном. Но нам хотелось, чтобы новая модель была сильнее действующих, поэтому разметку мы дорабатывали. Да, вручную. Да, всей командой. Нет, не страшно. И даже очень классно. \nНапомним что, NER (Named entity recognition) — это распознавание именованных сущностей. Сущности — наиболее важные фрагменты конкретного предложения (словосочетания с существительными, глагольные словосочетания и др.). По-другому можно сказать, что NER — это процесс обнаружения в тексте именованных объектов. Например, имен людей, названий мест, компаний и т. д. В нашем конкретном случае, в отличие от классической задачи NER, мы будем выделять только один тип именованных сущностей — бренды.\nДля своих экспериментов мы использовали большие данные чеков онлайн-покупок, которые совершали исследуемые сегменты. Для обучения модели мы взяли в качестве данных случайные чеки, которые относятся к разным дням недели, так и к разным месяцам года (2022.01.02-2022.01.20, 2022.02.01-2022.02.15, 2022.03.01-2022.03.15). Общее число строк составило 415 тысяч. \nМодели, которые планировали тестировать: \nrubert-tiny\n, \nrubert-tiny2\n, \nparaphrase-multilingual-MiniLM-L12-v2\n, \ndistiluse-base-multilingual-cased-v1\n и \nDeBERTa-v2\n.\nКак планировали эксперимент\nОбщий пайплайн при первом приближении можно описать так.\nСтрока чека подвергается предобработке токенизатором, с целью корректного разделения слов (и знаков препинания) перед тэгированием чеков и подачей в токенизаторы моделей.\nЧеку сопоставляется последовательность тэгов формата [B — beginning, I — in brand, O — out of brand], согласно выделенным брендам (тэгирование). B-brand означает, что на текущем токене начинается название бренда; I-brand — что текущий токен является продолжением названия предыдущего токена;  O — токены, которые не относятся к брендам. \nДанные совершенно случайным образом разделяются на train-valid-test выборки.\nДанные [чек — список тэгов] подаются в трансформеробразную предобученную языковую модель, поверх которой ставится классификатор (на каждый элемент последовательности).\nВо время обучения считаются метрики (в качестве основной используется взвешенная f1-метрика): по каждой последовательности тэгов [B, I, …].\nДля получения разметки вида «строка чека — бренд(ы)» к этим данным был применён пайплайн, описанный в прошлой статье.\nПосле применения пайплайна с целью усиления разметки своими естественными нейросетями или методом «внимательного взгляда» мы классифицировали бренды, выделенные на уже обработанных данных, на три класса: \nусловно-чистые, очевидные (пример: Samsung, Tefal, Бирюса) — такие строки на всякий случай отсматривали, предполагая, что скорее всего ошибок не будет;\nподозрительные и неочевидные (пример: Apple. Вроде как Samsung, но ведь есть и духи Green Apple) — отсматривали и разделяли на нужное и ненужное;\n«мусорные» — те, которые мы в первой итерации вообще не отсматривали глазами, так как там сплошной мусор (пример: Сад. Какой сад? Фруктовый? Гигант? Или сокращение от «садовый»?).\nСемена овощей Русский огород 319029 Огурец Отелло F1 10 шт.\n \nА все ли так легко\nВо время разметки данных возникали вопросы следующего характера.\nВыделять ли бренд в чеке, если он не имеет отношения к самому товару (пример: «стекло для Apple iphone» — Apple не является брендом стекла).\nКакие именованные сущности выделять в строках, где указан один и тот же бренд на оригинальном языке и в русской транслитерации?\nНасколько глубоко нужно «проваливаться» вглубь линейки при выделении бренда (apple/apple iphone/apple iphone 12/ apple iphone 12 Pro/ …).\nПоразмыслив, при перепроверке разметки руководствовались двумя принципами: перечислять все бренды, которые указаны в строке и брать только основной бренд (не проваливаться в линейки).\nПосле обработки датасета получили данные следующего вида: 101336 строк (без дубликатов, очищенные от «спорных» для разметки строк), содержащие более тысячи уникальных брендов (1064).\n \nС брендом\nБез бренда\nИтого\nСтрок, тыс.\n75.6\n25.7\n101.3\nДоля, %\n74.64\n25.3\n100\nТокенизация \nПеред подачей в модель данные будут проходить этап токенизации, так как модели напрямую не работают с текстами. При этом, в зависимости от токенизатора, слитно написанные элементы могут рассматриваться как один токен. Например, 4.5% может быть рассмотрен как уникальный токен (как и 1% и др.), тогда как знак % сам по себе может быть информативным токеном и рассматривать (обучать) его стоит отдельно от связки с конкретным числом. \nДля того, чтобы наши данные токенизировались на желаемом уровне вне зависимости от самого принципа работы токенизатора, мы решили сделать подобное разделение на этапе препроцессинга. Для такого корректного разделения текстов по словам и символам, было рассмотрено четыре разных токенизатора. Выбрали nltk.tokenize.wordpunct_tokenize, так как он хорошо справлялся с разделением, в том числе при наличии специальных символов (например, ®).\nТаким образом, мы обработали достаточное, как нам казалось, количество строк. И в первой итерации на всех моделях получили высочайшие метрики. \nЭксперимент № 1\nВ качестве сэмпла для пробного обучения был взят датасет объемом 30 тысяч строк. Первая итерация обучения по всем моделям показала метрики 0.99+ f1 на train/validation/test-выборках.\nПри этом модели могли учиться более десяти эпох без признаков переобучения на валидационной выборке. Подозрительно, решили мы. Это натолкнуло нас на следующие вопросы.\nСтоит ли включить в валидационную/тестовую выборку бренды, неизвестные для тренировочной выборки?\nДостаточно ли используемой f1-метрики (по токенам) для оценки модели?\nНужно ли пересмотреть/перепроверить данные?\nВ самом деле, в тренировочной и в тестовой выборке встречаются достаточно похожие строки:\nАнализ ошибок эксперимента № 1\nК чему пришли.\n Надо найти те бренды, которые не выделяются пайплайном и добавить их в разметку.\nОдна из наших целей — выявлять в том числе те бренды, которых в обучающей выборке не наблюдалось. Поэтому сделаем два теста: один стратифицированный (набор брендов согласно распределению в обучающей и валидационной выборках), другой — неизвестный (только те бренды, которых нет в обучающей и валидационных выборках).\nБренды, не выявляемые пайплайном, — многословные и символосодержащие (внимательный читатель уже, наверное, задал себе и нам вопрос, что понимается под «словом» в понятии «многословный»).\nДобавить построчную метрику качества, то есть имеет смысл смотреть не только на потэговый взвешенный F1 score, но и на метрику, которая отражает корректность целиком предсказанной строки (построчная accuracy). Даже одна ошибка в определении тэгов строки будет влиять на итоговый результат: или мы получим лишние бренды (если неправильно определим O), или пропустим истинный бренд (неправильное определение B), или захватим лишь часть названия бренда (ошибка определения I-тэга).\nШаг назад\nПришлось возвращаться к разметке. И это было очень интересно. Имеем проблемы с многословными и символосодержащими брендами?  Давайте вместо того, чтобы пытаться их выявить,  пойдем  в обратном направлении.\nПоймем конструкцию многословных брендов (Dr. …, Mr…, person_1&person_2, пример — Dolce & gabbana).\nПо эвристикам, полученным в п. 1, найдем соответствующие таким брендам покупки.\nЭксперимент № 2\nИтак, обучаем.\nСостав дадасета:\nTrain\nValidation\nTest 1 (неизвестные для модели бренды)\nTest 2 (стратифицирован по брендам)\nСтрок, %\n70\n15\n6\n9\nБрендов, %\n90\n33\n10\n44\nПример поведения метрик:\nОтметим, что\n \nпри множестве тестовых запусков мы с удивлением обнаруживали, как модели сами исправляют ошибки разметки.\n  \n \nРезультаты \nПосле того, как разметка была исправлена, мы протестировали несколько предобученных моделей с разными гиперпараметрами. Лучшие результаты по каждой из моделей в таблице ниже.\nМодель\nТест «Неизвестные бренды»\nТест стратифицированный\nf1 (по лэйблам)\nAccuracy (построчно)\nf1 (по лэйблам)\nAccuracy (построчно)\nВремя инференса на 10 тыс. строк (gpu)\nmicrosoft/mdeberta-v3-base\n0.98\n0.84\n0.998\n0.98\n1min 16s\nparaphrase-multilingual-mpnet-base-v2\n0.92\n0.71\n0.999\n0.96\n3min 1s\ncointegrated/rubert-tiny2\n0.91\n0.58\n0.999\n0.986\n12.4 s\nparaphrase-multilingual-MiniLM-L12-v2\n0.91\n0.53\n0.998\n0.98\n1min 15s\nЛучшей по результатам оказалась deberta, идеально определяя 84% последовательностей в строках брендов, которых она никогда не видела, — отличный показатель для такой тяжелой задачи. Для стандартного стратифицированного теста метрика составила 98%, что также очень хорошо и более чем подходит для решения нашей задачи — построения отчёта для бренд-анализа.\nЕсли вы хотите следить за новостями CleverData — присоединяйтесь к нашему \nTelegram-каналу\n. \nСоавтор статьи — \n@Alex643\n \n ",
    "tags": [
        "бренд-анализ",
        "ланит",
        "персонализация",
        "маркетинговые исследования",
        "контент-маркетинг",
        "сегментация",
        "исследования и прогнозы в it",
        "повышение конверсии",
        "nlp",
        "ner"
    ]
}