{
    "article_id": "728030",
    "article_name": "Ускорение работы моделей Stable Diffusion на процессорах Intel",
    "content": "Недавно мы рассказывали о последнем поколении процессоров \nIntel Xeon\n (кодовое название Sapphire Rapids). Мы говорили об их новых аппаратных возможностях, ориентированных на ускорение задач глубокого обучения,  разбирались с тем, как использовать их для ускорения \nраспределённого дообучения\n трансформеров, занимающихся обработкой естественного языка, как применять их для ускорения \nработы\n таких моделей.\nВ этом материале мы собираемся остановиться на различных подходах к ускорению моделей Stable Diffusion на процессорах Sapphire Rapids. В следующем похожем посте речь пойдёт о распределённом дообучении.\nВо время написания этой статьи легче всего попробовать в деле сервер с процессором Sapphire Rapids можно, воспользовавшись инстансом Amazon EC2 семейства \nR7iz\n. Так как эти системы всё ещё находятся в статусе Preview, вам, чтобы получить к ним доступ, нужно \nзарегистрироваться\n. Я, как и в предыдущих материалах, пользуюсь инстансом \nr7iz.metal-16xl\n (64 vCPU, 512GB RAM) с установленной Ubuntu 20.04 AMI (\nami-07cd3e6c4915b2d18\n).\nКод к статье можно найти на \nGitLab\n.\nБиблиотека Diffusers\nБиблиотека \nDiffusers\n очень сильно упрощает генерирование изображений с помощью моделей Stable Diffusion. Если вы не знакомы с этими моделями — \nвот\n иллюстрированное объяснение принципов их работы.\nДля начала создадим виртуальное окружение, содержащее необходимые библиотеки: \nTransformers\n, \nDiffusers\n, \nAccelerate\n и \nPyTorch\n.\nvirtualenv sd_inference\nsource sd_inference/bin/activate\npip install pip --upgrade\npip install transformers diffusers accelerate torch==1.13.1\nТеперь напишем простую функцию для измерения производительности, которая по много раз запускает процесс генерирования изображения и возвращает среднее время выполнения одного действия (среднее время ожидания результата).\nimport time\n\ndef elapsed_time(pipeline, prompt, nb_pass=10, num_inference_steps=20):\n    # разогрев\n    images = pipeline(prompt, num_inference_steps=10).images\n    start = time.time()\n    for _ in range(nb_pass):\n        _ = pipeline(prompt, num_inference_steps=num_inference_steps, output_type=\"np\")\n    end = time.time()\n    return (end - start) / nb_pass\nДалее — соберём \nStableDiffusionPipeline\n с типом данных \nfloat32\n, используемым по умолчанию, и измерим скорость генерирования изображений.\nfrom diffusers import StableDiffusionPipeline\n\nmodel_id = \"runwayml/stable-diffusion-v1-5\"\npipe = StableDiffusionPipeline.from_pretrained(model_id)\nprompt = \"sailing ship in storm by Rembrandt\"\nlatency = elapsed_time(pipe, prompt)\nprint(latency)\nСреднее время ожидания результата составило 32,3 секунды. Как показано в \nразделе Intel\n на Hugging Face, тот же код, запускаемый на процессорах Intel Xeon предыдущего поколения (кодовое название Ice Lake), выдаёт результат примерно через 45 секунд.\nБез применения каких-то особых приёмов процессоры Sapphire Rapids, на точно таком же коде, оказываются значительно быстрее процессоров Ice Lake.\nА теперь давайте их ускорим!\nOptimum Intel и OpenVINO\nПакет \nOptimum Intel\n ускоряет все этапы ИИ-конвейеров на архитектурах Intel. Его API очень похож на обычный API библиотеки \nDiffusers\n. Это до крайности облегчает адаптацию существующего кода под него.\nOptimum Intel поддерживает \nOpenVINO\n — опенсорсный набор инструментов Intel, направленный на организацию высокопроизводительной работы моделей глубокого обучения.\nOptimum Intel и OpenVINO можно установить так:\npip install optimum[openvino]\nЕсли переделывать под Optimum Intel предыдущий фрагмент кода — достаточно будет заменить \nStableDiffusionPipeline\n на \nOVStableDiffusionPipeline\n. Чтобы, что называется, «на лету», загрузить модель PyTorch и конвертировать её в формат OpenVINO, можно, при загрузке модели, установить флаг \nexport=True\n.\nfrom optimum.intel.openvino import OVStableDiffusionPipeline\n...\nov_pipe = OVStableDiffusionPipeline.from_pretrained(model_id, export=True)\nlatency = elapsed_time(ov_pipe, prompt)\nprint(latency)\n\n# Не забудьте сохранить экспортированную модель\nov_pipe.save_pretrained(\"./openvino\")\nOpenVINO автоматически оптимизирует модель в расчёте на формат \nbfloat16\n. Благодаря этому среднее время генерирования одного изображения теперь составляет 16,7 секунд. Мы получили очень приятное двукратное ускорение кода.\nВышеприведённый конвейер поддерживает динамические входные данные, не накладывает ограничений ни на количество изображений, ни на их разрешение. При применении Stable Diffusion приложение обычно ограничено одним (или несколькими) различными выходными разрешениями, такими, как 512x512 или 256x256. Получается, что имеет смысл попытка значительного ускорения кода путём переделывания конвейера под фиксированное разрешение. Если вы нуждаетесь в более чем одном выходном разрешении, вы можете просто поддерживать несколько экземпляров конвейера — по одному для каждого необходимого разрешения.\nov_pipe.reshape(batch_size=1, height=512, width=512, num_images_per_prompt=1)\nlatency = elapsed_time(ov_pipe, prompt)\nПрименение статического разрешения ведёт к резкому сокращению среднего времени генерирования изображения — до 4,7 секунд. А это — дополнительное 3,5-кратное ускорение.\nКак видите, OpenVINO — это простой и эффективный инструмент для ускорения работы моделей Stable Diffusion. Если применить этот инструмент в коде, запускаемом на процессорах Sapphire Rapids, то, в сравнении с работой обычного кода на процессорах Xeon семейства Ice Lake, получается почти 10-кратное ускорение.\nЕсли вы не можете или не хотите пользоваться инструментами OpenVINO, тогда вам могут пригодиться другие техники оптимизации, о которых мы поговорим ниже. Пристегните ремни!\nОптимизация системного уровня\nМодели Diffusers — это большие структуры, размеры которых исчисляются многими гигабайтами. Генерирование изображений — это операция, предусматривающая интенсивное использование памяти. Установка высокопроизводительной библиотеки для выделения памяти должна способствовать ускорению операций по работе с памятью, должна помогать параллельному выполнению таких операций несколькими ядрами процессора Xeon. Прошу обратить внимание на то, что подобная операция изменит стандартную библиотеку выделения памяти в системе. Конечно, можно вернуться к библиотеке, используемой по умолчанию, удалив новую библиотеку.\nОдинаковый интерес для нас представляют библиотеки \njemalloc\n и \ntcmalloc\n. Тут я показываю установку библиотеки \njemalloc\n, так как она способствует несколько большему улучшению производительности моих тестов. Эту библиотеку, кроме того, можно подстроить в расчёте на определённую рабочую нагрузку. Например — для того, чтобы максимизировать использование процессора. Подробности можно найти в \nэтом\n руководстве по настройке \njemalloc\n.\nsudo apt-get install -y libjemalloc-dev\nexport LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libjemalloc.so\nexport MALLOC_CONF=\"oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms: 60000,muzzy_decay_ms:60000\"\nДалее — установим библиотеку \nlibiomp\n для оптимизации параллельной обработки данных. Она является частью библиотеки \nIntel OpenMP* Runtime\n.\nsudo apt-get install intel-mkl\nexport LD_PRELOAD=$LD_PRELOAD:/usr/lib/x86_64-linux-gnu/libiomp5.so\nexport OMP_NUM_THREADS=32\nИ наконец — устанавливаем инструмент командной строки \nnumactl\n. Он позволяет прикреплять Python-процессы к определённым ядрам и избегать некоторой части избыточной траты ресурсов, связанной с переключением контекста.\nnumactl -C 0-31 python sd_blog_1.py\nБлагодаря этим оптимизациям изначальный код, использующий \nDiffusers\n, теперь показывает результат в 11,8 секунд. Это — почти в 3 раза быстрее, и достигается это без каких-либо изменений кода. Эти инструменты, определённо, отлично работают на нашем 32-ядерном Xeon.\nНо мы ещё далеко не закончили. Добавим в наш арсенал пакет Intel Extension for PyTorch.\nIPEX и BF16\nПакет \nIntel Extension for Pytorch\n (IPEX) расширяет PyTorch и пользуется возможностями аппаратного ускорения, которые имеются в процессорах Intel. Например — это \nAVX-512\n Vector Neural Network Instructions (AVX512 VNNI) и \nAdvanced Matrix Extensions\n (AMX).\nУстановим пакет:\npip install intel_extension_for_pytorch==1.13.100\nЗатем отредактируем код, оптимизировав все элементы конвейера с использованием IPEX (получить их список можно, выведя на печать объект \npipe\n). Для этого потребуется конвертировать их в формат \ntorch.channels_last\n.\nimport torch\nimport intel_extension_for_pytorch as ipex\n...\npipe = StableDiffusionPipeline.from_pretrained(model_id)\n\n# преобразование в формат torch.channels_last\npipe.unet = pipe.unet.to(memory_format=torch.channels_last)\npipe.vae = pipe.vae.to(memory_format=torch.channels_last)\npipe.text_encoder = pipe.text_encoder.to(memory_format=torch.channels_last)\npipe.safety_checker = pipe.safety_checker.to(memory_format=torch.channels_last)\n\n# Создание случайных входных данных для включения JIT-компиляции\nsample = torch.randn(2,4,64,64)\ntimestep = torch.rand(1)*999\nencoder_hidden_status = torch.randn(2,77,768)\ninput_example = (sample, timestep, encoder_hidden_status)\n\n# Оптимизация с использованием IPEX\npipe.unet = ipex.optimize(pipe.unet.eval(), dtype=torch.bfloat16, inplace=True, sample_input=input_example)\npipe.vae = ipex.optimize(pipe.vae.eval(), dtype=torch.bfloat16, inplace=True)\npipe.text_encoder = ipex.optimize(pipe.text_encoder.eval(), dtype=torch.bfloat16, inplace=True)\npipe.safety_checker = ipex.optimize(pipe.safety_checker.eval(), dtype=torch.bfloat16, inplace=True)\nМы, кроме того, включили использование формата данных \nbloat16\n для того чтобы задействовать ускоритель AMX Tile Matrix Multiply Unit (TMMU) в процессорах Sapphire Rapids.\nwith torch.cpu.amp.autocast(enabled=True, dtype=torch.bfloat16):\n    latency = elapsed_time(pipe, prompt)\n    print(latency)\nЭта обновлённая версия кода демонстрирует дальнейшее снижение среднего времени ожидания — с 11,9 секунд до 5,4 секунд. Это значит, что IPEX и AMX дали нам двукратное ускорение.\nМожно ли выжать из процессоров Sapphire Rapids ещё немного скорости? Да — можно. В этом нам помогут планировщики.\nПланировщики\nБиблиотека \nDiffusers\n позволяет прикрепить \nпланировщик\n к конвейеру Stable Diffusion. Планировщики пытаются найти наилучший компромисс между скоростью и качеством устранения шума.\nВот что написано об этом в документации: «На момент написания этого документа \nDPMSolverMultistepScheduler\n даёт, вероятно, наилучшее соотношение скорость/качество, работа может быть выполнена всего за 20 шагов».\nПопробуем эту штуку:\nfrom diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n...\ndpm = DPMSolverMultistepScheduler.from_pretrained(model_id, subfolder=\"scheduler\")\npipe = StableDiffusionPipeline.from_pretrained(model_id, scheduler=dpm)\nЭта последняя версия кода демонстрирует дальнейшее снижение среднего времени ожидания результата. Теперь это — 5,05 секунды. Если сравнить это с нашим исходным экспериментом на Sapphire Rapids (32,3 секунды), то мы получим почти 6,5-кратное ускорение!\nРезультаты испытания разных вариантов кода на процессорах Intel Ice Lake (первый столбец слева) и Intel Sapphire Rapids. Тесты проводились в следующем окружении: Amazon EC2 r7iz.metal-16xl, Ubuntu 20.04, Linux 5.15.0-1031-aws, libjemalloc-dev 5.2.1-1, intel-mkl 2020.0.166-1, PyTorch 1.13.1, Intel Extension for PyTorch 1.13.1, transformers 4.27.2, diffusers 0.14, accelerate 0.17.1, openvino 2023.0.0.dev20230217, optimum 1.7.1, optimum-intel 1.7*\nИтоги\nВозможность создания высококачественных изображений за время, исчисляемое секундами, должна прийтись очень кстати во многих ситуациях. Например — в клиентских приложениях, в задачах генерирования контента для маркетинга и сетевых изданий, при формировании синтетических картинок, используемых для расширения наборов данных.\nВот несколько полезных ресурсов на эту тему:\nДокументация\n по Diffusers.\nДокументация\n по Optimum Intel.\nСтраница \nIntel IPEX\n на GitHub.\nМатериалы для разработчиков\n от Intel и Hugging Face.\nЕсли у вас есть вопросы или пожелания — мы ждём их на форуме \nHugging Face\n.\nСпасибо всем, кто дочитал до этого места!\nО, а приходите к нам работать? 🤗 💰\nМы в \nwunderfund.io\n занимаемся \nвысокочастотной алготорговлей\n с 2014 года. Высокочастотная торговля — это непрерывное соревнование лучших программистов и математиков всего мира. Присоединившись к нам, вы станете частью этой увлекательной схватки.\nМы предлагаем интересные и сложные задачи по анализу данных и low latency разработке для увлеченных исследователей и программистов. Гибкий график и никакой бюрократии, решения быстро принимаются и воплощаются в жизнь.\nСейчас мы ищем плюсовиков, питонистов, дата-инженеров и мл-рисерчеров.\nПрисоединяйтесь к нашей команде\n.\n \n ",
    "tags": [
        "Stable diffusion",
        "машинное обучение"
    ]
}