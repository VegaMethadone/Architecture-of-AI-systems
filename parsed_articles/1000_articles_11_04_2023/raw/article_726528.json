{
    "article_id": "726528",
    "article_name": "Подключаем VictoriaMetrics в Deckhouse и настраиваем уведомления в Telegram",
    "content": "В статье мы рассмотрим, как в Kubernetes-кластере под управлением Deckhouse удобно и быстро настраивать мониторинг с уведомлениями в Telegram. Воспользуемся VictoriaMetrics для хранения метрик, добавим дашборд в Grafana, создадим алерт и настроим оповещение.\nПодготовка окружения\nНам потребуются:\nКластер Kubernetes с установленным \nDeckhouse\n и включенным \nмодулем мониторинга\n.\nkubectl. Команды можно выполнять как на master-узле, так и с любого другого устройства, \nнастроив доступ\n к API кластера.\nУстановленный Helm\n.\nУчетная запись Telegram.\nУстановка VictoriaMetrics\nРазвернём \nVictoriaMetrics\n в пространстве имен \nvictoria-metrics-test\n, выполнив следующие команды для установки из Helm-чарта с конфигурацией по умолчанию:\nhelm repo add vm https://victoriametrics.github.io/helm-charts/ && \\\nhelm repo update && \\\nhelm upgrade --install victoria-metrics vm/victoria-metrics-single --namespace victoria-metrics-test --create-namespace\nПодключение VictoriaMetrics к Prometheus\nВнешнее хранилище метрик в Deckhouse подключается с помощью CustomResource \nPrometheusRemoteWrite\n:\n---\napiVersion: deckhouse.io/v1\nkind: PrometheusRemoteWrite\nmetadata:\n  name: victoria-metrics\nspec:\n  url: http://victoria-metrics-victoria-metrics-single-server.victoria-metrics-test.svc.cluster.local:8428/api/v1/write\nВ минимальной конфигурации в ресурсе PrometheusRemoteWrite достаточно указать адрес VictoriaMetrics. Также при необходимости можно ввести данные для аутентификации и настроить преобразование label’ов перед отправкой данных\n.\nВсе использованные конфигурационные файлы доступны \nв репозитории\n.\nПрименим конфигурационный файл в кластере:\nkubectl create -f https://raw.githubusercontent.com/flant/examples/master/2023/03-monitoring/prometheus-remote-write.yaml\nЗдесь мы использовали подготовленный заранее template из репозитория на GitHub. Это не обязательно — применить файл с нужным содержимым в кластере можно любым удобным способом.\nПодождем несколько минут и убедимся, что метрики поступают в кластер. Для этого достаточно посмотреть статус базы данных метрик \nна master-узле\n с помощью следующей команды:\ncurl $(kubectl --namespace victoria-metrics-test get ep -l \"app=server\" -o jsonpath=\"{.items[0].subsets[0].addresses[0].ip}\"):8428/api/v1/status/tsdb\nВ результате должно отобразиться примерно следующее:\n{\"status\":\"success\",\"data\":{\"totalSeries\":100477,\"totalLabelValuePairs\":1099074,\"seriesCountByMetricName\":[{\"name\":\"apiserver_request_slo_duration_seconds_bucket\",\"value\":15576},{\"name\":\"apiserver_request_duration_seconds_bucket\",\"value\":8496},{\"name\":\"etcd_request_duration_seconds_bucket\",\"value\":4284},{\"name\":\"apiserver_response_sizes_bucket\",\"value\":3080},{\"name\":\"apiserver_watch_events_sizes_bucket\",\"value\":1872},{\"name\":\"trivy_vulnerability_id\",\"value\":1541},{\"name\":\"workqueue_queue_duration_seconds_bucket\",\"value\":1089},{\"name\":\"workqueue_work_duration_seconds_bucket\",\"value\":1089},{\"name\":\"container_memory_failures_total\",\"value\":904},{\"name\":\"scheduler_plugin_execution_duration_seconds_bucket\",\"value\":903}],\"seriesCountByLabelName\":[{\"name\":\"__name__\",\"value\":100477},{\"name\":\"prometheus\",\"value\":100477},{\"name\":\"job\",\"value\":100171},{\"name\":\"instance\",\"value\":99736},{\"name\":\"tier\",\"value\":81434},{\"name\":\"service\",\"value\":57725},{\"name\":\"le\",\"value\":51720},{\"name\":\"resource\",\"value\":36606},{\"name\":\"version\",\"value\":34823},{\"name\":\"namespace\",\"value\":33844}],\"seriesCountByFocusLabelValue\":[],\"seriesCountByLabelValuePair\":[{\"name\":\"prometheus=deckhouse\",\"value\":100475},{\"name\":\"tier=cluster\",\"value\":81434},{\"name\":\"service=kubernetes\",\"value\":45705},{\"name\":\"instance=192.168.199.9:6443\",\"value\":45196},{\"name\":\"job=kube-apiserver\",\"value\":45196},{\"name\":\"component=apiserver\",\"value\":31879},{\"name\":\"version=v1\",\"value\":19859},{\"name\":\"scope=cluster\",\"value\":19344},{\"name\":\"container=kube-rbac-proxy\",\"value\":17796},{\"name\":\"__name__=apiserver_request_slo_duration_seconds_bucket\",\"value\":15576}],\"labelValueCountByLabelName\":[{\"name\":\"__name__\",\"value\":1870},{\"name\":\"name\",\"value\":693},{\"name\":\"resource\",\"value\":538},{\"name\":\"le\",\"value\":379},{\"name\":\"type\",\"value\":323},{\"name\":\"secret\",\"value\":282},{\"name\":\"hook\",\"value\":201},{\"name\":\"kind\",\"value\":198},{\"name\":\"controller_name\",\"value\":173},{\"name\":\"installed_version\",\"value\":167}]}}\n\"status\":\"success\"\n указывает на то, что все работает как нужно; если \ntotalSeries\n больше нуля, то данные уже начали накапливаться.\nПроконтролировать сбор метрик можно через веб-интерфейс VictoriaMetrics. Для этого пробросьте порт VictoriaMetrics на свой компьютер:\nexport POD_NAME=$(kubectl get pods --namespace victoria-metrics-test -l \"app=server\" -o jsonpath=\"{.items[0].metadata.name}\") &&  kubectl --namespace victoria-metrics-test port-forward $POD_NAME 8428\nДля успешного выполнения этой команды на машине, с которой выполняется запрос, должен быть настроен удаленный доступ для kubectl.\nВеб-интерфейс станет доступен по адресу \nhttp://localhost:8428\n:\nПерейдем на адрес \nhttp://localhost:8428/api/v1/status/tsdb\n — там также должен быть отображен статус \n\"status\":\"success\"\n; значение \ntotalSeries\n больше нуля будет сигнализировать об успешном сборе метрик:\nТеперь метрики хранятся в VictoriaMectrics. Подключим это хранилище к Grafana.\nПодключение VictoriaMetrics к Grafana\nДобавление нового datasource\nПодключение datasource к Grafana в Deckhouse выполняется с помощью CustomResource \nGrafanaAdditionalDatasource\n:\n---\napiVersion: deckhouse.io/v1\nkind: GrafanaAdditionalDatasource\nmetadata:\n  name: victoria-metrics\nspec:\n  access: Proxy\n  basicAuth: false\n  jsonData:\n    timeInterval: 30s\n  type: prometheus\n  url: http://victoria-metrics-victoria-metrics-single-server.victoria-metrics-test.svc.cluster.local:8428/\nПрименим его в кластере:\nkubectl create -f https://raw.githubusercontent.com/flant/examples/master/2023/03-monitoring/grafana-additional-datasource.yaml\nDeckhouse перезапустит Grafana с новой конфигурацией, проверим ее готовность:\nkubectl -n d8-monitoring get po -l app=grafana\nСтатус должен быть \nRunning\n:\nkubectl -n d8-monitoring get po -l app=grafana\nNAME                       READY   STATUS    RESTARTS   AGE\ngrafana-56df555c67-glzqr   3/3     Running   0          67s\nПроверка подключения\nЗайдем в веб-интерфейс Grafana и перейдем на вкладку \nConfiguration\n -> \nData sources\n:\nВ списке доступных источников должен отображаться \nvictoria-metrics\n:\nОткроем меню \nExplore\n:\nВыберем \nvictoria-metrics\n в качестве источника данных:\nРаскроем \nMetrics browser\n: в нем должен отобразиться список метрик, полученных из VictoriaMetrics:\nВсе настроено и готово к работе.\nДобавление алерта\nНастроим уведомление о событиях, создав новый алерт. \nВ Deckhouse алерты описываются в CustomResource \nCustomPrometheusRules\n:\n---\napiVersion: deckhouse.io/v1\nkind: CustomPrometheusRules\nmetadata:\n  name: always-firing-alert\nspec:\n  groups:\n  - name: cluster-state-alert.rules\n    rules:\n      - alert: PrometheusCanScrapeTragets\n        annotations:\n          description: This is a fake alert only for a demo.\n          summary: The alert shows that Prometheus can scrape targets.\n        expr: |\n          up{job=\"deckhouse\"}\nСозданный алерт «бесполезный» — он всегда активный, но хорошо подходит для целей тестирования.\nПрименим его в кластере:\nkubectl create -f https://raw.githubusercontent.com/flant/examples/master/2023/03-monitoring/custom-prometheus-rule.yaml\nДля проверки перейдем в Prometheus по адресу \n<адрес_Grafana>/prometheus/\n и откроем вкладку \nAlerts\n. В строке поиска введем имя алерта — \nPrometheusCanScrapeTragets\n:\nАлерт создан.\nДобавление дашборда в Grafana\nСоздадим дашборд \nServices Up\n в каталоге \nServices\n (через CustomResource \nGrafanaDashboardDefinition\n):\n---\napiVersion: deckhouse.io/v1\nkind: GrafanaDashboardDefinition\nmetadata:\n  name: up-services\nspec:\n  folder: Services\n  definition: |\n    {\n      \"annotations\": {\n        \"list\": [\n          {\n            \"builtIn\": 1,\n            \"datasource\": {\n              \"type\": \"grafana\",\n              \"uid\": \"-- Grafana --\"\n            },\n            \"enable\": true,\n            \"hide\": true,\n            \"iconColor\": \"rgba(0, 211, 255, 1)\",\n            \"name\": \"Annotations & Alerts\",\n            \"target\": {\n              \"limit\": 100,\n              \"matchAny\": false,\n              \"tags\": [],\n              \"type\": \"dashboard\"\n            },\n            \"type\": \"dashboard\"\n          }\n        ]\n      },\n      \"editable\": false,\n      \"fiscalYearStartMonth\": 0,\n      \"graphTooltip\": 0,\n      \"id\": 31,\n      \"links\": [],\n      \"liveNow\": false,\n      \"panels\": [\n        {\n          \"datasource\": {\n            \"type\": \"prometheus\",\n            \"uid\": \"P0D6E4079E36703EB\"\n          },\n          \"fieldConfig\": {\n            \"defaults\": {\n              \"color\": {\n                \"mode\": \"palette-classic\"\n              },\n              \"custom\": {\n                \"axisLabel\": \"\",\n                \"axisPlacement\": \"auto\",\n                \"barAlignment\": 0,\n                \"drawStyle\": \"bars\",\n                \"fillOpacity\": 15,\n                \"gradientMode\": \"none\",\n                \"hideFrom\": {\n                  \"legend\": false,\n                  \"tooltip\": false,\n                  \"viz\": false\n                },\n                \"lineInterpolation\": \"linear\",\n                \"lineWidth\": 10,\n                \"pointSize\": 5,\n                \"scaleDistribution\": {\n                  \"type\": \"linear\"\n                },\n                \"showPoints\": \"never\",\n                \"spanNulls\": false,\n                \"stacking\": {\n                  \"group\": \"A\",\n                  \"mode\": \"normal\"\n                },\n                \"thresholdsStyle\": {\n                  \"mode\": \"off\"\n                }\n              },\n              \"mappings\": [],\n              \"thresholds\": {\n                \"mode\": \"absolute\",\n                \"steps\": [\n                  {\n                    \"color\": \"green\",\n                    \"value\": null\n                  },\n                  {\n                    \"color\": \"red\",\n                    \"value\": 80\n                  }\n                ]\n              }\n            },\n            \"overrides\": []\n          },\n          \"gridPos\": {\n            \"h\": 26,\n            \"w\": 24,\n            \"x\": 0,\n            \"y\": 0\n          },\n          \"id\": 2,\n          \"options\": {\n            \"legend\": {\n              \"calcs\": [\n                  \"lastNotNull\"\n              ],\n              \"displayMode\": \"table\",\n              \"placement\": \"right\"\n            },\n            \"tooltip\": {\n              \"mode\": \"single\",\n              \"sort\": \"none\"\n            }\n          },\n          \"pluginVersion\": \"8.5.2\",\n          \"targets\": [\n            {\n              \"datasource\": {\n                \"type\": \"prometheus\",\n                \"uid\": \"P0D6E4079E36703EB\"\n              },\n              \"editorMode\": \"code\",\n              \"exemplar\": false,\n              \"expr\": \"sum by (job, scrape_endpoint, scrape_source) (up)\",\n              \"format\": \"time_series\",\n              \"instant\": false,\n              \"legendFormat\": \"{{ job }} {{ scrape_source }}\",\n              \"range\": true,\n              \"refId\": \"A\"\n            }\n          ],\n          \"title\": \"Up\",\n          \"transformations\": [],\n          \"type\": \"timeseries\"\n        }\n      ],\n      \"refresh\": \"30s\",\n      \"schemaVersion\": 36,\n      \"style\": \"dark\",\n      \"tags\": [],\n      \"templating\": {\n        \"list\": []\n      },\n      \"time\": {\n        \"from\": \"now-3h\",\n        \"to\": \"now\"\n      },\n      \"timepicker\": {\n        \"refresh_intervals\": [\n            \"30s\"\n        ]\n      },\n      \"timezone\": \"\",\n      \"title\": \"Services Up\",\n      \"uid\": \"f_8jGXenz\",\n      \"version\": 1,\n      \"weekStart\": \"\"\n    }\nИ применим его в кластере:\nkubectl create -f https://raw.githubusercontent.com/flant/examples/master/2023/03-monitoring/grafana-dashboard-definition.yaml\nПроверим, что все отработало, перейдя по пути \nServices \n->\n Services Up\n на вкладке \nDashboards\n:\nВ созданном дашборде отобразится вот такой «частокол» алертов, привязанных ко всем сервисам Deckhouse в кластере:\nНастройка уведомлений в Telegram\nПодключение к Telegram\nЕсли у вас уже есть ID чата, в который вы хотите получать уведомления, и токен бота, через которого вы хотите отправлять уведомления, то следующий этап можно пропустить и перейти сразу \nк настройке\n.\nСоздание бота и получение ID чата\nПодробную инструкцию по созданию бота можно найти \nв документации Telegram\n.\nВ строке поиска клиента Telegram введем адрес \nhttps://t.me/botfather\n и найдем бота \n@BotFather\n. Не перепутайте его с другими — у настоящего отображается значок верификации:\nВыберем его и нажмем Start. Затем отправим боту сообщение \n/newbot\n и ответим на поступившие вопросы, введя имя создаваемого бота и его ник.\nПример диалога создания бота \n@mytestalert2023bot\n:\nМы получим токен бота и ссылку. Пройдем по ней к диалогу с созданным ботом (в примере это \nhttp://t.me/mytestalert2023bot\n) и нажмем Start.\nТеперь нужно узнать ID чата, куда будут отправляться сообщения. \nЭто могут быть как личные сообщения, так и группа или канал в Telegram. Во втором случае нужно предварительно добавить в них бота, а в конфиге указывать ID канала.\nУзнаем ID учетной записи. Сделать это можно, выбрав бота \n@getmyid_bot\n и нажав \nStart\n. В ответ отобразится ID учетной записи Telegram:\nПроверим работу сообщений через API Telegram.\nВыполним следующую команду, указав вместо XXX токен бота (обратите внимание на префикс \nbot\n — токен идет после него), а вместо YYY —  ID учетной записи Telegram:\ncurl -X POST \"https://api.telegram.org/botXXX/sendMessage\" -d \"chat_id=YYY&text=text for test\"\nВ ответ отобразится сообщение об успешной отправке:\n{\"ok\":true,\"result\":{\"message_id\":2,\"from\":{\"id\":2222222222,\"is_bot\":true,\"first_name\":\"mytestbot\",\"username\":\"mytestalert2023bot\"},\"chat\":{\"id\":2222222222,\"first_name\":\"Joe\",\"type\":\"private\"},\"date\":1674327896,\"text\":\"text for test\"}}\n… а нам придет личное сообщение от созданного бота.\nНастройка отправки уведомлений\nСоздадим Secret \ntelegram-bot-secret\n, указав токен бота:\nkubectl create secret generic -n d8-monitoring telegram-bot-secret --from-literal=token=XXX\nСоздадим ресурс \nCustomAlertmanager\n:\n---\napiVersion: deckhouse.io/v1alpha1\nkind: CustomAlertmanager\nmetadata:\n  name: telegram\nspec:\n  type: Internal\n  internal:\n    route:\n      groupBy: [ 'job' ]\n      groupWait: 30s\n      groupInterval: 5m\n      repeatInterval: 12h\n      receiver: 'telegram'\n    receivers:\n      - name: telegram\n        telegramConfigs:\n          - botToken:\n              key: token\n              name: telegram-bot-secret\n            chatID: 111\nПрименим его в кластере:\nkubectl create -f https://raw.githubusercontent.com/flant/examples/master/2023/03-monitoring/alertmanager.yaml\nВ  пространстве имен \ndb-monitoring\n запустится Pod с AlertManager. Проверим, что он имеет статус \nRunning\n:\nkubectl -n d8-monitoring get po -l alertmanager\nПример вывода:\nNAME                      READY   STATUS    RESTARTS       AGE\nalertmanager-telegram-0   3/3     Running   1 (4m1s ago)   3m57s\nСейчас в журнале Alertmanager’а будут ошибки отправки, т. к. не указан ID чата, в который нужно отправлять сообщения. Посмотреть лог можно командой:\nkubectl -n d8-monitoring logs alertmanager-telegram-0 -c alertmanager\nID чата указывается в параметре \nspec.internal.receivers.telegramConfigs.chatID\n CustomAlertmanager. \nРесурс можно либо отредактировать вручную (\nkubectl edit customalertmanager telegram\n), либо воспользоваться следующей командой, указав в переменной \nCHAT_ID\n ID учетной записи Telegram:\nCHAT_ID=2222222222 && \\\nkubectl patch customalertmanager telegram --type json -p \"[{\\\"op\\\": \\\"replace\\\", \\\"path\\\": \\\"/spec/internal/receivers/0/telegramConfigs/0/chatID\\\", \\\"value\\\": ${CHAT_ID}}]\"\nAlertmanager обновит свою конфигурацию и пришлет сообщение об алерте \nPrometheusCanScrapeTragets, \nсозданном ранее, а также сообщения о других алертах, активных в кластере.\nУбираем за собой\nДля удаления созданных выше ресурсов выполните:\nkubectl delete prometheusremotewrite victoria-metrics\nkubectl delete grafanaadditionaldatasource victoria-metrics\nkubectl delete customprometheusrules always-firing-alert\nkubectl delete grafanadashboarddefinitions up-services\nkubectl delete customalertmanager telegram\nhelm uninstall victoria-metrics -n victoria-metrics-test\nkubectl delete ns victoria-metrics-test\nЗаключение\nМы рассмотрели, как подключить новое хранилище метрик к кластеру под управлением Deckhouse, создать алерт и настроить уведомления в Telegram. Здесь стоит обратить внимание, что все действия и настройки выполняются через custom resources. Такой декларативный подход соответствует общему тренду, при котором описание конфигурации инфраструктуры хранится в репозитории и последний используется как единственный источник настроек (\nGitOps\n).\nС предложениями и вопросами ждем вас в комментариях, а также в Telegram-чате \ndeckhouse_ru\n, где вам всегда помогут. Также будем рады Issues (и, конечно, звёздам) \nв GitHub-репозитории Deckhouse\n.\nP.S.\nЧитайте в нашем блоге:\n«Мониторинг межсервисного взаимодействия Kubernetes с помощью протокола NetFlow»\n;\n«Мониторинг с Prometheus в Kubernetes за 15 минут»\n;\n«Мониторинг и Kubernetes (обзор и видео доклада)»\n.\n \n ",
    "tags": [
        "deckhouse",
        "victoriametrics",
        "kubernetes",
        "telegram",
        "metrics",
        "alerting"
    ]
}