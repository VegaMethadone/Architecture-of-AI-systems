{
    "article_id": "726012",
    "article_name": "Bag of tricks для разметки текстовых данных: Часть 2. Удаление дубликатов",
    "content": "Привет! Меня зовут Ирина Кротова, я NLP-исследователь из компании MTS AI. В этой статье из цикла про разметку данных я расскажу об ещё одном способе собирать данные более качественно и экономить на разметке — фильтрации похожих друг на друга текстов.\nВ предыдущей \nстатье\n я рассказывала о том, что такое аннотация данных, как это связано с работой инженера машинного обучения и о способах сократить количество ручной разметки в проекте.\nТочные совпадения\nИдея удалить полные дубликаты перед разметкой кажется очевидной: предполагается, что мы уже имеем дело с отфильтрованным чистым датасетом. На практике почти на каждом этапе у нас есть шанс получить дублирующиеся тексты. Причины могут быть разнообразными, начиная от спам-сообщений пользователей и заканчивая багом в коде.\nЕсли ресурсов на разметку данных мало, то будет особенно неприятно случайно разметить одни и те же примеры.\nДаже если есть полная уверенность в том, что никаких дубликатов не должно было остаться, вызов \ndrop_duplicates\n в \npandas\n занимает меньше минуты, но может легко сэкономить часы работы аннотаторов.\nНеточные совпадения\nКроме полного совпадения, часто встречаются неполные дубликаты — тексты с \nпочти одним и тем же содержанием\n.\nНапример, почти одинаковые страницы в поисковой выдаче, разнообразный плагиат, зеркала одного и того же вебсайта и копирование контента новостными агрегаторами.\nНа уровне коротких текстов подобные проблемы возникают в user-generated контенте и том, что им притворяется: одна площадка для сбора отзывов копирует отзывы с другой, недовольный продуктом пользователь оставил похожие сообщения несколько раз на каждом сайте или, как в примере ниже, спамеры шлют сообщения в игровой чат.\nСпам про биткоины в чате\nТакие тексты тоже стоило бы удалять из выборки, но часто между ними нет полного совпадения: есть лишние слова или абзацы, пропуски, в конце потерян знак препинания.\nСразу отмечу, что дальше под дубликатами я буду иметь в виду именно сообщения, которые отличаются друг от друга на уровне неудачного копипастинга текстов, но не на уровне смысла (например, перефразирование). В каких-то случаях может быть полезно удалить ещё и тексты, близкие по смыслу — всё зависит от постановки задачи. Но для классических задач вроде классификации интентов или определения тональности отзывов разные способы выразить одну и ту же мысль обычно как раз интересны (\n\"Телефон просто супер!\", \"Отличный телефон\", \"Телефон очень понравился\"\n), а вот из отзывов \n\"Телефон прсто супер!!!\"\n и \n\"Телефон просто супер\"\n может быть полезно оставить только один.\nКак найти такие дубликаты?\nМера близости Жаккара\nЕсли датасет не очень большой, то можно построить попарную матрицу расстояний между всеми текстами. Один из самых простых способов посчитать такие расстояния —\n \nмера близости Жаккара\n.\nИдея в том, чтобы представить оба текста в виде множества n-грамм (посимвольно или пословно) и найти соотношение между пересечением n-грамм и всеми n-граммами обоих текстов.\nПопробуем посчитать такие расстояния для текстов из чата с картинки выше.\nmessage = \"Selling cheap coins. 1K=5.9$\"\n\n# текст с теми же словами, но в другом порядке: \npermuted_message = \"1K=5.9$ Selling cheap coins.\"\n# текст с частичным совпадением:\nsimilar_message = \"Selling cheap coins. good stock. Price 1000 coins =$5.9\"\n# полностью отличающийся текст:\ndifferent_message = \"food is out of combat\"\nРазбиваем текст самым простым способом — по пробелам, выводим на экран множество всех оригинальных токенов:\nmessage = set(message.split())\npermuted_message = set(permuted_message.split())\nsimilar_message = set(similar_message.split())\ndifferent_message = set(different_message.split())\n\nprint(message)\n# {'cheap', '1K=5.9$', 'Selling', 'coins.'}\n\nprint(permuted_message)\n# {'cheap', '1K=5.9$', 'Selling', 'coins.'}\n\nprint(similar_message)\n# {'=$5.9', 'Selling', 'good', 'Price', 'coins', \\\n#'coins.','1000', 'stoc.', 'cheap'}\n\nprint(different_message)\n# {'out', 'combat', 'of', 'food', 'is'}\nВычисляем индекс Жаккара между текстами:\ndef jaccard(x: set, y: set):\n    shared = x.intersection(y)  # выбираем пересекающиеся токены\n    return len(shared) / len(x.union(y))\n\n# Для идентичных сообщений:\nprint(jaccard(message, message))\n# 1.0\n\n# Для того же текста с другим порядком слов:\nprint(jaccard(message, permuted_message))\n# 1.0\n\n# Для частично дублирующихся сообщений:\nprint(jaccard(message, similar_message))\n# 0.3\n\n# Для полностью разных текстов:\nprint(jaccard(message, different_message))\n# 0.0\nСпособ поиска близких текстов максимально простой: нет почти никакого предпроцессинга текста (удаление пунктуации или стоп-слов), никак не учитывается схожесть элементов \n1K=5.9$\n и \n=$5.9\n и порядок слов в тексте. Тем не менее, даже такой достаточно примитивный подход может быть хорошим бейзлайном, и часто его будет достаточно.\nКак этот подход улучшить?\nShingling\nКачественно улучшить поиск дубликатов можно с помощью шинглов (\nw-shingling\n), также известных как n-граммы. Идея в том, чтобы представить текст как последовательности из n идущих подряд в тексте элементов, посимвольно или пословно.\nНапример, посимвольными би-граммами для строки \nУдаление дубликатов\n будут \n['Уд', 'да', 'ал', 'ле', 'ен', 'ни', 'ие', 'е ', ' д', 'ду', 'уб', 'бл', 'ли', 'ик', 'ка', 'ат', 'то', 'ов']\n.\nПопробуем такой подход для три-грамм:\ndef text_to_ngrams(text, N=3):\n    return [text[i:i+N] for i in range(len(text)-N+1)]\n\nprint(text_to_ngrams(message))\n# ['Sel', 'ell', 'lli', 'lin', 'ing', 'ng ', \\\n# 'g c', ' ch', 'che', 'hea', 'eap', 'ap ', 'p c', \\\n# ' co', 'coi', 'oin', 'ins', 'ns.', 's. ', '. 1', \\\n# ' 1K', '1K=', 'K=5', '=5.', '5.9', '.9$']\n\nmessage = set(text_to_ngrams(message))\npermuted_message = set(text_to_ngrams(permuted_message))\nsimilar_message = set(text_to_ngrams(similar_message))\ndifferent_message = set(text_to_ngrams(different_message))\n\n# Для идентичных сообщений:\nprint(jaccard(message, message))\n# 1.0\n\n# Для того же текста с другим порядком слов:\nprint(jaccard(message, permuted_message))\n# 0.7931034482758621\n\n# Для частично дублирующихся сообщений:\nprint(jaccard(message, similar_message))\n# 0.37037037037037035\n\n# Для полностью разных текстов:\nprint(jaccard(message, different_message))\n# 0.022727272727272728\nПо сравнению с предыдущим подходом метод с n-граммами явно более чувствителен к перестановке слов в тексте и опечаткам.\nКак понять, какие параметры для разбиения текста на n-граммы лучше выбрать для вашего датасета?\nМой опыт и \"общее знание\" подсказывают, что если вы имеете дело с небольшими сообщениями (реплики в чатах, твиты), то хорошо работают посимвольные n-граммы с N от трёх до пяти. С большими текстами (поиск дубликатов между статьями, веб-страницами) можно брать пословные n-граммы с N от семи до десяти.\nВ классической книге \nMining of Massive Datasets\n это число убедительно обосновывается таким способом:\nпредставим, что мы работаем с электронными письмами на английском языке, состоящими только из букв (26 в английском алфавите) и пробелов;\nв таком случае мы получим \n возможных шинглов (n-грамм);\nпоскольку типичное письмо меньше, чем 14 миллионов символов, мы предполагаем, что \nбудет работать достаточно хорошо. \nНо в любом случае нужно смотреть на данные и задачу.\nНапример, если в датасете содержатся длинные тексты, но с большим количеством повторяющегося контента (типовые правовые договоры или медицинские выписки, где важные детали добавляются в общий шаблон), то более низкое значение N может работать эффективнее.\nС другой стороны, если в датасете много текстов, отличающихся в первую очередь порядком слов, то более высокое значение N может подойти лучше, так как будет к нему более чувствительно.\nМеняем в предыдущем коде параметр \nN\n с \n3\n на \n10\n и смотрим, как изменился результат:\nprint(jaccard(message, message))\n# 1.0\n\nprint(jaccard(message, permuted_message))\n# 0.4074074074074074 (vs. ~0.79)\n\nprint(jaccard(message, similar_message))\n# 0.23076923076923078 (vs. ~0.37)\n\nprint(jaccard(message, different_message))\n# 0.0 (vs. ~0.02)\nПоэтому проще всего поэкспериментировать с небольшой частью данных и разными значениями N и выбрать наиболее подходящий вариант.\nКак ещё можно искать близкие тексты?\nДругие \nметрики близости между текстами\n (edit distance metrics), которые часто используются в компьютерной лингвистике и биоинформатике:\nРасстояние Левенштейна \n— метрика, для которой считается минимальное количество односимвольных операций (вставка, удаление, замена символа), которые нужны для того, чтобы превратить одну строку в другую.\nРасстояние Дамерау-Левештейна\n — к операциям, определенных в метрике расстояния Левенштейна, добавляется ещё операция транспозиции (перестановка соседних символов).\nОбе метрики опираются на алгоритм динамического программирования \nВагнера-Фишера\n.\nLCS (наибольшая общая подпоследовательность)\n — решается задача поиска всех наибольших подпоследовательностей. Используется, например, в утилите \ndiff\n.\nВнимание\n: подпоследовательность != подстроке. Например, \nвла\n является только подпоследовательностью слова \nвилка\n, а \nвил\n — и подпоследовательностью, и подстрокой. \nРасстояние Хэмминга\n — использует только операции перестановки. Этот алгоритм работает подходит дли последовательностей одной длины.\nРеализацию на Питоне можно найти, например, в пакете \npyeditdistance\n или \nсоответствующем модуле NLTK\n.\nПоиск текстов, близких по смыслу\nДля того, чтобы найти тексты, близкие друг другу не только на уровне похожести строк, но и \nпо смыслу\n, можно использовать векторные представления текста.\nИдея следующая:\nкодируем текст при помощи понравившейся модели (\nword2vec\n,\n \nfastText\n или\n \nSentence Transformers\n, например,\n \nLABSE\n);\nвычисляем близость между получившимися векторами при помощи, например,\n \nкосинусной меры сходства\n;\nранжируем тексты по схожести и, в зависимости от задачи, определяем подходящий порог, после которого данные будут отфильтровываться;\nудаляем слишком близкие друг к другу тексты.\nПо моему опыту, для многих задач именно на этапе фильтра датасета такой подход уже становится overkill'ом. Во-первых, часто тексты с близким по смыслу значением, выраженным по-разному, как раз интересны для разработки модели (нужно много перефразированных вариантов одного и того же интента для чат-бота, например). Во-вторых, качество ранжирования упирается в качество самих векторных представлений. Но в любом случае полезно знать, что можно попробовать очистить данные ещё и так.\nРабота с большими данными: LSH, MinHash\nЕсли датасет достаточно большой, то может быть сложно построить попарную матрицу расстояний между текстами за разумное время.\nНапример, есть датасет из миллиона текстов реплик в чатах, из которых нужно удалить похожие.\nДля того, чтобы попарно сравнить все \nтекстов, необходимо сделать\nсравнений. Соответственно, для \nнужно будет сравнить тексты \nраз.\nДопустим, мы сравниванем тексты попарно со скоростью миллион в секунду. В одних сутках 86400 секунд. Это значит, что для обработки датасета из миллиона записей понадобится \nпочти 6 дней.\nДаже если 6 дней вычислений нас в целом устраивают, такой способ поиска дубликатов плохо масштабируется: как только алгоритм поиска близости усложнится или данных станет существенно больше, время на обработку может в разы увеличится.\nИзбежать долгих вычислений можно, если вместо попарного сравнения решать задачу \nприближенного \nпоиска ближайших соседей (\nANN\n).\nИдея такая: чтобы сократить количество вычислений, нужно сократить число возможных сравнений. Это можно сделать, если сравнивать вектор текста не с векторами всех остальных текстов из датасета, а ограничить количество кандидатов только теми, которые мы предположительно считаем возможными дубликатами.\nНайти таких наиболее вероятных кандидатов можно при помощи\n \nLocality-Sensitive Hashing (LSH)\n, одного из наиболее популярных алгоритмом для задачи ANN. Это вероятностный метод снижения размерности, при котором подбираются такие \nхэш-функции\n (функция, которая преобразует объекты в битовые последовательности одинаковой длины), чтобы похожие тексты с высокой вероятностью попадали в одну \"корзину\" (\nbuckets\n).\nПоскольку длина битовой последовательности, в которую хэш-функция преобразует объекты, ограничена, могут возникать \nколлизии.\n Коллизия означает, что разные объекты преобразуются в одну и ту же хэш-сумму.\nВ обычной ситуации цель при использовании хэш-функций — \nминимизировать количество коллизий.\nНапример, мы хэшируем пароли пользователей и храним в базе данных только хэш-суммы. Хакер получил доступ к базе данных, но не находит там оригинальных паролей пользователей, поэтому на первый взгляд кажется, что все в порядке. Но если при хэшировании паролей использовалась функция, коллизии для которой можно найти гораздо быстрее, чем простым перебором, то существует высокая вероятность, что хакер сможет подобрать \nдругой\n пароль \nс той же\n хэш-суммой, что и пароль от личного кабинета пользователя, и воспользоваться им.\nПоэтому для хэширования ключей в словаре Python, паролей или электронных подписей большое количество коллизий — недостаток. Но не для поиска дубликатов.\nХэш-функции для LSH, наоборот, \nмаксимизируют количество коллизий\n. В отличие от ситуации с паролями, если похожие друг на друга тексты получится положить в одну и ту же ячейку, то мы только выиграем.\nЭти функции чувствительны к местоположению (\nlocality sensitive\n), из-за чего в одну и ту же ячейку помещаются не случайные объекты, а близкие друг к другу точки. Это позволяет сравнивать текст не со всеми остальными данными, а только с подмножеством наиболее близких текстов, попавших с ним в одну корзину.\nВерхнеуровневый обзор LSH из статьи James Briggs\nМетодов LSH много, но основная идея для всех: при помощи хэш-функций сложить похожие объекты в одни и те же ячейки.\nВот из каких этапов состоит \nклассический\n подход:\nShingling\n: уже описанное выше разделение текста на n-граммы;\nOne-hot-encoding\n: преобразуем полученные n-граммы, получаем векторы для текстов;\nMinHash\n —  алгоритм, который позволяет находить похожие множества эффективнее, чем мера близости Жаккара, с помощью хэш-функций.\nВ данном случае мы используем его для того, чтобы преобразовать векторы в сигнатуры меньшей размерности, которые при этом с некоторой вероятностью всё ещё позволяют оценить их сходство.\nLSH\n: сигнатуры, полученные на предыдущем этапе, разбиваем на b частей, а потом раскидываем на k корзин (последний этап на схеме выше). Сигнатуры, части которых хотя бы раз попали в одну и ту же корзину, считаем кандидатами в дубликаты.\nИщем дубликаты\n, сравнивая тексты друг с другом только внутри одной корзины.\nПодробнее почитать про LSH в простом изложении и с кодом можно в\n \nпосте James Briggs\n (иллюстрация выше как раз из него) и cтатье\n \nMatti Lyra\n.\nЕсли хочется чего-то более фундаментального, то тема поиска дубликатов, на мой взгляд, лучше всего раскрывается\n \nв 3 главе\n классической книги \nMining of Massive Datasets\n (\nкнига\n,\n \nвидео\n).\nА реализации, помимо больших фреймворков для бигдаты, есть, например, здесь:\nимплементация SimHash\n от scrapinghub\nфреймворк\n \nNearPy\nпакет\n \ndatasketch\nНа этом всё. Надеюсь, что эти советы по удалению дубликатов будут вам полезны. Делитесь своими лайфхаками в комментариях: какие подходы обычно используете и на каких данных, что хорошо работает, а что практически не меняет картину? И не забывайте лайкать пост 🙂\n \n ",
    "tags": [
        "annotations",
        "machine learning",
        "machinelearning",
        "datasets",
        "hash function",
        "nearest-neighbor",
        "nlp",
        "natural language processing",
        "data annotation",
        "data cleaning"
    ]
}