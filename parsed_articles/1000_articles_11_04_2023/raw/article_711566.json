{
    "article_id": "711566",
    "article_name": "Измеряй и властвуй: как мы покрытие автоматическими тестами измеряли",
    "content": "Привет, Хабр! Меня зовут Марина Петрова, я QA Lead в Cloud. В нашей QA-команде уже более 35 человек, а количество тестируемых продуктов превышает десяток. Мы пишем автоматические тесты для повышения качества продуктов и сокращения времени ручного тестирования. Для эффективной организации этого процесса требуются числовые индикаторы — метрики. Нам хотелось иметь инструмент, в котором аккумулируются данные о метриках в простом и понятном всем виде. Тогда мы предложили инициативу по созданию приложения для автоматического сбора и визуализации метрик покрытия автоматическими тестами.\nВ статье расскажу, какой путь мы прошли, чтобы измерить покрытие функциональности проектов автотестами. \nКакие цели и задачи перед собой поставили\nНа начальном этапе у нас возникло множество вопросов. Какие метрики выбрать? С какой периодичностью их собирать? Как измерить величину покрытия тестами разного уровня разрабатываемых продуктов? Какими метриками мы можем пользоваться, чтобы оценить уровень доверия к качеству продукта? Где и как собирать метрики? В каком виде предоставлять эту информацию на регулярной основе членам команд и руководству?\nПри этом мы точно знали, что хотим:\nсократить время работы QA-инженеров при планировании задач\n. Ранее каждый сотрудник тратил от одного до трех часов\n \nна оценку текущего покрытия системы, выявление «высокорисковых» сценариев, недостаточно покрытых автоматическими тестами, и саму постановку задач на автоматизацию тестирования;\nунифицировать подход\n к планированию автоматизации и измерению покрытия во всех QA-командах компании, чтобы видеть общую картину и знать, сколько в продукте автотестов и что находится на проверке.\nОтталкиваясь от этих соображений, в первом квартале 2022 года мы поставили перед собой следующие \nцели\n:\nсформулировать и задокументировать методику подсчета покрытия для автоматических тестов всех уровней;\nорганизовать сбор и хранение метрик покрытия продукта автоматическими тестами;\nвыявить зоны рисков — недостаточное покрытие автоматическими тестами сценариев приложения в целом и отдельных сервисов;\nсократить время, затрачиваемое на планирование автоматизации тестирования;\nвизуализировать метрики и их динамику в простом и понятном всем виде для всех продуктов, имеющих автотесты.\nДля достижения целей нужно было решить следующие \nзадачи\n:\nдоговориться о классификации автоматических тестов со всеми командами;\nвыбрать метрики покрытия для каждого типа тестов и обобщенные для проекта;\nпроанализировать достаточность имеющихся инструментов в компании для сбора и визуализации метрик, провести дополнительный анализ решений в OpenSource при необходимости;\nописать методологию интеграции с выбранным инструментом;\nиспользовать автоматически получаемые метрики в ежедневной работе и при планировании.\nКакие инструменты используем для автоматизации и тестирования\nСтек автоматизации в нашей QA-команде — это Pytest + Selenium/Playwright. Для тест-кейсов и отчетов по автотестам используем Allure TestOps, pipelines запускаем в GitLab. \nНам не хватало имеющихся инструментов для сбора и измерения покрытия. Конечно, для анализа результатов прогонов автотестов мы пользуемся дашбордами в Allure TestOps, но проблема в том, что в этом инструменте нет возможности создать кросс-командный дашборд и добавлять собственные метрики. \nКакие метрики покрытия выбрали\nМы выбрали классификацию тестов на основе «классической» пирамиды тестирования: UNIT, API и E2E.\nСледующий этап — выбор подходящих метрик покрытия для каждого сервиса в отдельности и каждого проекта в целом.\nUNIT\nДля unit-тестов стандартной метрикой покрытия является покрытие кода. Эту метрику мы и решили выгружать в наше приложение. В качестве обобщенной метрики договорились использовать медиану покрытия кода.\nAPI\nДля измерения покрытия API мы используем пакет \nswagger-coverage\n. Обязательным условием для успеха при его использовании является наличие спецификации в сервисах OpenAPI. \nПосле ночного запуска всех API-тестов мы сохраняем отчеты по покрытию в артефакты pipelines GitLab CI. Ниже пример отчета одного из наших сервисов:\nОтчет о покрытии API одного из сервисов проекта Cloud\nНа основе данных отчетов мы решили собирать метрики по покрытию:\nполное покрытие;\nчастичное покрытие;\nнепокрытые методы;\nаналогичные суммарные метрики по всем API интерфейcам продукта.\nДля метрик 1-3 оставалось решить вопрос с хранением данных, для 4 — научиться рассчитывать.\nE2E\nДля покрытия пользовательских сценариев выбрали функциональную модель. То есть QA-инженеры в каждом проекте составляют список критичных сценариев использования, создают такие тест-кейсы в Allure TestOps с тегом critical. Список критичных сценариев договорились обновлять при необходимости, но не реже чем один раз в квартал. \nЕсли для критичного сценария уже был написан автоматический тест, то в коде достаточно было добавить для него соответствующий тег. Для нашего стека автоматизации это декоратор @pytest.mark.critical.\nДалее в каждом проекте в Allure TestOps создали \nE2E critical\n \nDashboards\n для визуализации покрытия с \nPie Chart\n-виджетом со следующими параметрами:\nType = Test Case Pie Chart\nGroup by = By automation\nTest cases query = tag is \"critical\" \nТак мы достаточно просто и в автоматическом режиме получили метрику покрытия критичных E2E-сценариев автоматическими тестами. За счет разметки автотестов по функциональным блокам бонусом получили карту E2E-покрытия.\nДашборд покрытия критичных сценариев использования\nИзмерять покрытие \nE2E\n будем по формуле = \ne2e automation test \\ (e2e automation test + e2e manual test\n). Но проблема с кросс-продуктовым дашбордом на данном этапе осталась нерешенной.\nКак разрабатывали инструмент\nМы обсудили с нашим руководителем отдела выбранные метрики и способы их измерения, показали MVP проекта, обсудили детали технической реализации. MVP был реализован на стеке \nPython, Django, Postgresql, Bootstrap\n. \nВ результате обсуждения получили «добро» на разработку и внедрение метрик покрытия на всю компанию. Создали страницу с планом работ по развитию дашборда на внутреннем портале компании, выбрали периодичность встреч для обсуждения результатов и принялись за работу. \nТак получилось, что основные задачи по развитию приложения мы делали вдвоем с коллегой \nМаксимом Алексеевым\n, старшим инженером тестирования\n. Нам обоим понравилось то, как удалось организовать процесс разработки. Оказалось, что в условиях ограниченности ресурсов, получается работать эффективнее и продуктивнее. У нас не было достаточно времени для работы над нашей идеей в рабочие часы, мы занимались этим в свое свободное время. На протяжении трех месяцев один раз в две недели мы встречались, чтобы зафиксировать результаты и запланировать следующие шаги. Затем реализовывали запланированное без обсуждения деталей исполнения: делали так, как считали правильным.\nПараллельно с разработкой описывали, как работает дашборд, какие шаги нужно выполнить для интеграции с ним, делились результатами с коллегами, получали обратную связь и вносили корректировки.\nГлавная страница приложения\nКак собирали данные для расчета метрик\nДля сбора метрик покрытия мы реализовали в приложении несколько API-методов. Подробнее о них будет ниже.\nE2E\nИнформацию о количестве ручных и автоматизированных сценариев использования, как упоминалось ранее, мы получаем из Allure TestOps через \nallure-testops-api\n. \nТаким образом, для сбора статистики E2E-покрытия по проекту QA-инженеру необходимо:\nсоздать дашборд для критичных сценариев в Allure TestOps;\nдобавить widget в дашборд Allure TestOps для получения корректной информации о покрытии;\nсохранить widget_id в дашборде. Для этого нужно отправить post-запрос на создание записи о widget-проекта;\nEndpoint \n{COVERAGE_DASHBOARD_URL}/api/widget\n \nBody:\n{      \t\n  \"project\": \"<Project name>\",\n  \"widget_allure_id\": <widget id>\n}\nсохранить ссылки на список критичных функциональный возможностей и на widget из Allure для ознакомления всем заинтересованным лицам;\nEndpoint \nPOST {COVERAGE_DASHBOARD_URL}/test_cases\nBody:\n{\n   \t\"project\": \"<project name>\",\n   \t\"wiki_ref\": \"wiki ref ex. wiki/pages/…\",\n    \"allure_test_ops_ref\": \"allure project ref ex. {ALLURE_URL}/project/{pr_id}/dashboards/{widget_id}\"\n   }\n\nпроверить корректность выполненных действий — на странице \n{COVERAGE_DASHBOARD_URL}/e2e\n \nинформация по покрытию должна совпадать с данными из widget.\nДля случая, когда виджет в Allure TestOps еще не создан, решили отображать общее число ручных и автоматических сценариев с пометкой, что данные о покрытии E2E сценариев отсутствуют. \nПокрытие критичных E2E\nТакже мы добавили визуальное отображение динамики покрытия.\nAPI\nКак упоминала выше, измерение покрытия Rest API выполняем с использованием swagger-coverage с дополнительным подсчетом суммарных метрик покрытия всех Rest API-сервисов проекта и по каждому сервису отдельно. Вначале мы реализовали хранение только суммарных метрик по Rest API, но быстро поняли, что бóльшую ценность имеет покрытие по каждому сервису отдельно.\nИнженеру тестирования нужно было настроить в своем проекте по автотестам отправку данных о покрытии со следующим синтаксисом\nEndpoint \n \nPOST {COVERAGE_DASHBOARD_URL}/api/coverage\nBody:\n{\n    \"project\": \"Project name 1\",\n    \"all_cnt\": 932,\n    \"actual_cnt\": 912,\n    \"full_cnt\": 138,\n    \"partial_cnt\": 406,\n    \"empty_cnt\": 368,\n    \"deprecated_cnt\": 20,\n    \"partial_coverage\": 41.517,\n    \"full_coverage\": 17.132,\n    \"empty_coverage\": 41.351,\n    \"services\": [\n        {\n            \"AICloud Service 1\": {\n                \"all\": 31,\n                \"full\": 0,\n                \"partial\": 0,\n                \"empty\": 31\n            }\n        },\n        {\n            \"AICloud Service  2\": {\n                \"all\": 14,\n                \"full\": 2,\n                \"partial\": 2,\n                \"empty\": 10\n            }\n        },\n       ….\n        }\n    ]\n}\nДля того чтобы каждая команда не пошла своим путем, мы написали: \nкласс-обертку для использования swagger-coverage с pytest-фреймворком;\nгенератор общего отчета о покрытии API-проекта, на основе отчетов о покрытии каждого сервиса в отдельности;\nскрипт для отправки данных в наше приложение;\nпример CI для выгрузки данных из pipeline в coverage dashboard:\ncoverage_dashboard:\n  stage: coverage_dashboard\n  image:\n    name: $TESTS_IMAGE:$IMAGE_TAG\n    entrypoint: [\"\"]\n  only:\n    variables:\n      - $SWAGGER_COVERAGE == \"all\"\n  script:\n    - python3 coverage_dashboard_uploader.py\nПосле проделанной работы мы начали получать первые результаты по метрикам покрытия API.\nПокрытие API\nUNIT\nДля unit-тестов также был реализован отдельный интерфейс — метод для отправки покрытия \n{COVERAGE_DASHBOARD_URL}/unit/coverage\n{\n    \"project\": \"<Project name>\",\n    \"code_coverage\": 50,\n    \"service\": \"Service 2\"\n}\nКак внедряли самописный инструмент, с какими трудностями столкнулись\nКогда основная функциональность нашего инструмента была готова, мы рассказали о нем на внутреннем демо компании и начали активно помогать командам собирать метрики. В ходе этого процесса обнаружили несколько промахов.\nМы не учли особенности окружений запуска GitLab runners проектов. У некоторых из них возникла проблема сетевой связности с виртуальной машиной, где было развернуто приложение. Потребовалась помощь сетевых инженеров.\nНеобходимость измерения метрик покрытия была актуальна не для всех проектов из-за разницы в степени их зрелости, процессов тестирования и разработки. Но мы приходили во все команды с вопросом, почему их метрик нет в дашборде? Возможно, этим мы вгоняли QA-инженеров команд в стресс. Они объективно не могли обеспечить сбор таких метрик.  \nНе был учтен фактор времени и нагрузки на QA-инженеров. Внутри команд QA-инженеров все были проинформированы о необходимости выгружать метрики, но в их продуктовой команде были свои сроки и планы. Как итог за второй квартал 2022 года из 13-ти команд метрики E2E и API выгружали четыре команды, для unit-тестов — только три.\nМы не предложили вариантов измерения покрытия для сервисов с GRPC. \nНе учли риски изменения API Allure TestOps — однажды после обновления Allure TestOps наш дашборд сломался. \nКаких результатов достигли\nБлагодаря созданию приложения для автоматического сбора и визуализации метрик покрытия автоматическими тестами QA-инженеры в \nCloud\n: \nСократили время планирования автоматизации тестирования примерно на 50%. Теперь мы планируем автоматизацию за 30 минут.\nЗнают процент покрытия каждого сервиса и его динамику. Известно также, падает покрытие или растет, и в какой период началось изменение покрытия.\nИспользуют инструмент для кросс-командного взаимодействия. В QA-команде заметно улучшилась коммуникация и вовлеченность в процессы по улучшению качества. Теперь мы знаем, к кому нужно обратиться за помощью по автоматизации (конечно же к команде, у которой растет покрытие!).\nСняли с себя часть нагрузки по отчетности о результатах автоматизации и степени покрытия — все заинтересованные лица в любой момент времени могут открыть дашборд.\nСнизили риски избыточного и недостаточного ручного тестирования.\nДля руководителей появились свои очевидные плюсы:\nПовысилась прозрачность и понятность состояния автоматизации тестирования на проекте.\nПроцесс автоматизации тестирования стал управляемым — при планировании спринта, квартальных и годовых целей команды QA-инженеров оперируют конкретными цифрами, которые каждый член команды может проверить лично без участия QA. \nПланы\nУ нас была задача, мы предложили решение и реализовали его. Получили опыт взаимодействия с членами разных команд тестирования и, конечно же, сделали работающий инструмент — наш дашборд.\nСейчас мы выбираем дополнительные метрики по процессам тестирования и автоматизации в частности. Подключили Grafana для расширения возможностей подсчета метрик.\nНам интересно, какой путь проходят другие компании, какими инструментами и метриками пользуются. Делитесь вашим опытом в комментариях.\n \n \n ",
    "tags": [
        "qa automation",
        "qa",
        "тестирование веб-приложений"
    ]
}