{
    "article_id": "725640",
    "article_name": "Гайд для новичков по установке Kubernetes",
    "content": "\r\n\n© кадр из к/ф «Пираты Карибского моря»\n\r\n\n\r\nС чего начинается практическое освоение любой системы? Правильно, с установки. Данный гайд является компиляцией из народной мудрости, официальной документации, а также собственного опыта и призван помочь новичкам разобраться с тем, как же все таки устанавливать \nKubernetes\n.\n\r\n\n\r\nМы потренируемся ставить как вырожденный кластер «все-в-одном», состоящий только из одного узла, так и настоящий высокодоступный (high available) кластер с полным резервированием. В процессе работы мы рассмотрим применение различных \nконтейнерных движков\n (Container Runtimes): \ncri-o\n, \ncontainerd\n, связки \nDocker\n + \ncri-dockerd plugin\n. Кроме этого, потренируемся настраивать отказоустойчивый балансировщик нагрузки на базе \nkeepalived\n и \nhaproxy\n.\n\r\n\n\r\nВесь процесс установки будет детальным образом прокомментирован и разложен по шагам, а в реперных точках мы будем делать снимки состояния виртуальных машин (snapshots), что позволит рассмотреть различные варианты установки без необходимости делать одну и ту же работу по несколько раз.\n\r\n\n\r\n\n\n                        \nОглавление\n\n                        \n1. Зачем все это надо?\n\r\n\n2. Как устроен Kubernetes\n\r\n\n3. Способы установки Kubernetes\n\r\n\n4. Схема виртуального стенда\n\r\n\n5. Постановка задач\n\r\n\n6. Решение\n\r\n\n6.1. Предварительная настройка узлов кластера\n\r\n\n6.1.1. Настройка статических IP адресов узлов кластера\n\r\n\n6.1.2. Настройка имен узлов кластера\n\r\n\n6.1.3. Настройка DNS\n\r\n\n6.1.4. Настройка файла hosts\n\r\n\n6.1.5. Проверка сетевых настроек\n\r\n\n6.1.6. Установка вспомогательных пакетов\n\r\n\n6.1.7. Предварительная подготовка Linux для использования Kubernetes\n\r\n\n6.1.8. [ОПЦИОНАЛЬНО] Разрешение авторизации в SSH от пользователя root\n\r\n\nЛайфхак. Использование TMUX для одновременного конфигурирования нескольких узлов\n\r\n\n6.1.9. Установка kubeadm и kubectl\n\r\n\n6.2. Установка контейнерного движка\n\r\n\n6.2.А. Вариант A. Установка cri-o\n\r\n\n6.2.B. Вариант B. Установка containerd\n\r\n\n6.2.C. Вариант C. Установка Docker + cri-dockerd\n\r\n\n6.3. Развёртывание Kubernetes\n\r\n\n6.3.A. Вариант A. Установка вырожденного Kubernete кластера\n\r\n\n6.3.A.1. Инициализация кластера Kubernetes\n\r\n\n6.3.A.2. Конфигурирование утилиты управления kubectl\n\r\n\n6.3.A.3. Установка сетевого плагина\n\r\n\n6.3.A.4. Настройка управляющего узла для выполнения рабочих нагрузок\n\r\n\n6.3.B. Вариант B. Организация отказоустойчивого кластера Kubernetes\n\r\n\n6.3.B.1. Объяснение, за счет чего достигается отказоустойчивость\n\r\n\n6.3.B.2. Настройка балансировщика нагрузки\n\r\n\n6.3.B.2.1. Настройка демона keepalived\n\r\n\n6.3.B.2.2. Настройка демона haproxy\n\r\n\n6.3.B.3. Установка управляющих узлов кластера\n\r\n\n6.3.B.3.1. Установка первого управляющего узла\n\r\n\n6.3.B.3.2. Установка последующих управляющих узлов\n\r\n\n6.3.B.4. Установка рабочих узлов кластера\n\r\n\n6.3.B.5. Настройка kubeсtl\n\r\n\n6.3.B.6. Установка сетевого плагина\n\r\n\n7. Проверка работы кластера Kubernetes\n\r\n\n8. Тестовые запуски \"\nподов\n\" в Kubernetes\n\r\n\n8.1. Тест 1. Запуск \"\nпода\n\" в интерактивном режиме\n\r\n\n8.2. Тест 2. Запуск NGINX\n\r\n\n9. Диагностика балансировщика нагрузки\n\r\n\nX. Заключение\n\r\n\n\n                    \n\r\n\n\r\n\n1. Зачем все это надо?\n\r\nОдним из золотых правил построения надежной и безопасной информационной инфраструктуры является максимальная изоляция ее компонентов друг от друга. Хорошим признаком достижения этой цели можно считать кейс, когда один сервер выполняет только одну основную функцию. Например, контроллер Active Directory – это только контроллер Active Directory, а не файловый или Интернет-прокси сервер в придачу.\n\r\n\n\r\nИзначально инфраструктура разделялась с помощью выделенных аппаратных серверов, но это было очень дорогое удовольствие. Потом появилась технология виртуализации. Затраты на изоляцию существенно снизились, но все же оставались довольно высоки: виртуальные машины потребляли значительно количество вычислительных ресурсов, медленно запускались, им требовались отдельные лицензии на системное и прикладное ПО. \n\r\n\n\r\nСледующим этапом развития стала контейнеризация. Если виртуальная машина — это почти отдельный компьютер со своим BIOS, операционной системой, драйверами и так далее, то контейнер — это изолированная часть операционной системы узла с минимумом прикладного ПО, обеспечивающего его запуск. В результате этого контейнеры занимают мало места, быстро стартуют и существенно минимизируют другие сопутствующие расходы. \n\r\n\n\r\nКонтейнеры идеологически очень похожи на портативный (portable) софт. Они содержат в себе лишь те файлы, что нужны для запуска конкретной программы. Однако, в отличии от обычных портативных программ, технологии контейнеризации отделяют контейнеры друг от друга и от хозяйской (host) операционной системы, позволяя каждому контейнеру считать себя отдельным компьютером, что очень похоже на работу виртуальных машин. \n\r\n\n\r\nС точки зрения безопасности изоляция контейнеров хуже, нежели изоляция виртуальных машин. Тем не менее, достигаемого уровня безопасности достаточно для большинства сценариев применения.\n\r\n\n\r\nКонтейнеризация получила широкую известность вместе с Docker. Эта система сделала работу с контейнерами чрезвычайно простой и доступной. Она хорошо подходит для управления контейнерами в небольших проектах, но для серьезных задач, когда нужно оперировать большим числом контейнеров, организовывать отказоустойчивые конфигурации, гибко управлять вычислительными ресурсами, ее возможностей недостаточно, и здесь на сцену выходит герой нашей статьи – система управления/оркестрации (orchestration) контейнерами Kubernetes.\n\r\n\n\r\n\n2. Как устроен Kubernetes\n\r\nПо факту Kubernetes — очень гибкое решение, которое может быть настроено бесчисленным количеством способов. Это, конечно, очень здорово для работы, но является сущим адом при изучении. Поэтому здесь мы не будем рассматривать все возможные варианты построения системы, а ограничимся лишь базовым, достаточным для её первичного освоения.\n\r\n\n\r\nKubernetes кластер состоит из двух типов узлов: управляющих и рабочих.\n\r\n\n\r\nУправляющие узлы, как видно из названия, предназначены для управления кластером. Они отдают команды рабочим узлам на запуск и остановку рабочих нагрузок (workloads), отслеживают состояние кластера, перераспределяют задачи в случае отказов и совершают множество других управленческих действий. Рабочие узлы — это пчелки, выполняющие всю полезную работу, ради которой функционирует кластер.\n\r\n\n\r\nНа самом деле управляющие узлы тоже могут выполнять рабочие нагрузки, правда с точки зрения безопасности это считается нежелательным. Поскольку, если в одной из рабочих нагрузок будет вредоносный код, то, будучи запущенным на управляющем узле, он сможет натворить гораздо больше бед, нежели будучи запущенным на рабочем узле.\n\r\n\n\r\nТиповой состав ПО рабочего узла включает в себя (\nРисунок 1\n):\n\r\n\n\r\n\nСлужебные компоненты Kubernetes: агент управления узлом \nkubelet\n, узловой прокси \nkube-proxy\n\r\n\nСетевой плагин (Container Network Interface, CNI plugin)\n.\n\r\n\nКонтейнерный движок: cri-o, containerd или Docker + cri-dockerd plugin.\n\r\n\nРабочие нагрузки (workloads), то есть сами контейнеры, из-за которых все и затевалось. Однако, здесь важно уточнить один существенный момент — минимальной единицей управления рабочей нагрузкой в Kuberbetes является \n\"\nпод\n\" (pod)\n, состоящий из одного (как правило) или нескольких контейнеров.\n\r\n\n\r\n\n\r\n\nРисунок 1\n\r\n\n\r\nСостав ПО управляющих узлов (\nРисунок 2\n) дополнительно включает в себя:\n\r\n\n\r\n\nУправляющие компоненты Kubernetes: планировщик \nkube-scheduler\n, базовый демон управления \nkube-controller-manager\n, REST API сервер управления \nkube-apiserver\n.\n\r\n\nОтказоустойчивое хранилище \netcd\n.\n\r\n\nБалансировщик нагрузки (для случаев использования нескольких управляющих узлов в кластере).\n\r\n\n\r\n\n\r\n\nРисунок 2\n\r\n\n\r\nВажно отметить, что практически все компоненты, кроме kubelet и kube-proxy, могут функционировать в Kubernetes в качестве рабочих нагрузок. Другими словами, Kubernetes может управлять сам собой.\n\r\n\n\r\nЧитая о Kubernetes, часто можно услышать фразу: «Kubernetes – это просто: всего 5 бинарников». Под пятью бинарниками обычно понимают: kubelet, kubeproxy, kube-scheduler, kube-controller-manager, kube-apiserver. При этом почему-то всегда умалчивается о других обязательных компонентах кластера, хотя бы о том же etcd, так что Kubernetes — это далеко не просто и далеко не пять бинарников.\n\r\n\n\r\n\n3. Способы установки Kubernetes\n\r\nKubernetes в учебных целях может быть реализован с помощью различных утилит и готовых дистрибутивов:\n\r\n\n\r\n\nkind (Kubernetes in Docker)\n. Кластер, функционирующий на локальном компьютере «внутри» Docker.\n\r\n\nminikube\n. Кластер в одной утилите для запуска на локальном компьютере.\n\r\n\nDocker desktop\n – дистрибутив Docker для запуска на локальном компьютере с возможностью включить Kubernetes одной галкой – наверное, самый дружественный вариант для людей, которые не представляют, что такое Kubernetes, и хотят просто на него глянуть.\n\r\n\n\r\nДля промышленного использования Kubernetes должен быть развернут «по-честному». Для этого существуют следующие варианты:\n\r\n\n\r\n\nРазвертывание с помощью \nkubespay\n – набора скриптов (playbook’s) для системы управления инфраструктурой \nAnsible\n.\n\r\n\nПолностью ручное развёртывание «hard way». Гайд на английском можно почитать \nтут\n, на русском \nтут\n.\n\r\n\nРазвертывание с помощью утилиты \nkubeadm\n. Это то, чем мы будем заниматься далее.\n\r\n\n\r\n\n\r\n\n4. Схема виртуального стенда\n\r\nСогласно официальной \nдокументации\n, к машинам, на которых разворачивается Kubernetes, выдвигаются следующие требования:\n\r\n\n\r\n\n2+ GB ОЗУ;\n\r\n\n2+ процессорных ядра;\n\r\n\nLinux хост с отключенным файлом подкачки (swap);\n\r\n\n\r\nПри разворачивании кластера на нескольких узлах, согласно тем же документам, накладываются дополнительные требования:\n\r\n\n\r\n\nполная сетевая связанность узлов;\n\r\n\nна каждом узле должны быть уникальные: \n\r\n\n\r\n\nимена узлов (проверка с помощью команды \"\nhostname\n\"), \n\r\n\nMAC-адреса (проверка с помощью команды \"\nip link\n\"),\n\r\n\nпараметр \nproduct_uuid\n, являющийся уникальным идентификатором виртуальной машины (проверка с помощью команды \"\ncat /sys/class/dmi/id/product_uui\n\").\n\r\n\n\r\n\n\r\nВ ходе экспериментов выяснились, что дополнительно к этому узлы должны иметь статические IP-адреса и зарегистрированные DNS имена, что требуется для автоматического выпуска сертификатов во время работы \nkubeadm\n.\n\r\n\n\r\nМинимальное количество рабочих узлов для схемы с резервированием – 2. Логичное требование: один сломался другой на замену. Минимальное количество управляющих узлов для схемы с резервирование – 3. Данное странное требование продиктовано официальной \nдокументацией\n: в Kubernetes должно быть нечетное количество управляющих улов. Минимальное число нечетное число для обеспечения избыточности — 3.\n\r\n\n\r\nТаким образом, наш виртуальный стенд (\nРисунок 3\n) будет состоять из пяти виртуальных машин, находящихся в одноранговой сети, имеющей выход в Интернет, с помощью виртуального маршрутизатора, реализующего NAT. \n\r\n\n\r\n\n \n\r\n\nРисунок 3\n\r\n\n\r\nВиртуальные машины будут работать под управлением ОС \nDebian\n 11 x64, установленной с минимальным количеством пакетов. Все необходимое будем явно доставлять. \n\r\n\n\r\n\n5. Постановка задач\n\r\n\nЗадача 1.\n Организовать на базе узла \nnode1\n вырожденный Kubernetes кластер («все-в-одном»).\n\r\n\nЗадача 2.\n Организовать высокодоступный кластер Kubernetes, в котором узлы \nnode1\n, \nnode2\n, \nnode3\n будут управляющими (control-panel), а \nnode3\n и \nnode4\n – рабочими (workers).\n\r\n\n\r\n\n6. Решение\n\r\nДоговоримся, что все действия в данном гайде будем выполнять от пользователя \nroot\n. Сначала обе задачи будем решать параллельно, а затем в нужных местах сделаем развилки. Для минимизации переделки, в случае выявления ошибок, будем периодически проверять настройки и делать снимки состояния виртуальных машин.\n\r\n\n\r\nВ начальной точке у нас должно быть 5 свежеустановленных виртуальных машин, работающих под ОС Debian 11 x64. Машины должны быть в виртуальной сети с настройками 172.30.0.0/24. Шлюз по умолчанию – 172.30.0.2, он же DHCP, DNS и NAT сервер. Все машины должны иметь доступ в Интернет. Если все так, то делаем снимок состояния виртуальных машин и назовем его «START».\n\r\n\n\r\n\n6.1. Предварительная настройка узлов кластера\n\r\n\n\r\n\n6.1.1. Настройка статических IP адресов узлов кластера\n\r\n\nНа узле node1\n cодержимое файла \n/etc/network/interfaces\n заменим следующим:\n\r\n\n\n                        \nСкрытый текст\n\n                        \n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto ens33\niface ens33 inet static\naddress 172.30.0.201\nnetmask 255.255.255.0\ngateway 172.30.0.2\n\r\n\n\n                    \n\r\n\nНа узле node2\n cодержимое файла \n/etc/network/interfaces\n заменим следующим:\n\r\n\n\n                        \nСкрытый текст\n\n                        \n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto ens33\niface ens33 inet static\naddress 172.30.0.202\nnetmask 255.255.255.0\ngateway 172.30.0.2\n\r\n\n\n                    \n\r\n\n\r\n\nНа узле node3\n cодержимое файла \n/etc/network/interfaces\n заменим следующим:\n\r\n\n\n                        \nСкрытый текст\n\n                        \n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto ens33\niface ens33 inet static\naddress 172.30.0.203\nnetmask 255.255.255.0\ngateway 172.30.0.2\n\r\n\n\n                    \n\r\n\nНа узле node4\n cодержимое файла \n/etc/network/interfaces\n заменим следующим:\n\r\n\n\n                        \nСкрытый текст\n\n                        \n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto ens33\niface ens33 inet static\naddress 172.30.0.204\nnetmask 255.255.255.0\ngateway 172.30.0.2\n\r\n\n\n                    \n\r\n\n\r\n\nНа узле node5\n cодержимое файла \n/etc/network/interfaces\n заменим следующим:\n\r\n\n\n                        \nСкрытый текст\n\n                        \n# This file describes the network interfaces available on your system\n# and how to activate them. For more information, see interfaces(5).\n\nsource /etc/network/interfaces.d/*\n\n# The loopback network interface\nauto lo\niface lo inet loopback\n\n# The primary network interface\nauto ens33\niface ens33 inet static\naddress 172.30.0.205\nnetmask 255.255.255.0\ngateway 172.30.0.2\n\r\n\n\n                    \n\r\n\n\r\n\n6.1.2. Настройка имен узлов кластера\n\r\n\nНа узле node1\n выполним команду:\n\r\n\nhostnamectl set-hostname node1.internal\n\r\n\n\r\n\nНа узле node2\n выполним команду:\n\r\n\nhostnamectl set-hostname node2.internal\n\r\n\n\r\n\nНа узле node3\n выполним команду:\n\r\n\nhostnamectl set-hostname node3.internal\n\r\n\n\r\n\nНа узле node4\n выполним команду:\n\r\n\nhostnamectl set-hostname node4.internal\n\r\n\n\r\nН\nа узле node5\n выполним команду:\n\r\n\nhostnamectl set-hostname node5.internal\n\r\n\n\r\n\n6.1.3. Настройка DNS\n\r\n\nНа всех узлах\n содержимое файла \n/etc/resolv.conf\n заменим следующим:\n\r\n\nnameserver 172.30.0.2\n\r\n\n\r\n\n6.1.4. Настройка файла hosts\n\r\nПоскольку мы не используем DNS-сервер, то для разрешения важных для нас DNS-имен настроим файлы hosts на всех узлах кластера.\n\r\n\n\r\n\nНа всех узлах\n выполним следующую команду:\n\r\n\n\n                        \nСкрытый текст\n\n                        \ncat > /etc/hosts <<EOF\n127.0.0.1       localhost\n\n# The following lines are desirable for IPv6 capable hosts\n::1     localhost ip6-localhost ip6-loopback\nff02::1 ip6-allnodes\nff02::2 ip6-allrouters\n\n# Cluster nodes\n172.30.0.201 node1.internal\n172.30.0.202 node2.internal\n172.30.0.203 node3.internal\n172.30.0.204 node4.internal\n172.30.0.205 node5.internal\nEOF\n\r\n\n\n                    \n\r\n\n\r\n\n6.1.5. Проверка сетевых настроек\n\r\nУбедимся, что все сетевые настройки сделаны правильно. \n\r\nДля этого на каждом узле по очереди выполним следующие тестовые команды:\n\r\n\n\n                        \nСкрытый текст\n\n                        \n# Проверка доступности шлюза по умолчанию\nping 172.30.0.2 -c 1\n\n# Проверка доступности node1\nping 172.30.0.201 -c 1\nping node1.internal -c 1\n\n# Проверка доступности node2\nping 172.30.0.202 -c 1\nping node2.internal -c 1\n\n# Проверка доступности node3 \nping 172.30.0.203 -c 1\nping node3.internal -c 1\n\n# Проверка доступности node4\nping 172.30.0.204 -c 1\nping node4.internal -c 1\n\n# Проверка доступности node5\nping 172.30.0.205 -c 1\nping node5.internal -c 1\n\n# Проверка «видимости» Интернета\nping 8.8.8.8 -c 1\n\r\n\n\n                    \n\r\n\n\r\nВсе тесты должны проходить без ошибок. Если все так, то делаем снимок состояния виртуальных машин и называем его «NETWORK».\n\r\n\n\r\n\n6.1.6. Установка вспомогательных пакетов\n\r\n\nВариант A.\n Самый простой и быстрый вариант — это установить все и везде, не зависимо от того, нужно оно там или нет.\n\r\n\n\r\n\nНа всех узлах\n выполним команду:\n\r\n\napt install -y curl wget gnupg sudo iptables tmux keepalived haproxy\n\r\n\n\r\n\nВариант B.\n Более трудоемкий вариант — это на каждом узле поставить только то, что нужно. \n\r\n\n\r\n\nНа всех узлах\n выполним команду:\n\r\n\napt install -y curl wget gnupg sudo iptables\n\r\n\n\r\n\nНа узле node1\n выполним команду:\n\r\n\napt install -y tmux\n\r\n\n\r\n\nНа узлах node1, node2, node3\n выполним команду:\n\r\n\napt install -y keepalived haproxy\n\r\n\n\r\n\n6.1.7. Предварительная подготовка Linux для использования Kubernetes\n\r\nСогласно официальной \nдокументации\n, для работы Kubernetes необходимо разрешить маршрутизацию IPv4 трафика, настроить возможность iptables видеть трафик, передаваемый в режиме моста, а также отключить файлы подкачки. \n\r\n\n\r\n\nНа всех узлах\n выполним команды:\n\r\n\n\n                        \nСкрытый текст\n\n                        \n# Настройка автозагрузки и запуск модуля ядра br_netfilter и overlay\ncat <<EOF | tee /etc/modules-load.d/k8s.conf\noverlay\nbr_netfilter\nEOF\n\nmodprobe overlay\nmodprobe br_netfilter\n\n# Разрешение маршрутизации IP-трафика\necho -e \"net.bridge.bridge-nf-call-ip6tables = 1\\nnet.bridge.bridge-nf-call-iptables = 1\\nnet.ipv4.ip_forward = 1\" > /etc/sysctl.d/10-k8s.conf\nsysctl -f /etc/sysctl.d/10-k8s.conf\n\n# Отключение файла подкачки\nswapoff -a\nsed -i '/ swap / s/^/#/' /etc/fstab\n\r\n\n\n                    \n\r\n\n\r\n\nПроверка корректности настройки\n\r\nЧтобы убедиться, что все требуемые параметры настроены правильно, рекомендуется перезагрузить виртуальную машину.\n\r\n\n\r\nДля проверки автоматической загрузки модулей \nbr_netfilter\n и \noverlay\n выполним команды:\n\r\n\n\n                        \nСкрытый текст\n\n                        \nlsmod | grep br_netfilter\nlsmod | grep overlay\n\n## Ожидаемый результат должен быть следующим (цифры могут отличаться):\n# br_netfilter           32768  0\n# bridge                258048  1 br_netfilter\n# overlay               147456  0\n\r\n\n\n                    \n\r\nДля проверки успешности изменения настроек в параметрах сетевого стека выполним команду:\n\r\n\n\n                        \nСкрытый текст\n\n                        \nsysctl net.bridge.bridge-nf-call-iptables net.bridge.bridge-nf-call-ip6tables net.ipv4.ip_forward\n\n## Ожидаемый результат:\n# net.bridge.bridge-nf-call-iptables = 1\n# net.bridge.bridge-nf-call-ip6tables = 1\n# net.ipv4.ip_forward = 1\n\r\n\n\n                    \n\r\nДля проверки отключения файла подкачки выполним команду:\n\r\n\n\n                        \nСкрытый текст\n\n                        \nswapon -s\n\n## Ожидаемый вывод команды – пустой. Она ничего не должна отобразить.\n\r\n\n\n                    \n\r\n\n\r\n\n6.1.8. [ОПЦИОНАЛЬНО] Разрешение авторизации в SSH от пользователя root\n\r\n\nВнимание!\n Данный шаг снижает безопасность узлов. Выполнять его можно только в учебной среде, когда весь виртуальный стенд работает на вашем локальном компьютере, и виртуальные машины не доступны из сети.\n\r\nПри выполнении этой работы нам придется часто подключаться по SSH к машинам нашего стенда. По умолчанию настройки безопасности Debian запрещают нам напрямую подключаться под \nroot\n, и приходится выполнять двойную авторизацию: сначала заходить под пользователем, а потом переключаться на root. Упростим себе жизнь и разрешим SSH сразу пускать нас под \nroot\n/\n\r\n\n\r\n\nНа всех узлах\n выполним следующие команды:\n\r\n\n\n                        \nСкрытый текст\n\n                        \necho \"PermitRootLogin yes\" > /etc/ssh/sshd_config.d/01-permitroot.conf\nservice sshd restart\n\r\n\n\n                    \n\r\nВосстановление запрета авторизации под root выглядит следующим образом.\n\r\n\n\r\n\nНа всех узлах\n нужно будет выполнить команды:\n\r\n\n\n                        \nСкрытый текст\n\n                        \nrm /etc/ssh/sshd_config.d/01-permitroot.conf\nservice sshd restart\n\r\n\n\n                    \n\r\n\n\r\n\nЛайфхак. Использование TMUX для одновременного конфигурирования нескольких узлов\n\r\nЕще одним способом упростить себе жизнь будет использование программы \ntmux\n для совершения одинаковых действий (например, установки программ) на нескольких узлах. Магия работает за счет того, что \ntmux\n позволяет одновременно открыть несколько окон, а затем включить синхронизацию, и консольные команды, введенные в одном окне, будут автоматически транслироваться во все другие окна.\n\r\n\n\r\nВ частности, для проведения одинаковых работ на всех узлах стенда (например, как в следующем шаге) необходимо сделать следующее:\n\r\n\n\r\n\nНа узле \nnode1\n запустить \ntmux\n. Напомню, что именно на этот узел мы ранее поставили \ntmux\n.\n\r\n\nС помощью интерфейса \ntmux\n сделать 4 дополнительных окна.\n\r\n\nВ полученных окнах поочередно совершить подключение по \nssh\n к оставшимся узлам: \nnode2\n, \nnode3\n, \nnode4\n, \nnode5\n.\n\r\n\nПерейти в окно, соответствующее узлу \nnode1\n.\n\r\n\nАктивировать режим синхронизации команд между окнами\n\r\n\nПровести необходимые работы.\n\r\n\n\r\nПо окончании работ:\n\r\n\n\r\n\nОтключить режим синхронизации команд.\n\r\n\nЗакрыть все созданные окна.\n\r\n\nВыйти из \ntmux\n.\n\r\n\n\r\nПрограмма \ntmux\n может управляться как горячими клавишами, так и текстовыми командами. Принцип управления горячими клавишами заключается в нажатии префикса Ctrl-B (одновременно Ctrl и кнопку «B»), а затем кнопки соответствующей команды. Например, <Ctrl-B %> – разделит окно по вертикали. Аналогичная ей текстовая команда «split-window -h» сделает тоже самое. Для перехода в командный режим необходимо нажать <Ctrl-B :>. Перечень всех горячих клавиш можно узнать, нажав <Ctrl-B ?>.\n\r\n\n\r\nС теорией разобрались, рассмотрим пример использования. \n\r\n\n\n                        \nПример проверки доступности Интернет одновременно на всех узлах кластера\n\n                        \n\r\n\nНа узле \nnode1\n запускаем \ntmux\n (Рисунок 4).\n\r\n\n\r\n\nРисунок 4\n\r\n\n\r\n\n\r\n\nНажимая <Ctrl-B “> 3 раза, разделяем окно на 3 горизонтальных окна (Рисунок 5).\n\r\n\n\r\n\nРисунок 5\n\r\n\n\r\n\n\r\n\nНажимая <Ctrl-B %> разделяем нижнее окно на 3 вертикальных окна (Рисунок 6).\n\r\n\n\r\n\nРисунок 6\n\r\n\n\r\n\n\r\n\nИспользуя навигацию между окнами с помощью <Ctrl-B стрелки>, а также возможности изменения размера окон <Ctrl-B Alt-стрелки>, можно добиться более красивого размера окон (Рисунок 7).\n\r\n\n\r\n\nРисунок 7\n\r\n\n\r\nЕсли не хочется мудрить с размерами окон, можно просто нажать <Ctrl-B Alt-2>, тогда появится окно, равномерно разделенное на 5 горизонтальных частей (Рисунок 8).\n\r\n\n\r\n\nРисунок 8\n\r\n\n\r\n\n\r\n\nПоочередно в каждом окне, кроме самого верхнего, с помощью команды «\nssh root@nodeX.internal\n», где X – номер узла (например, node4.internal), подключимся ко всем узлам кластера (Рисунок 9).\n\r\n\n\r\n\nРисунок 9\n\r\n\nПримечание.\n Для отчистки экрана от лишнего вывода можно воспользоваться командой «\nclear\n».\n\r\n\n\r\n\nТеперь перейдем в самое верхнее окно, в этом примере оно будет у нас командным. Затем перейдем в режим ввода команд, нажав <Ctrl-B :> и в командной строке введем команду «\nset synchronize-panes on\n» (Рисунок 10)\n\r\n\n\r\n\nРисунок 10\n\r\n\n\r\n\n\r\n\nТеперь введем команду «\nping 8.8.8.8 -c 1\n», и она одновременно выполнится на всех панелях (Рисунок 11)\n\r\n\n\r\n\nРисунок 11\n\r\n\n\r\n\n\r\n\nДля завершения режима синхронизации необходимо в командном режиме \ntmux\n ввести команду «\nset synchronize-panes of\nf». Закрытие панелей производится клавишами <Ctrl-B x>. Другой вариант в режиме активной синхронизации просто набрать команду «\nexit\n».\n\r\n\n\r\n\n\n                    \n\r\n\nВНИМАНИЕ!\n Использование \ntmux\n для установок ПО может привести к психологической зависимости от чувства азарта, вызванного переживанием за скорость выполнения процесса в том или ином окне. Не надо расстраиваться и удивляться, когда в каждом конкретном случае окно победитель будет отличаться от ваших ожиданий. Помните, азартные игры до добра не доводят.\n\r\n\n\r\n\n6.1.9. Установка kubeadm и kubectl\n\r\n\nkubectl\n – основная утилита командной строки для управления кластером Kubernetes, \nkubeadm\n – утилита для развертывания кластера Kubernetes. Установка данных утилит осуществляется в соответствии с официальным \nруководством\n.\n\r\n\n\r\n\nНа всех узлах\n выполним следующие команды:\n\r\n\n\n                        \nСкрытый текст\n\n                        \n# Настройка deb-репозитория Kubernetes\ncurl -fsSLo /etc/apt/trusted.gpg.d/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg\n\necho \"deb [signed-by=/etc/apt/trusted.gpg.d/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main\" | tee /etc/apt/sources.list.d/kubernetes.list\n\n# Обновление перечня доступных пакетов\napt update\n\n# Установка пакетов kubeadm и kubectl\napt install -y kubeadm kubectl\n\r\n\n\n                    \n\r\nНа этом первый этап завершен. Наши узлы готовы к дальнейшим экспериментам. Делаем снимок состояния виртуальных машин и называем его «HOST IS PREPARED». К этому снимку мы будем неоднократно возвращаться.\n\r\n\n\r\n\n6.2. Установка контейнерного движка\n\r\nKubernetes модульная система и может работать с различными контейнерными движками. При проведении экспериментов будем устанавливать по одному движку за раз. Хотя, как вы наверное догадались, на хосте одновременно может быть несколько движков и на разных узлах используемые движки могут отличаться, но это уже совсем другая история.\n\r\n\n\r\nПо окончанию установки движка будем делать снимок состояния виртуальных машин, затем откатываться на предыдущий снимок («HOST IS PREPARED») и ставить следующий движок.\n\r\n\n\r\n\n6.2.А. Вариант A. Установка cri-o\n\r\nУстановка по осуществляется по официальной \nдокументации\n. \n\r\n\n\r\n\nНа всех узлах\n выполним следующие команды:\n\r\n\n\n                        \nСкрытый текст\n\n                        \nexport VERSION=1.26\nexport OS=Debian_11\n\necho \"deb https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/ /\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\necho \"deb http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/ /\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$VERSION.list\n\ncurl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$VERSION/$OS/Release.key | apt-key add -\ncurl -L https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$OS/Release.key | apt-key add -\n\napt-get update\napt-get install -y cri-o cri-o-runc\n\nmkdir /var/lib/crio\n\nsystemctl daemon-reload\nsystemctl enable --now crio\n\r\n\n\n                    \n\r\n\n\r\n\n6.2.А.1. Проверка доступности сокета cri-o\n\r\nДля тестов мы будем использовать утилиту \ncrictl\n, которая автоматически установилась вместе с \nkubeadm\n.\n\r\n\n\r\n\nНа всех узлах\n запустим команду:\n\r\n\n\n                        \nСкрытый текст\n\n                        \ncrictl --runtime-endpoint unix:///var/run/crio/crio.sock version\n\n## Ожидаемый результат:\n# Version:  0.1.0\n# RuntimeName:  cri-o\n# RuntimeVersion:  1.25.2\n# RuntimeApiVersion:  v1\n\r\n\n\n                    \n\r\n\n6.2.А.2. Проверка запуска контейнеров с помощью cri-o\n\r\nКроме доступности сокета мы можем проверить фактическую возможность запуска контейнеров.\n\r\n\n\r\n\nНа всех узлах\n запустим команды:\n\r\n\n\n                        \nСкрытый текст\n\n                        \nexport CONTAINER_RUNTIME_ENDPOINT=unix:///run/crio/crio.sock\n\ncat > pod.config << _EOF_\n{\n    \"metadata\": {\n        \"name\": \"test-pod\",\n        \"namespace\": \"default\",\n        \"attempt\": 1,\n        \"uid\": \"18fbfef14ae3a43\"\n    },\n    \"log_directory\": \"/tmp\",\n    \"linux\": {\n    }\n}\n_EOF_\n\ncat > container.config << _EOF_\n{\n  \"metadata\": {\n    \"name\": \"hello-world-container\"\n  },\n  \"image\":{\n    \"image\": \"hello-world\"\n  },\n  \"log_path\":\"hello-world.log\",\n  \"linux\": {\n  }\n}\n_EOF_\n\ncrictl pull hello-world\n#crictl run container.config pod.config\n\nPOD_ID=$(crictl runp pod.config)\nCONTAINER_ID=$(crictl create $POD_ID container.config pod.config)\ncrictl start $CONTAINER_ID\n\ncat /tmp/hello-world.log\n\n## Ожидаемый ответ\n# …\n# 2023-03-28T20:06:48.436017504+03:00 stdout F\n# 2023-03-28T20:06:48.436017504+03:00 stdout F Hello from Docker!\n# …\n\r\n\n\n                    \n\r\nЕсли все тесты прошли успешно, то делаем снимок состояния виртуальных машин и называем его «CRI-O».\n\r\n\n\r\n\n6.2.B. Вариант B. Установка containerd\n\r\nОткатываемся к состоянию виртуальных машин «HOST IS PREPARED». Установка осуществляется в соответствии с официальным \nруководством\n.\n\r\n\n\r\n\nНа всех узлах\n выполним команды:\n\r\n\n\n                        \nСкрытый текст\n\n                        \n# Установка containerd\nwget https://github.com/containerd/containerd/releases/download/v1.7.0/containerd-1.7.0-linux-amd64.tar.gz\ntar Cxzvf /usr/local containerd-1.7.0-linux-amd64.tar.gz\nrm containerd-1.7.0-linux-amd64.tar.gz\n\n# Создание конфигурации по умолчанию для containerd\nmkdir /etc/containerd/\ncontainerd config default > /etc/containerd/config.toml\n\n# Настройка cgroup драйвера\nsed -i 's/SystemdCgroup \\= false/SystemdCgroup \\= true/g' /etc/containerd/config.toml\n\n# Установка systemd сервиса для containerd\nwget https://raw.githubusercontent.com/containerd/containerd/main/containerd.service\nmv containerd.service /etc/systemd/system/\n\n# Установка компонента runc\nwget https://github.com/opencontainers/runc/releases/download/v1.1.4/runc.amd64\ninstall -m 755 runc.amd64 /usr/local/sbin/runc\nrm runc.amd64\n\n# Установка сетевых плагинов:\nwget https://github.com/containernetworking/plugins/releases/download/v1.2.0/cni-plugins-linux-amd64-v1.2.0.tgz\nmkdir -p /opt/cni/bin\ntar Cxzvf /opt/cni/bin cni-plugins-linux-amd64-v1.2.0.tgz\nrm cni-plugins-linux-amd64-v1.2.0.tgz\n\n# Запуск сервиса containerd\nsystemctl daemon-reload\nsystemctl enable --now containerd\n\r\n\n\n                    \n\r\n\n\r\n\n6.2.B.1. Проверка доступности сокета containerd\n\r\n\nНа всех узлах\n выполним команду:\n\r\n\n\n                        \nЗаголовок спойлера\n\n                        \ncrictl --runtime-endpoint unix:///var/run/containerd/containerd.sock version\n\n## Ожидаемый результат:\n# Version:  0.1.0\n# RuntimeName:  containerd\n# RuntimeVersion:  v1.7.0\n# RuntimeApiVersion:  v1\n\r\n\n\n                    \n\r\n\n6.2.B.2. Проверка возможности запуска контейнеров с помощью containerd\n\r\n\nНа всех узлах\n выполним команды:\n\r\n\n\n                        \nСкрытый текст\n\n                        \nctr images pull docker.io/library/hello-world:latest\nctr run docker.io/library/hello-world:latest hello-world\n\n## Ожидаемый результат:\n# …\n# Hello from Docker!\n# This message shows that your installation appears to be working correctly.\n# …\n\r\n\n\n                    \n\r\nЕсли все тесты прошли успешно, то делаем снимок состояния виртуальных машин и называем его «CONTAINERD».\n\r\n\n\r\n\n6.2.C. Вариант C. Установка Docker + cri-dockerd\n\r\nОткатываемся к состоянию виртуальных машин «HOST IS PREPARED». В начале следует установить Docker. Для этого воспользуется официальным \nруководством\n.\n\r\n\n\r\n\nНа всех узлах\n выполним следующие команды:\n\r\n\n\n                        \nСкрытый текст\n\n                        \ncurl -fsSL https://download.docker.com/linux/debian/gpg | gpg --dearmor -o /etc/apt/trusted.gpg.d/docker.gpg\n\necho \\\n  \"deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/trusted.gpg.d/docker.gpg] https://download.docker.com/linux/debian \\\n  $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null\n\napt update\n\napt install -y docker-ce docker-ce-cli containerd.io docker-compose-plugin\n\r\n\n\n                    \n\r\nПосле установки Docker установим плагин cri-dockerd. Делать это будем по размещённому в сети \nгайду\n.\n\r\n\n\r\n\nНа всех узлах\n выполним следующие команды:\n\r\n\n\n                        \nСкрытый текст\n\n                        \nwget https://github.com/Mirantis/cri-dockerd/releases/download/v0.3.1/cri-dockerd-0.3.1.amd64.tgz\ntar xvf cri-dockerd-0.3.1.amd64.tgz\nmv cri-dockerd/cri-dockerd /usr/local/bin/\n\nwget https://raw.githubusercontent.com/Mirantis/cri-dockerd/master/packaging/systemd/cri-docker.service\nwget https://raw.githubusercontent.com/Mirantis/cri-dockerd/master/packaging/systemd/cri-docker.socket\n\nmv cri-docker.socket cri-docker.service /etc/systemd/system/\nsed -i -e 's,/usr/bin/cri-dockerd,/usr/local/bin/cri-dockerd,' /etc/systemd/system/cri-docker.service\n\nsystemctl daemon-reload\nsystemctl enable cri-docker.service\nsystemctl enable --now cri-docker.socket\n\r\n\n\n                    \n\r\n\n\r\n\n6.2.C.1. Проверка доступности сокета cri-dockerd\n\r\n\nНа всех узлах\n выполним следующую команду:\n\r\n\n\n                        \nСкрытый текст\n\n                        \ncrictl --runtime-endpoint unix:///var/run/cri-dockerd.sock version\n\n## Ожидаемый результат:\n# Version:  0.1.0\n# RuntimeName:  docker\n# RuntimeVersion:  23.0.1\n# RuntimeApiVersion:  v1\n\r\n\n\n                    \n\r\n\n6.2.C.2. Проверка возможности Docker запускать контейнеры\n\r\n\nНа всех узлах\n выполним следующую команду:\n\r\n\n\n                        \nСкрытый текст\n\n                        \ndocker run hello-world\n\n## Ожидаемый результат:\n# …\n# Hello from Docker!\n# This message shows that your installation appears to be working correctly.\n# …\n\r\n\n\n                    \n\r\nЕсли все тесты прошли успешно, то делаем снимок состояния виртуальных машин и называем его «CRI-DOCKERD».\n\r\n\n\r\nНа этом этап завершен. В зависимости от выбранного варианта на всех узлах должен быть установлен один из контейнерных движков, причем везде он должен быть одинаковым.\n\r\n\n\r\n\n6.3. Развёртывание Kubernetes\n\r\n\n\r\n\n6.3.A. Вариант A. Установка вырожденного Kubernete кластера\n\r\nКластер «все-в-одном» будем разворачивать на узле \nnode1\n, соответственно все приведенные команды будут выполняться на нем же. Процесс разворачивания кластера состоит из следующих этапов:\n\r\n\n\r\n\nИнициализация кластера Kubernetes.\n\r\n\nКонфигурирование утилиты управления kubectl.\n\r\n\nУстановка сетевого плагина.\n\r\n\nНастройка управляющего узла для выполнения рабочих нагрузок.\n\r\n\n\r\n\n\r\n\n6.3.A.1. Инициализация кластера Kubernetes\n\r\nСначала рассмотрим инициализацию кластера на базе cri-o или containerd. Как это делать для Docker + cri-dockerd, расскажем чуть дальше. Инициализация кластера проводится одной командой:\n\r\n\nkubeadm init --pod-network-cidr=10.244.0.0/16\n\r\nЗдесь в параметре --pod-network-cidr мы явно указываем IP-подсеть, которая будет использоваться \"\nподами\n\". Конкретный диапазон 10.244.0.0/16 выбран таким образом, чтобы совпадать с диапазоном по умолчанию для сетевого плагина \nflannel\n, который мы будем устанавливать чуть позже.\n\r\n\n\r\nДля варианта с Docker + cri-dockerd рассмотренную ранее команду нужно чуть-чуть изменить:\n\r\n\nkubeadm init \\\n               --pod-network-cidr=10.244.0.0/16 \\\n               --cri-socket unix:///var/run/cri-dockerd.sock\n\r\nЗдесь мы добавили параметр --cri-socket. Он нужен нам, чтобы указать, через какой Unix сокет Kubernetes должен общаться с контейнерным движком. Для cri-o или containerd мы этого не делали, так как доступный сокет там был всего один, и \nkubeadm\n об этом знал. Связка Docker + cri-dockerd создает на узле два сокета: один для Docker, а второй для cri-dockerd, поэтому Kubernetes требуется явно указать, какой из них использовать.\n\r\n\n\r\n\n6.3.A.2. Конфигурирование утилиты управления kubectl\n\r\nkubectl – основной рабочий инструмент по управлению кластером Kubernetes. По окончанию инициализации кластера необходимо настроить ее конфигурацию. Поскольку в начале статьи мы договорились, что будем работать от root, то для конфигурирования kubectl выполним следующие команды:\n\r\n\necho \"export KUBECONFIG=/etc/kubernetes/admin.conf\" > /etc/environment\nexport KUBECONFIG=/etc/kubernetes/admin.conf\n\r\nВыполнять подобную операцию нужно на всех узлах, с которых мы хотим управлять Kubernetes. Для этого, конечно, требуется убедиться, что существует конфигурационный файл \n/etc/kubernetes/admin.conf\n. В текущем случае данный файл был автоматически создан утилитой \nkubeadm\n, в других случаях его нужно явно поместить на требуемый узел.\n\r\n\n\r\n\n6.3.A.3. Установка сетевого плагина\n\r\nКак уже упоминалось выше, в качестве сетевого плагина мы будем использовать flannel. Тема сетевого взаимодействия в Kubernetes довольно обширна, и ей посвящено большое количество различных статей и документации. Лезть в эти дебри на данном этапе противопоказано, поэтому просто поставим один из самых распространённых сетевых плагинов. \n\r\n\n\r\n\nНа node1\n выполним команду:\n\r\n\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n\r\n\n\r\n\n6.3.A.4. Настройка управляющего узла для выполнения рабочих нагрузок\n\r\nДля управления тем, какая нагрузка (рабочая / управляющая) может выполняться на узле, используется механизм \n«заражения и толерантности»\n (taint and toleration). Если говорить простыми словами, то это механизм меток. С помощью «заражений» узлу присваиваются определенные метки. Механизм толерантности показывает, с какими метками \"\nпод\n\" готов мириться, а с какими он работать не будет. По умолчанию на управляющих узлах Kubernetes устанавливается метка \n«node-role.kubernetes.io/control-plane»\n. Рабочие \"\nподы\n\" без дополнительный настроек на узлах с подобной меткой запускаться не будут.\n\r\n\n\r\nДля того чтобы разрешить выполнение рабочих \"\nподов\n\" на управляющем узле, с последнего нужно снять метку \nnode-role.kubernetes.io/control-plane\n, что может быть сделано командой:\n\r\n\nkubectl taint nodes --all node-role.kubernetes.io/control-plane-\n\r\n\nПримечание.\n Для простоты данная команда снимает метку со всех узлов кластера. Для случая кластера из одной машины это нормально, но в многомашинном варианте рекомендуется все же явно указывать узлы, с которых будут сниматься метки.\n\r\nВырожденный кластер «все-в-одном» готов. На узле \nnode1\n можно сделать снимок состояния виртуальной машины и назвать «ALL IN ONE». \n\r\n\n\r\n\n6.3.B. Вариант B. Организация отказоустойчивого кластера Kubernetes\n\r\nНастройку данного варианта следует начать с отката состояния виртуальных машин на момент завершения установки любого из контейнерных движков.\n\r\n\n\r\n\n6.3.B.1. Объяснение, за счет чего достигается отказоустойчивость\n\r\nОрганизация отказоустойчивости в кластере Kubernetes базируется на решении двух стратегических задач:\n\r\n\n\r\n\nОтказоустойчивое выполнение рабочих нагрузок.\n\r\n\nОтказоустойчивое управление кластером.\n\r\n\n\r\nРешение первой задачи основывается на логике управления кластером. В отличии от Docker, здесь администратор не запускает контейнеры явным образом. Вместо этого он описывает желаемое состояние (desired state) кластера, в котором указывает какие \"\nподы\n\" должны быть запущены. Система управления кластером сравнивает текущее состояние с желаемым. Если эти состояния различаются, то система выполняет действия по приведению желаемого к текущему с учетом доступных ресурсов.\n\r\n\n\r\nПоясним на примере. Администратор во время конфигурации кластера указал, что желает видеть запущенными два \"\nпода\n\" с контейнерами \nnginx\n. После получения воли высшего существа система управления кластером смотрит – \"\nподов\n\" нет, nginx нет – не дела, скачивает nginx из репозитория, настраивает по конфигурации админа \"\nподы\n\", затем запускает их. Вот теперь желаемое = текущее. Если далее по каким-то мистическим причинам узел кластера, на котором крутились \"\nподы\n\", откажет, то система управления увидит, что текущее состояние отличается от желаемого, и автоматически запустит недостающие \"\nподы\n\" на другом доступном узле кластера, вновь делая желаемое = текущее. Вот таким интересным способом и достигается отказоустойчивость выполнения рабочих нагрузок.\n\r\n\n\r\nРешение второй задачи разбивается на подзадачи:\n\r\n\n\r\n\nОрганизация отказоустойчивого хранения и использования конфигурации кластера.\n\r\n\nОрганизация отказоустойчивого доступа к API системы управления кластером.\n\r\n\n\r\nПервая задача решается путем применения системы распределенного хранения данных etcd. Данная система хранит конфигурацию кластера одновременно на нескольких узлах и обеспечивает непротиворечивость имеющихся копий. Она позволяет продолжать нормальную работу кластера при выходе из строя некоторых узлов с репликами данных, а потом их горячее переподключение после восстановления. При использовании \nkubeadm\n для настройки кластера вся предварительная настройка etcd полностью ложится на его кремниевые плечи, и нам ничего делать не нужно.\n\r\n\n\r\nРешение второй задачи базируется на использовании внешнего компонента – балансировщика нагрузки. С его помощью создается виртуальный IP-адрес, запросы к которому автоматически транслируются на один из управляющих узлов кластера. Вся магия заключается в том, что все управляющие узлы кластера равноправны, и кластер может находиться под управлением любого из них. Поэтому и создается виртуальный IP-адрес, через который будет доступен как минимум один управляющий узел, а иногда и все управляющие узлы кластера в режиме очередности (round robin).\n\r\nК сожалению, настроить балансировщик нагрузки \nkubeadm\n не может, и эту работу нам придется делать самим.\n\r\n\n\r\n\n6.3.B.2. Настройка балансировщика нагрузки\n\r\nДля создания виртуального IP адреса и перераспределение нагрузки между управляющими узлами будем использовать комбинацию двух демонов: keepalived и haproxy. Этих ребят мы с вами установили ранее на узлы: \nnode1\n, \nnode2\n, \nnode3\n.\n\r\n\n\r\nРеализуемый нами балансировщик нагрузки будет работать следующим образом (Рисунок 12):\n\r\n\n\r\n\nДемон keepalived обеспечит функционирование виртуального IP-адреса и его привязку к одному из управляющих узлов. Виртуальный IP будет вторым адресом на сетевом интерфейсе узла. Если данный узел откажет, то keepalived обнаружит это и перекинет виртуальный IP-адрес на другой доступный узел.\n\r\n\nПоступающие на управляющий узел запросы будут обрабатываться демоном haproxy, который, выполняя роль реверс-прокси (reverse proxy), будет поочередно (round robin) пересылать их на API сервера управляющих узлов Kubernetes.\n\r\n\n\r\n \n\r\n\nРисунок 12\n\r\n\n\r\nС одной стороны, одного keepalived должно хватить для построения полноценного балансировщика нагрузки, но официальный \nгайд\n регламентирует его использование в паре с haproxy. \nНародная мудрость\n мотивирует использование сразу двух демонов тем, что так якобы лучше балансируется нагрузка. Сложно сказать, так это или нет, на самом деле, но в учебных целях мы все же прислушаемся к рекомендациям и построим балансировщик с использованием обоих демонов.\n\r\n\n\r\nКак во всем, что касается Kubernetes, демонов можно внедрить различными способами:\n\r\n\n\r\n\nДемоны могут быть реализованы в виде \"\nподов\n\" Kubernetes.\n\r\n\nДемоны могут устанавливаться отдельно в операционную систему управляющих узлов.\n\r\n\n\r\nДля большей наглядности выберем второй вариант. При развертывания балансировщика нагрузки будем использовать следующие сетевые настройки:\n\r\n\n\r\n\nв качестве виртуального адреса будет использоваться 172.30.0.210;\n\r\n\nсвязанное с виртуальным адресом DNS имя: k8s-cp.internal;\n\r\n\nTCP порт для доступа к системе управления: 8888;\n\r\n\nв качестве бэкендов будут использоваться порты 6443 на управляющих узлах, другими словами, 172.30.0.201:6443, 172.30.0.202:6443, 172.30.0.203:6443;\n\r\n\n\r\nПри проведении работ все параметры будут жестко вбиты в конфигурационные файлы и скрипты.\n\r\n\n\r\n\n6.3.B.2.1. Настройка демона keepalived\n\r\n\nНа узлах node1, node2, node3\n создадим и отредактируем основной конфигурационный файл демона keepalived \n/etc/keepalived/keepalived.conf\n следующим образом:\n\r\n\n\n                        \nСкрытый текст\n\n                        \n# File: /etc/keepalived/keepalived.conf\n\nglobal_defs {\n    enable_script_security\n    script_user nobody\n}\n\nvrrp_script check_apiserver {\n  script \"/etc/keepalived/check_apiserver.sh\"\n  interval 3\n}\n\nvrrp_instance VI_1 {\n    state BACKUP\n    interface ens33\n    virtual_router_id 5\n    priority 100\n    advert_int 1\n    nopreempt\n    authentication {\n        auth_type PASS\n        auth_pass ZqSj#f1G\n    }\n    virtual_ipaddress {\n        172.30.0.210\n    }\n    track_script {\n        check_apiserver\n    }\n}\n\r\n\n\n                    \n\r\n\n\r\n\nНа узлах node1, node2, node3\n создадим и отредактируем скрипт \n/etc/keepalived/check_apiserver.sh\n, предназначенный для проверки доступности серверов.\n\r\n\n\n                        \nСкрытый текст\n\n                        \n#!/bin/sh\n# File: /etc/keepalived/check_apiserver.sh\n\nAPISERVER_VIP=172.30.0.210\nAPISERVER_DEST_PORT=8888\nPROTO=http\n\nerrorExit() {\n    echo \"*** $*\" 1>&2\n    exit 1\n}\n\ncurl --silent --max-time 2 --insecure ${PROTO}://localhost:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit \"Error GET ${PROTO}://localhost:${APISERVER_DEST_PORT}/\"\nif ip addr | grep -q ${APISERVER_VIP}; then\n    curl --silent --max-time 2 --insecure ${PROTO}://${APISERVER_VIP}:${APISERVER_DEST_PORT}/ -o /dev/null || errorExit \"Error GET ${PROTO}://${APISERVER_VIP}:${APISERVER_DEST_PORT}/\"\nfi\n\r\n\n\n                    \n\r\n\nНа узлах node1, node2, node3\n установим атрибут, разрешающий исполнение скрипта, и запустим демона keepalived.\n\r\n\n\n                        \nСкрытый текст\n\n                        \nchmod +x /etc/keepalived/check_apiserver.sh\nsystemctl enable keepalived\nsystemctl start keepalived\n\r\n\n\n                    \n\r\n\n\r\n\n6.3.B.2.2. Настройка демона haproxy\n\r\n\nНа узлах node1, node2, node3\n отредактируем основной конфигурационный файл демона haproxy \n/etc/haproxy/haproxy.cfg\n следующим образом:\n\r\n\n\n                        \nСкрытый текст\n\n                        \n# File: /etc/haproxy/haproxy.cfg\n#---------------------------------------------------------------------\n# Global settings\n#---------------------------------------------------------------------\nglobal\n    log /dev/log local0\n    log /dev/log local1 notice\n    daemon\n\n#---------------------------------------------------------------------\n# common defaults that all the 'listen' and 'backend' sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    option forwardfor       except 127.0.0.0/8\n    option                  redispatch\n    retries                 1\n    timeout http-request    10s\n    timeout queue           20s\n    timeout connect         5s\n    timeout client          20s\n    timeout server          20s\n    timeout http-keep-alive 10s\n    timeout check           10s\n\n#---------------------------------------------------------------------\n# apiserver frontend which proxys to the control plane nodes\n#---------------------------------------------------------------------\nfrontend apiserver\n    bind *:8888\n    mode tcp\n    option tcplog\n    default_backend apiserver\n\n#---------------------------------------------------------------------\n# round robin balancing for apiserver\n#---------------------------------------------------------------------\nbackend apiserver\n    option httpchk GET /healthz\n    http-check expect status 200\n    mode tcp\n    option ssl-hello-chk\n    balance     roundrobin\n        server node1 172.30.0.201:6443 check\n        server node2 172.30.0.202:6443 check\n        server node3 172.30.0.203:6443 check\n\n\n                    \n\r\n\nНа узлах node1, node2, node3\n запустим демона haproxy, выполнив команды:\n\r\n\n\n                        \nСкрытый текст\n\n                        \n\nsystemctl enable haproxy\nsystemctl restart haproxy\n\n\n                    \n\r\n\n\r\n\nПримечание.\n Демон будет ругаться, что не обнаружены backend сервера. Это нормально, так как Kubernetes API еще не запущен.\n\r\n\n\r\n\n6.3.B.3. Установка управляющих узлов кластера\n\r\nУстановка производится по официальной \nдокументации\n, выбран режим stacked control plane.\n\r\n\nПримечание!\n При использовании Docker + cri-dockerd к строке инициализации нужно добавить параметр: --cri-socket unix:///var/run/cri-dockerd.sock\n\r\n\n\r\n\n6.3.B.3.1. Установка первого управляющего узла\n\r\n\nНа node1 \n\r\nПри использовании контейнерных движков cri-o и containerd выполняем следующую команду:\n\r\n\nkubeadm init \\\n               --pod-network-cidr=10.244.0.0/16 \\\n               --control-plane-endpoint \"172.30.0.210:8888\" \\\n               --upload-certs\n\r\n\nПримечание 1. \n--pod-network-cidr=10.244.0.0/16 выбрано для упрощения дальнейшей установки сетевого плагина flannel.\n\r\n\nПримечание 2.\n --control-plane-endpoint «172.30.0.210:8888» указывает на виртуальный IP адрес, используемый для управления кластером.\n\r\n\n\r\nНа случай использования Docker + cri-dokerd команда инициализации будет выглядеть так:\n\r\n\nkubeadm init \\\n               --cri-socket unix:///var/run/cri-dockerd.sock \\\n               --pod-network-cidr=10.244.0.0/16 \\\n               --control-plane-endpoint \"172.30.0.210:8888\" \\\n               --upload-certs\n\r\n\n\r\nПо окончанию процедуры должна появиться строка для добавления \nуправляющих\n узлов в кластер.\n\r\n\n….\nYou can now join any number of the control-plane node running the following command on each as root:\n\n  kubeadm join 172.30.0.210:8888 --token 4uvhjf.pmq742i3rofly0qr \\\n        --discovery-token-ca-cert-hash sha256:9cf1614b335f50f8a0014d45534f4ab702319c32111d2124285655cc7cbcdf60 \\\n        --control-plane --certificate-key 15489535ff4f00324eb23808585d3b9acddf38801069f92bf39f8404c677ffa9\n….\n\r\n\n\r\nКроме указанной строки, будет показана строка для добавления \nрабочих\n узлов в кластер.\n\r\n\n\n...\nThen you can join any number of worker nodes by running the following on each as root:\n\nkubeadm join 172.30.0.210:8888 --token 4uvhjf.pmq742i3rofly0qr \\\n        --discovery-token-ca-cert-hash sha256:9cf1614b335f50f8a0014d45534f4ab702319c32111d2124285655cc7cbcdf60\n...\n\n\r\n\n\r\n\nПримечание.\n Приведенные выше строки будут изменяться от инсталляции к инсталляции. Здесь они приведены для примера, копировать их отсюда не имеет смысла.\n\r\n\n\r\n\n6.3.B.3.2. Установка последующих управляющих узлов\n\r\n\nНа node2, node3\n\r\nИспользуем строку подключения, полученную после создания первого управляющего узла кластера. При использовании в качестве движка связки Docker + cri-dokerd не забудьте добавить к этой строке параметр --cri-socket unix:///var/run/cri-dockerd.sock\n\r\n\nВнимание! \nВ строке содержится конфиденциальная информация. Сертификаты для подключения будут автоматически удалены после 2-х часов с момента первичной инициализации кластера.\n\r\nВ случае успешного добавления узла среди вывода \nkubeadm\n должна быть строка:\n\r\n\n…\nThis node has joined the cluster and a new control plane instance was created:\n…\n\r\n\n\r\n\n6.3.B.4. Установка рабочих узлов кластера\n\r\n\nНа узлах node4, node5\n запускаем команду добавления рабочих узлов, полученную при установке первого управляющего узла. \n\r\n\nПримечание.\n Если по каким-то причинам вы ее потеряли, то на любом узле кластера введите команду «kubeadm token create --print-join-command», и она отобразится снова.\n\r\n\n\r\n\n6.3.B.5. Настройка kubeсtl\n\r\n\nНа узлах node1, node2, node3\n выполним команду:\n\r\n\necho \"export KUBECONFIG=/etc/kubernetes/admin.conf\" > /etc/environment\nexport KUBECONFIG=/etc/kubernetes/admin.conf\n\r\n\nПримечание.\n Выполнение этой команды на рабочих узлах не имеет смысла, так как на них отсутствует файл \n/etc/kubernetes/admin.conf\n, автоматически создаваемый \nkubeadm\n при добавлении управляющего узла.\n\r\n\n\r\n\n6.3.B.6. Установка сетевого плагина\n\r\nДанная процедура полностью повторяет аналогичную для случая вырожденного кластера.\n\r\n\n\r\n\nНа node1\n запускаем команду:\n\r\n\nkubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml\n\r\n\n\r\nОтказоустойчивый кластер Kubernetes готов. Рекомендуется на всех узлах стенда сделать снимок состояния виртуальных машины и назвать его «HA CLUSTER».\n\r\n\n\r\n\n7. Проверка работы кластера Kubernetes\n\r\nПриведенные ниже проверки можно запускать на любом узле кластера, на котором настроена работа kubectl.\n\r\n\n\r\nПроверка включения узлов в кластер:\n\r\n\n\n                        \nСкрытый текст\n\n                        \nkubectl get nodes\n\n## Ожидаемый ответ:\n# NAME             STATUS   ROLES           AGE   VERSION\n# node1.internal   Ready    control-plane   33m   v1.26.1\n# node2.internal   Ready    control-plane   11m   v1.26.1\n# node3.internal   Ready    control-plane   11m   v1.26.1\n# node4.internal   Ready    <none>          93s   v1.26.1\n# node5.internal   Ready    <none>          92s   v1.26.1\n\n\r\n\n\n                    \n\r\n\nПримечание. \n Если узел в кластер добавлен недавно, то ответ команды может отличаться от ожидаемого. Необходимо некоторое время, чтобы новый узел «освоился».\n\r\n\n\r\nПри использовании \nkubeadm\n все управляющее ПО кластера работает на нем в виде \"\nподов\n\". Очень важно, чтобы все \"\nподы\n\" работали правильным образом, и не было их циклических перезапусков (restarts).\n\r\n\n\r\nПроверить состояние \"\nподов\n\" можно с помощью команды:\n\r\n\n\n                        \nСкрытый текст\n\n                        \nkubectl get pods -A\n\n# Ожидаемый результат\n# NAMESPACE      NAME                                     READY   STATUS    RESTARTS      AGE\n# kube-flannel   kube-flannel-ds-2p64k                    1/1     Running   0             2m58s\n# kube-flannel   kube-flannel-ds-77vxc                    1/1     Running   0             17m\n# kube-flannel   kube-flannel-ds-9pk6s                    1/1     Running   0             12m\n# kube-flannel   kube-flannel-ds-dhlzp                    1/1     Running   0             11m\n# kube-flannel   kube-flannel-ds-sts89                    1/1     Running   0             2m58s\n# kube-system    coredns-787d4945fb-sx2kd                 1/1     Running   0             34m\n# kube-system    coredns-787d4945fb-xj2kq                 1/1     Running   0             34m\n# kube-system    etcd-node1.internal                      1/1     Running   0             34m\n# kube-system    etcd-node2.internal                      1/1     Running   0             12m\n# kube-system    etcd-node3.internal                      1/1     Running   0             12m\n# kube-system    kube-apiserver-node1.internal            1/1     Running   0             34m\n# kube-system    kube-apiserver-node2.internal            1/1     Running   0             12m\n# kube-system    kube-apiserver-node3.internal            1/1     Running   0             12m\n# kube-system    kube-controller-manager-node1.internal   1/1     Running   1 (12m ago)   34m\n# kube-system    kube-controller-manager-node2.internal   1/1     Running   0             12m\n# kube-system    kube-controller-manager-node3.internal   1/1     Running   0             12m\n# kube-system    kube-proxy-5m2lt                         1/1     Running   0             2m58s\n# kube-system    kube-proxy-bwvk6                         1/1     Running   0             2m58s\n# kube-system    kube-proxy-d4f89                         1/1     Running   0             12m\n# kube-system    kube-proxy-gx9fd                         1/1     Running   0             11m\n# kube-system    kube-proxy-w2skj                         1/1     Running   0             34m\n# kube-system    kube-scheduler-node1.internal            1/1     Running   1 (12m ago)   34m\n# kube-system    kube-scheduler-node2.internal            1/1     Running   0             12m\n# kube-system    kube-scheduler-node3.internal            1/1     Running   0             12m\n\r\n\n\n                    \n\r\n\nТиповые проблемы:\n\r\n\n\r\n\nЧасть \"\nподов\n\", например, coredns, запустившись, не переходит в состояние готовности. Это может происходить из-за проблем с сетевым плагином. Например, вы забыли его установить.\n\r\n\n\"\nПоды\n\" циклически перезагружаются. Одной из вероятных причин подобного события при использовании в качестве контейнерного движка containerd является неправильная настройка cgroup драйвера в нем. Переставьте containerd заново, в точности выполнив все указания настоящего гайда.\n\r\n\n\r\n\n\r\n\n8. Тестовые запуски \"\nподов\n\" в Kubernetes\n\r\n\n\r\n\n8.1. Тест 1. Запуск \"\nпода\n\" в интерактивном режиме.\n\r\nДанный тест позаимствован из официальной \nдокументации\n.\n\r\n\n\n                        \nСкрытый текст\n\n                        \nkubectl run -i --tty busybox --image=busybox -- sh\n\n## Ожидаемый результат:\n# If you don't see a command prompt, try pressing enter.\n# / #\n# / #\n\r\n\n\n                    \n\r\n\nВспомогательные команды:\n\r\n\n\r\nПереподключение к \"\nподу\n\" при выходе из интерактивного режима:\n\r\n\nkubectl attach busybox -i\n\r\n\n\r\nУдаление \"\nпода\n\":\n\r\n\nkubectl delete pod busybox\n\r\n\n\r\n\n8.2. Тест 2. Запуск NGINX\n\r\nТест заключается в запуске Web-сервера nginx и обращения к нему после этого. \n\r\n\n\n                        \nСкрытый текст\n\n                        \n\nkubectl create deployment nginx-app --image=nginx\nkubectl expose deployment nginx-app --type=NodePort --port=80 --external-ip=10.10.10.10\nsleep 5s\ncurl http://10.10.10.10\n\n## Ожидаемый результат\n# <!DOCTYPE html>\n# <html>\n# <head>\n# <title>Welcome to nginx!</title>\n# ...\n\n\r\n\n\n                    \n\r\n\nПримечание.\n Зафиксированы случаи, когда репозиторий накладывает ограничения на возможность скачивания базовых образов, из-за чего тесты проваливаются. Если вы попали в подобную ситуацию, попробуйте сменить IP-адрес, например, с помощью VPN.\n\r\n\n\r\n\n9. Диагностика балансировщика нагрузки\n\r\nДля того чтобы определить узел, на котором сейчас активен виртуальный IP-адрес, поочередно запускаем на управляющих узлах команду:\n\r\n\n\n                        \nСкрытый текст\n\n                        \nip a\n\n## Ожидаемый ответ:\n# 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen \n# 1000\n#     link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n#    inet 127.0.0.1/8 scope host lo\n#       valid_lft forever preferred_lft forever\n#    inet6 ::1/128 scope host\n#       valid_lft forever preferred_lft forever\n#2: ens33: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group #default qlen 1000\n#   link/ether 00:0c:29:a5:2b:48 brd ff:ff:ff:ff:ff:ff\n#    altname enp2s1\n#    inet 172.30.0.203/24 brd 172.30.0.255 scope global ens33\n#       valid_lft forever preferred_lft forever\n### !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n### Строка ниже показывает присвоение виртуального IP-адреса\n### vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\n#    inet 172.30.0.210/32 scope global ens33\n#       valid_lft forever preferred_lft forever\n#    inet6 fe80::20c:29ff:fea5:2b48/64 scope link\n#    valid_lft forever preferred_lft forever\n# ….\n\r\n\n\n                    \n\r\n\n\r\nДля проверки открытия сетевых сокетов воспользуемся следующей командой:\n\r\n\n\n                        \nСкрытый текст\n\n                        \nss -lt\n\n## Ожидаемый результат:\n# State   Recv-Q  Send-Q   Local Address:Port    Peer Address:Port Process\n# LISTEN  0       4096         127.0.0.1:10248        0.0.0.0:*\n# LISTEN  0       4096         127.0.0.1:10249        0.0.0.0:*\n# LISTEN  0       4096      172.30.0.202:2379         0.0.0.0:*\n# LISTEN  0       4096         127.0.0.1:2379         0.0.0.0:*\n# LISTEN  0       4096      172.30.0.202:2380         0.0.0.0:*\n# LISTEN  0       4096         127.0.0.1:2381         0.0.0.0:*\n# LISTEN  0       4096         127.0.0.1:10257        0.0.0.0:*\n# LISTEN  0       4096         127.0.0.1:10259        0.0.0.0:*\n# LISTEN  0       128            0.0.0.0:ssh          0.0.0.0:*\n# LISTEN  0       4096           0.0.0.0:8888         0.0.0.0:*\n# LISTEN  0       4096         127.0.0.1:35099        0.0.0.0:*\n# LISTEN  0       4096                 *:10250              *:*\n# LISTEN  0       4096                 *:6443               *:*\n# LISTEN  0       4096                 *:10256              *:*\n# LISTEN  0       128               [::]:ssh             [::]:*\n\r\n\n\n                    \n\r\n\n\r\nДля фактической проверки доступности API по виртуальному IP-адресу можно воспользоваться командой:\n\r\n\n\n                        \nСкрытый текст\n\n                        \ncurl --silent --max-time 2 --insecure https://172.30.0.210:8888/\n\n## Ожидаемый результат:\n# {\n#   \"kind\": \"Status\",\n#   \"apiVersion\": \"v1\",\n#   \"metadata\": {},\n#   \"status\": \"Failure\",\n#   \"message\": \"forbidden: User \\\"system:anonymous\\\" cannot get path \\\"/\\\"\",\n#   \"reason\": \"Forbidden\",\n#   \"details\": {},\n#   \"code\": 403\n# }\n\r\n\n\n                    \n\r\n\n\r\n\nX. Заключение\n\r\nKubernetes — большая и довольно сложная тема, но маленький первый шаг на пути его покорения вы уже сделали — у вас появился работающий стенд для экспериментов. Дело осталось за малым — продолжать учиться. В этом вам могут помочь следующие статьи:\n\r\n\n\r\n\nОсновы Kubernetes\n\r\n\nJust-in-Time Kubernetes: Руководство начинающим для понимания основных концепций Kubernetes\n\r\n\nРазличия между Docker, containerd, CRI-O и runc\n\r\n\nВизуальное руководство по диагностике неисправностей в Kubernetes\n\r\n\nЗаписки о containerd\n\r\n\nЗачем нужен контейнер pause в Kubernetes\n\r\n\nКак я клонировал Томми Версетти, или запускаем GUI/GPU приложения в Kubernetes\n\r\n\nОтказоустойчивый кластер с балансировкой нагрузки с помощью keepalived\n\r\n\n \n ",
    "tags": [
        "kubernetes",
        "keepalived",
        "haproxy",
        "cri-o",
        "docker",
        "cri-dockerd",
        "containerd",
        "debian",
        "crictl"
    ]
}