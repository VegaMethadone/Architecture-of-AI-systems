{
    "article_id": "727484",
    "article_name": "Распознаем автомобильные номера на TorchServe",
    "content": "Вокруг так много фреймворков для инференса нейронок, что глаза разбегаются. Продолжаем цикл о реализации сервинга одной задачи, но разными инструментами. В прошлый раз \nреализация\n была на Nvidia Triton Inference Serve (за анонсами прошу в мой \nтелеграм канал\n. Код к статье находится в \nрепозитории\n.\nЗадача\nВ качестве задачи взято распознавание российских автомобильных номеров. Модели были взяты из этого \nрепозитория\n.\nПайплайн распознавания следующий:\n 1. Детекция номеров с помощью Yolov5;\n 2. Вырезанные номера прогоняются через Spatial transformer (STN) для выравнивания;\n 3. Текст номера распознается с LPR-net.\nФреймворк\nДля инференса используется [TorchServe]. Данный фреймворк является частью экосистемы Pytorch. Он активно развивается.\nВ документации о нем говорится следующее:\nTorchServe is a performant, flexible and easy to use tool for serving PyTorch eager mode and torschripted models.\nВозможности:\nПоддержка \nнескольких форматов\n моделей (torchscript, onnx, ipex, tensorrt);\nОбъединение нескольких моделей в один \nграф/workflow\n;\nИнференс \nAPI\n (REST и GRPC);\nAPI\n для управления моделями;\nМетрики\n из коробки.\nКонвертируем модели\nКак и Triton, TorchServe требует от пользователя перевести модели в свой формат. Для этого есть утилиты \ntorch-model-archiver\n и \ntorch-workflow-archiver\n для моделей и графов соответственно.\nДля конвертации нам нужно:\nМодель в формате TorchServe/Onnx/др.;\nСкрипт, описывающий пайплайн работы модели.\nТакой скрипт называется handler. В нем \nопределяются\n основные этапы жизненного цикла модели (инициализация, предобработка, предсказание, постобработка и др.). Для типовых задач они уже \nпредопределены\n.\nМодели STN и LPR легко \nконверуются\n в TorchServe, поэтому в их хэндлерах не используются дополнительные библиотеки. \nИмпорты\n выглядят так:\nimport json\nimport logging\nfrom abc import ABC\nimport numpy as np\nimport torch\nfrom ts.torch_handler.base_handler import BaseHandler\n\nYolo нельзя было просто перевести в TorchScript, так как часть логики для обработки запросов оставалась снаружи модели. Так как копаться с этим желания не было, а также ради более приближенного к жизни сценария, в хэндлере модели Yolo инициализируется из \nTorchHub\n. В импортах мы уже видим и сторонние модули:\nfrom inference_torchserve.data_models import PlatePrediction\nfrom nn.inference.predictor import prepare_detection_input, prepare_recognition_input\nfrom nn.models.yolo import load_yolo\nfrom nn.settings import settings\n\nЧтобы это работало, необходимо в докерфайле \nустановить\n в глобальный интерпретатор необходимые вам пакеты.\nВ TorchServe не нужно жестко задавать тип и размерность входов и выходов модели, поэтому никакие конфиги для моделей определять не нужно. С одной стороны это удобно, а с другой - порождает хаос, если не следовать какому-то одному формату.\nКонвертированная модель представляет собой zip архив с расширением \n.mar\n, в котором лежат все артефакты (служебная информация, веса, скрипты и дополнительные файлы).\n.\n├── MAR-INF\n│   └── MANIFEST.json\n├── stn.pt\n└── stn.py\n\nНа мой взгляд, решение с архивом неудобно для разработки. После любого изменения необходимо заново конвертировать модель. Также я испытывал проблемы при запуске в нем удаленного дебагера.\nЧтобы TorchServe загрузил модели, их нужно положить в одну папку - \nmodel storage\n и \nуказать\n путь до нее в параметрах. Чтобы при запуске поднимались все модели, необходимо указать \n--models all\n.\nДелаем пайплайн распознавания\nВыбранный пайплайн распознавания номера автомобиля состоит из последовательного предсказания несколькими моделями. Для этого в TorchServe есть \nWorkflow\n. Он позволяет задать как последовательный, так и параллельный граф обработки:\n# последовательный \ndag:\n  pre_processing : [m1]\n  m1 : [m2]\n  m2 : [postprocessing]\n\ninput -> function1 -> model1 -> model2 -> function2 -> output\n\n# параллельный граф\ndag:\n  pre_processing: [model1, model2]\n  model1: [aggregate_func]\n  model2: [aggregate_func]\n\n                          model1\n                         /       \\\ninput -> preprocessing ->         -> aggregate_func\n                         \\       /\n                          model2\n\nДля рассматриваемой задачи \nполучился\n следующий последовательно-параллельный \nграф\n. Узел aggregate объединяет координаты номеров с распознанными текстами.\n    ┌──────┐\n    │ YOLO ├─────┐\n    └──┬───┘     │\n       │         v\n       │      ┌─────┐\nplate  │      │ STN │\ncoords │      └──┬──┘\n       │         │\n       │         v\n       │      ┌──────┐\n       │      │LPRNET│\n       │      └──┬───┘\n       v         │\n   ┌─────────┐   │ plate\n   │aggregate│<──┘ texts\n   └─────────┘\n\nДля удобства и простоты данные между моделями \nпередаются\n в виде словарей. Сериализация таких данных в TorchServe весьма \nнеэффективна\n (переводят в строку и добавляют переносов строк), поэтому старайтесь передавать их как тензоры или байты.\nУчтите, что workflow нельзя стартовать автоматически при запуске сервера - необходимо явно послать запрос на это. Если очень хочется делать при поднятии сервера, то можно \nтак\n.\ncurl -X POST http://localhost:8081/workflows?url=plate_recognition\n\nИспользование моделей\nМодели определены. Сервер запущен.\nЧтобы выполнить определенную ранее модель или workflow нужно послать в TorchServe запрос на использование plate_recognition (я пользовался \nREST\n, но есть еще и \nGRPC\n). Для моделей используется эндпоинт \npredictions\n, а для workflow \nwfpredict\n.\nresponse = requests.post(\n    \"http://localhost:8080/predictions/yolo\", data=image.open(\"rb\").read()\n)\n\nresponse = requests.post(\n    \"http://localhost:8080/wfpredict/plate_recognition\", \n    data=image.open(\"rb\").read()\n)\n\nЗаключение\nНу вот инференс и написан. Данный пример не слишком простой, чтобы быть в целом бесполезным, но и не слишком сложный, чтобы покрыть все фишки этого фреймоврка для инференса.\nВ этом туториале были раскрыты не все возможности TorchServe, поэтому советую посмотреть в документации про:\nПолучение \nexplanations\n;\nСнятие\n метрик работающего сервера;\nБатчевание\n.\nПодписывайтесь на мой \nканал\n - там я рассказываю про нейронки с упором в сервинг.\n \n ",
    "tags": [
        "torchserve",
        "triton",
        "инференс",
        "сервинг",
        "нейронные сети",
        "распознавание номеров",
        "plate recognition"
    ]
}