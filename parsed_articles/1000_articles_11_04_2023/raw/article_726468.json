{
    "article_id": "726468",
    "article_name": "–ù–∞—á–∏–Ω–∞–µ–º —Ä–∞–±–æ—Ç—É —Å PyTorch 2.0 –∏ Hugging Face Transformers",
    "content": "–í —ç—Ç–æ–º –ø–æ—Å—Ç–µ —Ä–∞–∑–±–µ—Ä–µ–º —Ä–∞–±–æ—Ç—É —Å PyTorch 2.0 –∏ Hugging Face Transformers –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ fine-tune –º–æ–¥–µ–ª–∏ BERT –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞.\nPyTorch 2.0\n –ª—É—á—à–µ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ä–∞–±–æ—Ç—ã, –±–æ–ª–µ–µ —É–¥–æ–±–Ω—ã–π –¥–ª—è Python, –Ω–æ –ø—Ä–∏ —ç—Ç–æ–º –æ—Å—Ç–∞–µ—Ç—Å—è —Ç–∞–∫–∏–º –∂–µ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–º, –∫–∞–∫ –∏ —Ä–∞–Ω–µ–µ.\n–†–∞–∑–±–µ—Ä–µ–º —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:\n–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch 2.0.\n–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞.\nFine-tune –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ BERT —Å –ø–æ–º–æ—â—å—é Hugging Face Trainer.\n–ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏.\n–ö—Ä–∞—Ç–∫–æ–µ –≤–≤–µ–¥–µ–Ω–∏–µ: PyTorch 2.0 \nPyTorch 2.0, –∏–ª–∏, —Ç–æ—á–Ω–µ–µ, 1.14, –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ–±—Ä–∞—Ç–Ω–æ —Å–æ–≤–º–µ—Å—Ç–∏–º —Å¬†–ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –≤–µ—Ä—Å–∏—è–º–∏. –û–Ω –Ω–µ¬†–ø–æ—Ç—Ä–µ–±—É–µ—Ç –∫–∞–∫–∏—Ö‚Äë–ª–∏–±–æ –∏–∑–º–µ–Ω–µ–Ω–∏–π –≤¬†—Å—É—â–µ—Å—Ç–≤—É—é—â–µ–º –∫–æ–¥–µ PyTorch, –Ω–æ¬†–º–æ–∂–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–¥, –µ—Å–ª–∏ –¥–æ–±–∞–≤–∏—Ç—å \nmodel = torch.compile(model)\n. –ö–æ–º–∞–Ω–¥–∞ PyTorch —Ç–∞–∫ –æ–±—ä—è—Å–Ω—è–µ—Ç –ø–æ—è–≤–ª–µ–Ω–∏–µ –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ –≤¬†—Å–≤–æ–µ–º \nFAQ\n: \n¬´–ú—ã –≤—ã–ø—É—Å—Ç–∏–ª–∏ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ, –Ω–∞¬†–Ω–∞—à –≤–∑–≥–ª—è–¥, –º–µ–Ω—è—é—Ç —Ç–æ, –∫–∞–∫¬†–≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ PyTorch, –ø–æ—ç—Ç–æ–º—É –º—ã –Ω–∞–∑–≤–∞–ª–∏ —ç—Ç–æ 2.0¬†–≤–º–µ—Å—Ç–æ 1.14.¬ª\n–°—Ä–µ–¥–∏ —ç—Ç–∏—Ö –Ω–æ–≤—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π: –ø–æ–ª–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ TorchDynamo, AOTAutograd, PrimTorch –∏ TorchInductor. –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç PyTorch 2.0 –¥–æ—Å—Ç–∏–≥–Ω—É—Ç—å —É—Å–∫–æ—Ä–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è –≤ 1,3-2 —Ä–∞–∑–∞ \n–Ω–∞ –±–æ–ª–µ–µ 40 –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞—Ö –º–æ–¥–µ–ª–µ–π\n –æ—Ç \nHuggingFace Transformers\n. –ü–æ–¥—Ä–æ–±–Ω–µ–µ –æ PyTorch 2.0 –º–æ–∂–Ω–æ —É–∑–Ω–∞—Ç—å –Ω–∞ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º \n\"GET STARTED\"\n.\n–ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –≠—Ç–æ—Ç —Ç—É—Ç–æ—Ä–∏–∞–ª –±—ã–ª —Å–æ–∑–¥–∞–Ω –∏ –∑–∞–ø—É—â–µ–Ω –Ω–∞ –∏–Ω—Å—Ç–∞–Ω—Å–µ AWS EC2 g5.xlarge, –≤–∫–ª—é—á–∞—è GPU NVIDIA A10G.\n1. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch 2.0 \n–ü–µ—Ä–≤—ã–π —à–∞–≥ - —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å PyTorch 2.0 –∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –æ—Ç Hugging Face, \ntransformers\n –∏ \ndatasets\n.\n# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ PyTorch 2.0 —Å cuda 11.7\n!pip install \"torch>=2.0\" --extra-index-url https://download.pytorch.org/whl/cu117 --upgrade --quiet \n–¢–∞–∫–∂–µ —Å—Ç–∞–≤–∏–º –ø–æ—Å–ª–µ–¥–Ω—é—é –≤–µ—Ä—Å–∏—é \ntransformers\n , –∫–æ—Ç–æ—Ä–∞—è –≤–∫–ª—é—á–∞–µ—Ç –Ω–∞—Ç–∏–≤–Ω—É—é –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é PyTorch 2.0 –≤ \nTrainer\n.\n# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ transformers –∏ dataset\n!pip install \"transformers==4.27.1\" \"datasets==2.9.0\" \"accelerate==0.17.1\" \"evaluate==0.4.0\" tensorboard scikit-learn\n# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ git-lfs –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ –∏ –ª–æ–≥–æ–≤ –≤ hugging face hub\n!sudo apt-get install git-lfs --yes\n–í —ç—Ç–æ–º –ø—Ä–∏–º–µ—Ä–µ –¥–ª—è –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å \nHugging Face Hub\n. –ß—Ç–æ–±—ã –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ Hub, –≤–Ω–∞—á–∞–ª–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ \nHugging Face\n. –î–ª—è –≤—Ö–æ–¥–∞ –≤ —Å–≤–æ—é —É—á–µ—Ç–Ω—É—é –∑–∞–ø–∏—Å—å –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ç–æ–∫–µ–Ω–∞ (–∫–ª—é—á–∞ –¥–æ—Å—Ç—É–ø–∞) –Ω–∞ –¥–∏—Å–∫–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º \nlogin\n –∏–∑ –ø–∞–∫–µ—Ç–∞ \nhuggingface_hub\n.\nfrom huggingface_hub import login\n\nlogin(\n  token=\"\", # ADD YOUR TOKEN HERE\n  add_to_git_credential=True\n)\n2. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ \n–ë—É–¥–µ–º –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ \nBANKING77\n. –î–∞—Ç–∞—Å–µ—Ç BANKING77 —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –æ–±—Ä–∞—â–µ–Ω–∏—è –æ—Ç –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏–∑ –æ–±–ª–∞—Å—Ç–∏ –±–∞–Ω–∫–æ–≤—Å–∫–æ–≥–æ/—Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–≥–æ —Å–µ–∫—Ç–æ—Ä–∞. –û–Ω —Å–æ—Å—Ç–æ–∏—Ç –∏–∑ 13 083 –æ–±—Ä–∞—â–µ–Ω–∏–π, —Ä–∞–∑–º–µ—á–µ–Ω–Ω—ã—Ö –Ω–∞ 77 –∏–Ω—Ç–µ–Ω—Ç–æ–≤ (–∫–ª–∞—Å—Å–æ–≤). \n–î–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ BANKING77 –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ—Ç–æ–¥ \nload_dataset()\n –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ \nü§ó Datasets\n.\nfrom datasets import load_dataset\n\n# Dataset id from huggingface.co/dataset\ndataset_id = \"banking77\"\n\n# Load raw dataset\nraw_dataset = load_dataset(dataset_id)\n\nprint(f\"Train dataset size: {len(raw_dataset['train'])}\")\nprint(f\"Test dataset size: {len(raw_dataset['test'])}\")\n–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –ø—Ä–∏–º–µ—Ä –∏–∑ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö.\nfrom random import randrange\n\nrandom_id = randrange(len(raw_dataset['train']))\nraw_dataset['train'][random_id]\n# {'text': \"I can't get google pay to work right.\", 'label': 2}\n–î–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –Ω—É–∂–Ω–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –≤ —Ç–æ–∫–µ–Ω—ã. –≠—Ç–æ –¥–µ–ª–∞–µ—Ç—Å—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–º. –¢–∞–∫–∂–µ –æ–Ω –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Ç–æ–∫–µ–Ω—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –∏–º –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –≤ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–º —Å–ª–æ–≤–∞—Ä–µ. –ü–æ–¥—Ä–æ–±–Ω–µ–µ –æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ –º–æ–∂–Ω–æ —É–∑–Ω–∞—Ç—å –≤ \n–≥–ª–∞–≤–µ 6\n –æ—Ç¬†\nHugging Face Course\n.\nfrom transformers import AutoTokenizer\n\n# Model id to load the tokenizer\nmodel_id = \"bert-base-uncased\"\n# Load Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# Tokenize helper function\ndef tokenize(batch):\n    return tokenizer(batch['text'], padding='max_length', truncation=True, return_tensors=\"pt\")\n\n# Tokenize dataset\nraw_dataset =  raw_dataset.rename_column(\"label\", \"labels\") # to match Trainer\ntokenized_dataset = raw_dataset.map(tokenize, batched=True,remove_columns=[\"text\"])\n\nprint(tokenized_dataset[\"train\"].features.keys())\n# dict_keys(['input_ids', 'token_type_ids', 'attention_mask','lable'])\n3. Fine-tune –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ BERT —Å –ø–æ–º–æ—â—å—é Hugging Face Trainer\n–ü–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –º–æ–∂–Ω–æ –Ω–∞—á–∏–Ω–∞—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏. –ú—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å \nbert-base-uncased\n. –ü–µ—Ä–≤—ã–º —à–∞–≥–æ–º –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –ø–æ–º–æ—â—å—é –∫–ª–∞—Å—Å–∞ \nAutoModelForSequenceClassification\n –∏–∑ Hugging Face Hub. –¢–∞–∫ –º—ã —Å–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å —Å –≤–µ—Å–∞–º–∏ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ BERT, –Ω–æ —Å \"–≥–æ–ª–æ–≤–æ–π\" —Å–≤–µ—Ä—Ö—É —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥ –Ω–∞—à—É –∑–∞–¥–∞—á—É –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –ó–¥–µ—Å—å –º—ã –ø–µ—Ä–µ–¥–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤ (77) –∏–∑ –Ω–∞—à–µ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –∏–º–µ–Ω–∞ –º–µ—Ç–æ–∫, —á—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –±–æ–ª–µ–µ —á–∏—Ç–∞–±–µ–ª—å–Ω—ã–º.\nfrom transformers import AutoModelForSequenceClassification\n\n# Model id to load the tokenizer\nmodel_id = \"bert-base-uncased\"\n\n# Prepare model labels - useful for inference\nlabels = tokenized_dataset[\"train\"].features[\"labels\"].names\nnum_labels = len(labels)\nlabel2id, id2label = dict(), dict()\nfor i, label in enumerate(labels):\n    label2id[label] = str(i)\n    id2label[str(i)] = label\n\n# Download the model from huggingface.co/models\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_id, num_labels=num_labels, label2id=label2id, id2label=id2label\n)\n–ë—É–¥–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è. \nTrainer\n –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ—Ü–µ–Ω–∫—É –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è, –¥–ª—è —ç—Ç–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–∏–º \ncompute_metrics\n. –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–∏–±–ª–∏–æ—Ç–µ–∫—É \nevaluate\n –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ \n–º–µ—Ç—Ä–∏–∫–∏ f1\n –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö.\nimport evaluate\nimport numpy as np\n\n# Metric Id\nmetric = evaluate.load(\"f1\")\n\n# Metric helper method\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    return metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n–ü–æ—Å–ª–µ–¥–Ω–∏–π —à–∞–≥ - –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã \nTrainingArguments\n –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ó–¥–µ—Å—å –∂–µ –¥–æ–±–∞–≤–∏–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ—Ç PyTorch 2.0 –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è. –ß—Ç–æ–±—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏—è PyTorch 2.0 –ø–µ—Ä–µ–¥–∞–µ–º –æ–ø—Ü–∏—é \ntorch_compile\n –≤ \nTrainingArguments\n.\n–¢–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é \nTrainer\n —Å Hugging Face Hub –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–∏, –ª–æ–≥–æ–≤ –∏ –º–µ—Ç—Ä–∏–∫ –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –≤–æ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.\nfrom huggingface_hub import HfFolder\nfrom transformers import Trainer, TrainingArguments\n\n# Id for remote repository\nrepository_id = \"bert-base-banking77-pt2\"\n\n# Define training args\ntraining_args = TrainingArguments(\n    output_dir=repository_id,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=8,\n    learning_rate=5e-5,\n\tnum_train_epochs=3,\n\t# PyTorch 2.0 specifics\n    bf16=True, # bfloat16 training\n\ttorch_compile=True, # optimizations\n    optim=\"adamw_torch_fused\", # improved optimizer\n    # logging & evaluation strategies\n    logging_dir=f\"{repository_id}/logs\",\n    logging_strategy=\"steps\",\n    logging_steps=200,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"f1\",\n    # push to hub parameters\n    report_to=\"tensorboard\",\n    push_to_hub=True,\n    hub_strategy=\"every_save\",\n    hub_model_id=repository_id,\n    hub_token=HfFolder.get_token(),\n\n)\n\n# Create a Trainer instance\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"test\"],\n    compute_metrics=compute_metrics,\n)\n–î–ª—è –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–µ–º –º–µ—Ç–æ–¥ \ntrain\n –æ—Ç \nTrainer\n.\n# Start training\ntrainer.train()\n–î–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –æ–±—É—á–µ–Ω–∏—è –º—ã —Ç–∞–∫–∂–µ –∑–∞–ø—É—Å—Ç–∏–ª–∏ –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ –æ–ø—Ü–∏–∏ \ntorch_compile\n:\n{'train_runtime': 696.2701, 'train_samples_per_second': 43.1, 'eval_f1': 0.928788}\n–ò—Å–ø–æ–ª—å–∑—É—è –æ–ø—Ü–∏—é \ntorch_compile\n –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é \nadamw_torch_fused\n, \n–≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è –Ω–∞\n \n52.5%\n –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –æ–±—É—á–µ–Ω–∏–µ–º –±–µ–∑ PyTorch 2.0:\n{'train_runtime': 457.7964, 'train_samples_per_second': 65.55, 'eval_f1': 0.931773}\n–¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è —Å–Ω–∏–∑–∏–ª–∏—Å—å —Å 696 –¥–æ 457 —Å–µ–∫—É–Ω–¥. –ó–Ω–∞—á–µ–Ω–∏–µ \ntrain_samples_per_second\n –≤—ã—Ä–æ—Å–ª–æ —Å 43 –¥–æ 65. –ó–Ω–∞—á–µ–Ω–∏–µ f1-–º–µ—Ç—Ä–∏–∫–∏ —Ç–∞–∫–æ–µ –∂–µ –∏–ª–∏ —á—É—Ç—å –ª—É—á—à–µ, —á–µ–º –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –±–µ–∑ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è \ntorch_compile\n.\nPyTorch 2.0 –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–æ –º–æ—â–µ–Ω! üöÄ\n–°–æ—Ö—Ä–∞–Ω–∏–º –Ω–∞—à–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –≤ Hugging Face Hub –∏ —Å–æ–∑–¥–∞–¥–∏–º –∫–∞—Ä—Ç–æ—á–∫—É –º–æ–¥–µ–ª–∏.\n# Save processor and create model card\ntokenizer.save_pretrained(repository_id)\ntrainer.create_model_card()\ntrainer.push_to_hub()\n4. –ó–∞–ø—É—Å–∫ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏\n–ü–æ—Å–º–æ—Ç—Ä–∏–º –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ–∫—Å—Ç–æ–≤–æ–º –ø—Ä–∏–º–µ—Ä–µ. –§–∏–Ω–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –ø–æ–ª—É—á–∞–µ–º —Å –ø–æ–º–æ—â—å—é \npipeline\n –∏–∑ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ \ntransformers\n.\nfrom transformers import pipeline\n\n# load model from huggingface.co/models using our repository id\nclassifier = pipeline(\"sentiment-analysis\", model=repository_id, tokenizer=repository_id, device=0)\n\nsample = \"I have been waiting longer than expected for my bank card, could you provide information on when it will arrive?\"\n\n\npred = classifier(sample)\nprint(pred)\n# [{'label': 'card_arrival', 'score': 0.9903606176376343}]\n–ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n–í¬†—ç—Ç–æ–º –ø–æ—Å—Ç–µ –º—ã —Ä–∞–∑–æ–±—Ä–∞–ª–∏—Å—å, –∫–∞–∫¬†–∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å PyTorch 2.0¬†–¥–ª—è¬†–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞ –Ω–∞¬†–Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö BANKING77. \nPyTorch 2.0¬†‚Äî —ç—Ç–æ –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç, –ø–æ–∑–≤–æ–ª—è—é—â–∏–π —É—Å–∫–æ—Ä–∏—Ç—å –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è.\n –í¬†–Ω–∞—à–µ–º –ø—Ä–∏–º–µ—Ä–µ, –∑–∞–ø—É—â–µ–Ω–Ω–æ–º –Ω–∞¬†NVIDIA A10G, –º—ã —Å–º–æ–≥–ª–∏ –¥–æ—Å—Ç–∏—á—å –Ω–∞ 52.5% –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏. \n–ö—Ä–æ–º–µ —Ç–æ–≥–æ, –º—ã —É–≤–∏–¥–µ–ª–∏, –∫–∞–∫ \n–Ω–µ—Å–ª–æ–∂–Ω–æ –¥–æ–æ–±—É—á–∏—Ç—å BERT –ø–æ–¥¬†—Å–≤–æ—é –∑–∞–¥–∞—á—É —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ Hugging Face –∏ PyTorch\n. \n–ú–æ–π —Ç–µ–ª–µ–≥—Ä–∞–º‚Äë–∫–∞–Ω–∞–ª\n –æ¬†DS –∏ –Ω–µ¬†—Ç–æ–ª—å–∫–æ.\n \n ",
    "tags": [
        "pytorch",
        "transformers",
        "huggingface",
        "machine learning",
        "bert",
        "nlp",
        "data science",
        "neural networks"
    ]
}