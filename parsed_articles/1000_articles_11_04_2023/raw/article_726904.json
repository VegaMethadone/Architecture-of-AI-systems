{
    "article_id": "726904",
    "article_name": "Фортран: пишем параллельные программы для суперкомпьютера",
    "content": "В \nпервой части статьи\n мы рассмотрели написание на современном Фортране простой программы, реализующей клеточный автомат \"Жизнь\", в виде классического последовательного кода (SISD), матричных операций (SIMD) и параллельных конструкций SMP (SIMD с частью функций MIMD). Сейчас мы будем рассматривать использование конструкций Фортрана для программирования массивно-параллельных архитектур (MPP), к которым, в частности, относятся современные суперкомпьютеры. Такие архитектуры реализуют классическую схему MIMD. \nКак и в прошлой статье, мы используем для иллюстрации материала компиляторы GNU Fortran и Intel Fortran.\nПостановка задачи\nКак и в первой части статьи, мы продолжим реализовывать тот же самый клеточный автомат с теми же самыми входными и выходными данными. К сожалению, компилятор Intel Fortran не поддерживает используемую для программирования MPP архитектуру Coarray под операционной системой macOS, поэтому примеры для Intel Fortran мы в этот раз будем выполнять на другой машине с Linux. Это делает невозможным сравнение результатов в абсолютных цифрах, но по относительным оценкам всё по-прежнему будет ясно.\nДля GNU Fortran, как и в первой части статьи, будет ипользоваться компилятор версии 12.2.0 и библиотека \nOpenCoarrays\n \nна компьютере Mac mini с 4-ядерным процессором Intel Core i3 @ 3.6 GHz под управлением macOS. Для Intel Fortran будут использоваться компилятор ifort версии  2021.9.0 20230302 и компилятор ifx 2023.1.0 20230320 на компьютере IBM x3250 M4 с 8-ядерным процессором Xeon 4C E3-1270v2 @ 3.9 GHz под управлением Linux. Ядер в этом процессоре больше, чем в Маке, но он постарее и потормознее.\n0-4. Повторение темы\nНапомним, что в прошлый раз мы достигли следующих результатов на Маке.\nПоследовательная программа в режиме автопараллелизации, gfortran и ifort:\n$ ./life_seq_g\n          11 сек,    124773000 ячеек/с\n$ ./life_seq\n           4 сек,    338120000 ячеек/с\nПараллельная SMP программа средствами OpenMP, лучший результат gfortran:\n$ ./life_omp_g\n3 сек, 377022000 ячеек/с\n$ ./life_omp\n3 сек, 356690000 ячеек/с\nПараллельная SMP программа средствами \nDO CONCURRENT\n, лучший результат ifort:\n$ ./life_con\n3 сек, 355890000 ячеек/с\nПовторим эти результаты на нашем сервере Linux, чтобы можно было в дальнейшем опираться на какие-то сопоставимые значения (с автопараллелизацией ifort; ifx её не поддерживает):\n$ ./life_seq_ifort\n           9 сек,    144360000 ячеек/с\n$ ./life_seq_ifx\n          19 сек,     71030000 ячеек/с\n$ ./life_con_ifort\n           8 сек, \n     \n162910000 ячеек/с\n$ ./life_con_ifx \n          37 сек,     37730000 ячеек/с\nЭто в целом совпадает с результатами, приведёнными \n@mobi\n в комментариях к первой части статьи.\nПрежде чем перейти к изложению нового материала, вспомним логику работы нашей программы в архитектуре SMP. \nМы написали классический последовательный алгоритм, а затем применили к нему некоторые небольшие подсказки компилятору о параллелизации, позволившие распараллелить код на несколько SMP ядер, сохраняя при этом его логическую эквивалентность с последовательным алгоритмом. Это очень важное обстоятельство. Вообще в SMP парадигме можно в параллельных процессах делать совершенно разные вещи, но в рассмотренной нами логике мы просто ускоряли последовательный процесс, имеющий один вход и один выход, методом его логически эквивалентных преобразований в частично параллельный.\nС другой стороны, архитектура SMP предопределила используемую нами модель оперативной памяти. Все объекты в памяти разделялись между всеми параллельными процессами, одинаково принадлежа им всем, за исключением некоторых отдельных локальных для процесса переменных, которые мы специально описывали как \nLOCAL\n или \nOMP PRIVATE\n.\n5. MPP параллелизм через комассивы\nМодель массивно-параллельных вычислений совершенно другая. Она просто излагается, но не всегда легко укладывается в голове для практического применения, поэтому тут нужно некоторое внимание.\nДля начала, давайте разберёмся, чем вообще плохо SMP? Не особо парясь, парой подсказок компилятору мы можем ускорять свои последовательные программы в несколько раз – казалось бы, хватило бы только ядер, как пирату в мультфильме.\n К сожалению, пулемётную ленту из ядер мы организовать не можем, так как очень быстро узким местом становится оперативная память. Разделение доступа к памяти между несколькими арифметико-логическими устройствами сложно чисто технически, и даже если мы это всё нужным образом припаяем и электрически нагрузим, то всё равно наши ядра будут конкурировать за доступ к одним и тем же страницам памяти (она же разделяемая) на более высоком уровне. Поэтому больше десятка-другого ядер в архитектуре SMP эффективно не подключить.\nЧто же нам делать, если мы хотим построить суперкомпьютер, включающий десятки тысяч вычислительных ядер (то есть собственно массивно-параллельную архитектуру)? В таком случае используются вычислительные узлы, каждый из которых содержит собственную оперативную память. Узлы объединены специальной высокоскоростной вычислительной сетью, позволяющей синхронизировать отдельные области памяти при помощи сообщений между узлами. Сам узел при этом чаще всего внутри себя, то есть на более низком уровне иерархии, использует несколько ядер, объединённых архитектурой SMP. Архитектура MPP не обязательно ограничивается двумя уровнями иерархии, как тут упрощённо описано; уровней может быть и больше (например, стойки суперкомпьютера взаимодействуют между собой с меньшей скоростью, чем узлы внутри стойки).\nПрограммная архитектура MPP систем восходит к транспьютерам и ранним разработкам массивно-параллельного кода на языке Оккам. \nСреда выполнения организована таким образом, что программа запускается одновременно в нескольких экземплярах, по одному экземпляру на узел (или ядро узла, что пока что для нас логически неважно). Текст и машинный код программы при этом на каждом используемом узле один и тот же, но динамика выполнения этого кода разная. \nТак как код одинаков, то получается, что динамика его выполнения должна зависеть от каких-то внешних обстоятельств. И самым главным из таких обстоятельств является просто-напросто номер узла. Этот номер узла передаётся программе системной утилитой, запускающей массивно-параллельный код на массиве узлов. В Фортране номер узла, на котором в данный момент выполняется код, выдаётся функцией \nTHIS_IMAGE()\n и всегда принимает значения от 1 до общего количества выполняющих программу узлов, которое, в свою очередь, выдаётся функцией \nNUM_IMAGES()\n.\nКак ясно из вышесказанного, каждый параллельно выполняющийся процесс в архитектуре MPP имеет свою собственную оперативную память. По этой причине все переменные, описанные в фортрановской программе, хранят собственные значения для каждого номера узла. В качестве исключения из этого правила, в коде могут быть специальным образом описаны комассивы, позволяющие программе получить доступ к оперативной памяти другого узла. Такой доступ к элементам комассивов имеет специальный синтаксис с квадратными скобками и не может применяться везде, где допустимо использование переменных в локальной оперативной памяти узла. Например, по очевидным причинам нельзя создать указатель на объект в чужой оперативной памяти.\nТеперь, как говорится, со всей этой фигнёй мы попытаемся взлететь. Ясно, что в такой модели уже одной-двумя подсказками компилятору не обойдёшься, надо менять логику кода.\nНаш общий подход будет типичным для этого стиля программирования. Игровое поле мы объявим комассивом, к которому потенциально имеют доступ все узлы. Но фактически каждый узел будет заниматься только своей частью поля. Для этого разделим все столбцы матрицы field на полосы по числу узлов. Каждый узел рассчитывает значения элементов массива в своей полосе и заполняет полосу массива в своей оперативной памяти. Кроме того, по краям полосы узла находятся две единичные полоски, интересные его соседям. Значения в этих полосках узел копирует в оперативную память своих соседей.\nВажно понять, что комассив field, как целое, вообще физически нигде не существует до момента окончательной сборки при записи результата в память. Он раскидан по своим отображениям на узлах, в которых каждый узел физически работает только со своей полосой из этого массива. По этой причине мы поменяли в программе также инициализацию матрицы, чтобы не занимать физическими нулями оперативную память из ненужных частей массива, которая фактически никогда не будет использоваться узлом.\nНапример, если у нас массив имеет 1002 столбца (1000 основных и 2 по краям для свёртки), а программа исполняется на 10 узлах, то узел номер 1 вычисляет столбцы 0:100, узел номер 2 – 101:200 и т.д. При этом столбец 100, рассчитанный узлом 1, помещается им не только в свою оперативную память, но и в память узла 2, так как этот столбец понадобится узлу 2 для расчёта столбца 101. Такие перекрывающиеся части комассивов называются “гало”.\nИтак, вот новый код:\nprogram life_mpp\n\n    implicit none\n\n    integer, parameter :: matrix_kind = 4\n    integer, parameter :: generations = 2\n    integer, parameter :: rows = 1000, cols = 1000\n    integer, parameter :: steps = 10000\n\n    integer (kind=matrix_kind) :: field (0:rows+1, 0:cols+1, generations) [*]\n    integer :: thisstep = 1, nextstep =2\n    integer :: i\n    integer :: clock_cnt1, clock_cnt2, clock_rate\n\n    integer, allocatable :: cols_lo (:), cols_hi (:) ! диапазоны столбцов для узлов\n\n    integer :: me ! номер текущего узла, чтобы обращаться покороче\n\n    me = this_image()\n\n    print *, \"it's me:\", me\n\n    ! заполним таблицы верхних и нижних границ полос для узлов\n\n    allocate (cols_lo (num_images()), cols_hi (0:num_images()))\n    cols_hi (0) = 0\n    do i = 1, num_images()\n      cols_lo (i) = cols_hi (i-1) + 1\n      cols_hi (i) = cols * i / num_images()\n    end do\n\n    ! проинициализируем матрицу (инициализация тоже изменилась)\n\n    call init_matrix (field (:, :, thisstep))\n\n    sync all\n\n    ! первый узел займётся хронометрированием\n\n    if (me == 1) call system_clock (count=clock_cnt1)\n\n    ! цикл выглядит как и раньше, но после каждого шага синхронизируемся, \n    ! чтобы не возникло разнобоя с сечениями источника и приёмника\n\n    do i = 1, steps\n      call process_step (field (:, :, thisstep), field (:, :, nextstep))\n      thisstep = nextstep\n      nextstep = 3 - thisstep\n      sync all\n    end do\n\n    ! первый узел заканчивает хронометрирование, печатает результат,\n    ! собирает матрицу и пишет в файл\n\n    if (me == 1) then\n\n      call system_clock (count=clock_cnt2, count_rate=clock_rate)\n      print *, (clock_cnt2-clock_cnt1)/clock_rate, 'сек, ', &\n        int(rows*cols,8)*steps/(clock_cnt2-clock_cnt1)*clock_rate, 'ячеек/с'\n\n      ! мы хотим вывести матрицу в файл, а разные её столбцы хранятся\n      ! на разных узлах. надо собрать всю матрицу на пишущем узле\n\n      do i = 2, num_images()\n        field (:, cols_lo(i):cols_hi(i), thisstep) = &\n          field (:, cols_lo(i):cols_hi(i), thisstep) [i]\n      end do\n\n      call output_matrix (field (:, :, thisstep))\n\n    end if\n\n    ! надо синхронизироваться в конце, а то освободится память\n    ! завершившегося процесса, пока её не скопировали\n  \n    sync all ! уходим всей бригадой\n\n    contains\n\n    pure subroutine init_matrix (m)\n\n      integer (kind=matrix_kind), intent (out) :: m (0:,0:)\n      integer j\n \n      ! обнулим поле в своей полосе и гало\n      ! не лезем далеко в чужие полосы, чтобы не распределять \n      ! ненужную узлу память\n\n      do j = cols_lo(me)-1, cols_hi(me)+1\n        m (:, j) = 0\n      end do\n\n      ! первый и последний узлы обнуляют кромки поля\n\n      if (me == 1)             m (:, 0) = 0\n      if (me == num_images())  m (:, cols+1) = 0\n\n      ! нарисуем \"мигалку\" на имеющих отношение к ней узлах\n\n      if (cols_lo(me) <= 51 .and. cols_hi(me) >= 49) m (50, 50) = 1\n      if (cols_lo(me) <= 52 .and. cols_hi(me) >= 50) m (50, 51) = 1\n      if (cols_lo(me) <= 53 .and. cols_hi(me) >= 51) m (50, 52) = 1\n\n    end subroutine init_matrix\n\n    ! gfortran считает эту подпрограмму impure из-за доступа в чужую память\n    ! ifort и ifx считают её pure\n\n    subroutine process_step (m1, m2)\n\n      integer (kind=matrix_kind), intent (in)  :: m1 (0:,0:) [*]\n      integer (kind=matrix_kind), intent (out) :: m2 (0:,0:) [*]\n      integer :: rows, cols\n      integer :: i, j, s\n\n      rows = size (m1, dim=1) - 2\n      cols = size (m1, dim=2) - 2\n\n        do j = cols_lo (me), cols_hi (me)\n        do i = 1, rows\n          s = m1 (i-1, j) + m1 (i+1, j) + m1 (i-1, j-1) + m1 (i+1, j-1) + m1 (i, j-1) + &\n                m1 (i-1, j+1) + m1 (i, j+1) + m1 (i+1, j+1)\n          select case (s)\n            case (3)\n              m2 (i, j) = 1\n            case (2)\n              m2 (i, j) = m1 (i, j)\n            case default\n              m2 (i, j) = 0\n          end select\n        end do\n      end do\n\n      ! обмениваемся гало с соседями снизу и сверху\n      if (me > 1) then\n        m2 (:, cols_lo (me)) [me-1] = m2 (:, cols_lo (me))\n      end if\n      if (me < num_images()) then\n        m2 (:, cols_hi (me)) [me+1] = m2 (:, cols_hi (me))\n      end if\n\n      ! заворачивание краёв здесь будет посложнее, так как выполняется разными узлами\n      ! в пределах своей компетенции\n\n      if (me == num_images()) m2 (:, 0) [1] = m2 (:, cols)\n      if (me == 1) m2 (:, cols+1) [num_images()] = m2 (:, 1)\n      m2 (0,cols_lo(me):cols_hi(me))       = m2 (rows, cols_lo(me):cols_hi(me))   \n      m2 (rows+1, cols_lo(me):cols_hi(me)) = m2 (1, cols_lo(me):cols_hi(me))\n\n    end subroutine process_step\n\n    subroutine output_matrix (m)\n\n      integer (kind=matrix_kind), intent (in) :: m (0:,0:)\n      integer :: rows, cols\n      integer :: i, j, n\n      integer :: outfile\n\n      rows = size (m, dim=1) - 2\n      cols = size (m, dim=2) - 2\n\n      open (file = 'life.txt', newunit=outfile)\n      do i = 1, rows\n        write (outfile, '(*(A1))') (char (ichar (' ') + &\n          m(i, j)*(ichar ('*') - ichar (' '))), j=1, cols)\n      end do\n      close (outfile)\n\n    end subroutine output_matrix\n\nend program life_mpp\nДля трансляции и запуска программ в mpp архитектуре зачастую используются специальные скрипты:\n$ caf life_mpp.f90 -o life_mpp -O3 -ftree-vectorize -fopt-info-vec -flto -fopenmp\n \n$ mpiexec -n 4 ./life_mpp\n it's me:           1\n it's me:           2\n it's me:           3\n it's me:           4\n           2 сек,    480267000 ячеек/с\nКак видим, gfortran ускорил нашу программу ещё на 30% по сравнению с лучшим результатом SMP, достигнув на 4 ядрах производительности 3.84 от последовательной версии. Очевидно, это связано с тем, что даже в SMP системе расшивка параллельных процессов по адресам памяти ускоряет работу с оперативной памятью.\nУ Intel Fortran:\n$ ifort life_mpp.f90 -o life_mpp_ifort -Ofast -coarray\n$ ifx life_mpp.f90 -o life_mpp_ifx -Ofast -coarray\n$ mpiexec ./life_mpp_ifort\n it's me:           1\n it's me:           2\n it's me:           3\n it's me:           4\n it's me:           5\n it's me:           6\n it's me:           7\n it's me:           8\n          12 сек,    112970000 ячеек/с\n$ mpiexec ./life_mpp_ifx\n it's me:           1\n it's me:           5\n it's me:           6\n it's me:           7\n it's me:           8\n it's me:           3\n it's me:           4\n it's me:           2\n          19 сек,     73470000 ячеек/с\nifort в полтора раза хуже, чем его же производительность SMP, про ifx нечего и говорить.\nДля полноты картины попробуем ещё забавный режим, запустив ifort с режимами одновременно -coarray и -parallel:\n$ mpiexec ./life_mpp_ifort\n it's me:           1\n it's me:           2\n it's me:           3\n it's me:           4\n it's me:           5\n it's me:           6\n it's me:           7\n it's me:           8\n          15 сек,     91120000 ячеек/с\nРезультат предсказуем, SMP и MPP параллелизации в данном случае мешают друг другу, и выполение программы замедляется. Если бы мы запускали комассивную программу на матрице узлов MPP, каждый из которых внутри был бы организован как SMP, тогда такой режим компиляции для ifort имел бы смысл.\nНадо отметить, что утилита создания MPP среды mpiexec из комплекта ifort (Intel(R) MPI Library, Version 2021.9  Build 20230307)\n \nработает в SMP режиме с ошибкой: вместо задания параметром количества узлов, она всегда создаёт узлы по количеству ядер, а в зависимости от значения параметра дублирует соответствующее число раз эти узлы с теми же самыми номерами, что, разумеется, недопустимо.\nВывод\nМы рассмотрели введение в программирование на Фортране для массивно-параллельных архитектур, а GNU Fortran опять показал своё превосходство над Intel Fortran при использовании языковых конструкций параллельного программирования.\nЗаметим, что, подняв вычислительный кластер с интерфейсом MPI в локальной сети, можно попробовать запускать подобные программы на вычислительных узлах сразу нескольких компьютеров с помощью той же самой утилиты mpiexec. Оставим это в качестве самостоятельного упражнения для читателей.\n \n ",
    "tags": [
        "программирование",
        "fortran",
        "фортран",
        "параллельное программирование"
    ]
}