{
    "article_id": "726122",
    "article_name": "ML-подходы по поиску похожих изображений",
    "content": "Привет, Хабр!\nМеня зовут Паймеров Владимир, я Data Scientist и участник \nпрофессионального сообщества NTA\n.\nКомпьютерное зрение\n (computer vision, CV) — активно развивающаяся научная область,\n связанная с анализом изображений и видео. В последнее время данному направлению\n уделяется большое внимание, так как CV позволяет решать множество задач, таких как\n детекцию объектов, классификацию изображений, распознавание лиц и т. д., которые\n в свою очередь применяются в разных сферах жизни от мобильных приложений для\n наложения масок на лицо во время звонка до построения систем безопасности,\n поиска преступников и мошенников. Задумка обрабатывать изображения для\n извлечения из них полезной информации возникла давно, однако возможности\n техники и технологий не позволяли это делать, так как при обработке изображений\n нужны большие объемы для хранения данных. Сейчас есть инструменты, позволяющие\n хранить большой объем данных и обрабатывать изображения, поэтому появилось\n множество инструментов для решения различных задач. Об одной из таких задач\n будет рассказано в данном посте.\nНавигация по материалу\nВведение\n.\nЗагрузка, обработка и работа с данными\nСвёрточные автоэнкодеры для извлечения признаков из изображения\nПостроение модели подобия изображений при помощи K-ближайших соседей (NearestNeighbours)\nИспользование предобученных моделей для извлечения признаков из изображения\nИспользование готовых библиотек для поиска похожих изображений\nВыводы\nВведение\nЕжедневно посетители интернета оставляют на разных сайтах и в социальных сетях свои персональные данные: e‑mail, имя, телефон, возраст, фотографии. Закон 152-ФЗ запрещает собирать, хранить и обрабатывать персональные данные человека без его согласия. \nНо владельцы некоторых сайтов, которые с полным основанием можно назвать мошенническими, игнорируют этот закон и не задумываются об ответственности.\nВ банковской сфере паспорта используются для идентификации клиентов. Мошенники могут подделывать паспорта, использовать старые недействительные паспорта для получения кредита, и проведения других операций, которые означают потерю прибыли для компании и потерю уважения клиентов, так как мошенниками могут быть затронуты их денежные средства. При этом мошенники используют паспорт с лицом одного и того же человека, меняя незначительные детали (цвет волос, форма ушей, добавление усов). \nТаким образом, возникла потребность в проверке паспортов, чтобы выявить: на фото в паспорте один и тот же человек или это разные люди и паспорт был подделан, для того чтобы вычислить мошенников и уберечь денежные средства клиентов.\nВ данном посте будет рассмотрена задача, называемая \nпоиском похожих изображений\n, в которой нужно будет найти все похожие изображения из датасета на загруженную фотографию из того же датасета.\nМодель поиска похожих изображений\nПоиск похожих изображений — активная и быстро развивающаяся область исследований в последнее десятилетие. Исследования в данной области позволили разработать модели, которые могут помочь в работе в различных областях, например:\nчтобы найти похожие изображения;\nпоиск фотографий‑плагиатов;\nсоздание возможностей для обратных ссылок;\nзнакомство с людьми, местами и продуктами;\nпоиск товаров по фотографии;\nобнаружение поддельных аккаунтов, поиск преступников и т. д.\nНаиболее известными системами являются Google Image Search и Pinterest Visual Pin Search. Мы познакомимся с легкими и популярными подходами поиска похожих изображений, а именно:\nприменение сверточных автоэнкодеров;\nприменение предобученных моделей на основе нейронных сетей;\nприменение готовых библиотек (face_recognition).\nИзображения в данных подходах не используют меток, т. е. дополнительных текстовых или числовых элементов, которые классифицируют изображения по категориям. Извлечение признаков из изображения будет происходить только с помощью их визуального содержимого (текстуры, формы, и т. д.). Этот тип извлечения изображений называется \nпоиск изображений на основе содержимого (CBIR)\n, в отличие от поиска ключевых слов или изображений на основе текста.\nCBIR при использовании глубокого обучения и поиска изображений можно назвать формой \nобучения без учителя:\nПри обучении не используется никаких меток для классов;\nПодходы используются для преобразования изображения в векторное представление (т. е. нашего «вектора признаков» для данного изображения);\nВо время поиска похожих изображений, вычисляется расстояние между векторами преобразованных изображений — \nчем \nменьше\n расстояние, тем \nболее релевантными / визуально похожими\n являются два изображения.\nЗагрузка, обработка и работа с данными   \nДля построения модели нужны данные — изображения, с которыми будет проведена работа.   В целях безопасности будем использовать изображения известных личностей вместо фотографий реальных людей из паспортов. Качество входных данных проверялось вручную и при помощи инструмента ABBYY FineReader PDF15.\nРабота проводилась на виртуальном окружении \nRAPIDS.AI CUDA 11.0.3 (cuDNN 8.0.5) TensorFlow, PyTorch Geometric \nс использованием графического процессора A100, ОП в 4 Гб, с 2 ядрами процессора.\nПеред началом работы необходимо провести импорт библиотек и модулей из Keras и Tensorflow.\nРазвернуть код\nimport os\nimport keras 2.4.3\nfrom keras.preprocessing import image\nfrom keras.applications.imagenet_utils import decode_predictions, preprocess_input\nfrom keras.models import Model\nfrom tensorflow.keras import applications\nimport tensorflow as tf 2.3.4\nfrom tensorflow.keras.models import save_model\nimport tensorflow.keras.layers as L\nimport numpy as np 1.18.5\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt 3.3.4\nimport cv2 3.4.5.20\nimport pandas as pd 1.1.5\nimport tqdm 4.62.3\nfrom skimage import io \nimport glob \nfrom PIL import Image, ImageEnhance, ImageChops, ImageStat, ImageDraw 8.4.0\nimport face_recognition 1.3.0\nimport fitz 1.21.1\nfrom pathlib import Path\nimport shutil 2.7\nimport openpyxl 3.1.2\nfrom itertools import chain 3.1\nПосле импорта библиотек загружаем сами изображения. Для этого нужно полностью прописать путь до папки, где хранятся изображения, и создать список из путей до каждого изображения. \npath =\"/Users/Desktop/Python/Passports\" (здесь Ваш путь до pdf-сканов документов) \n      gPDF=glob.glob('path/*.pdf')\nДля того, чтобы получить изображение лица с фотографии в паспорте, pdf‑сканы необходимо перевести в формат изображения, для этого была написана следующая функция:\ndef extract_images_from_pdf(pdf):\n    count = 0\n    for tpdf in pdf:\n        name = Path(tpdf).stem\n        doc=fitz.open(tpdf)\n        for i in range(len(doc)):\n            for img in doc.get_page_images(i):\n                xref=img[0]\n                pix = fitz.Pixmap(doc,xref)\n                if pix.n < 5:\n                    pix.save(f'image_from_pdf/{name}p%s-%s.png' % (i,xref))\n                else:\n                    pix1 = fitz.Pixmap(fitz.csRGB, pix)\n                    pix1.save(f'image_from_pdf/{name}p%s-%s.png' % (i,xref))\n                    pix1 = None\n                pix = None\n                count+=1\n    return f'Found {count} images'\n\n  # Применение функции\nextract_images_from_pdf(gPDF)\nДалее получаем путь до всех обработанных изображений из pdf‑сканов и применяем функцию «face_recog_pdf» для того, чтобы вырезать область где находится лицо на фотографии. Сохраняем результат в отдельную папку.\ng=glob.glob('image_from_pdf/*.png')\n\ndef face_recog_pdf(gimage):\n    count = 0\n    for timage in gimage:\n        name = Path(timage).stem\n        img = face_recognition.load_image_file(timage)\n        test_loc = face_recognition.face_locations(img)\n        for f in test_loc:\n            top, right,bottom, left = f\n            face_img = img[top:bottom,left:right]\n            pil_img = Image.fromarray(face_img)\n            pil_img.save(f'pdf_img/{name}_face_{count}.png')\n            count+=1\n    return f'Found {count} face(s) in this photos'\n\n# Применение функции\nface_recog_pdf(g)\nПри помощи функций extract_images_from_pdf() и face_recog_pdf(), (с использованием библиотеки OpenCV) из 20 000 pdf‑сканов паспортов было обнаружено около 10 000 паспортов с фотографиями (в сканах присутствовали изображения без фото). \nПосле обработки pdf‑сканов, приводим все полученные изображения к одному формату и преобразовываем в вектора (этот метод применяется для подхода с использованием сверточных автоэнкодеров), для этого используем функцию:\ndef image2array(filelist – путь до папки с фотографиями):\n    image_array = []\n    for image in filelist[:200]:\n        img = io.imread(image)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (224,224))\n        image_array.append(img)\n    image_array = np.array(image_array)\n    image_array = image_array.reshape(image_array.shape[0], 224, 224, 3)\n    image_array = image_array.astype('float32')\n    image_array /= 255\n    return np.array(image_array)\n\ntrain_data = image2array(filelist)\nprint(\"Length of training dataset:\", train_data.shape)\nВ результате, после выполнения функций мы получили изображения вырезанных из паспортов лиц. Следующий этап — преобразование их в вектора (для подхода с использованием сверточных автоэнкодеров функция указана выше — image2array), в последствии вектора будем использовать для сравнения и получения наборов похожих изображений. \nВ следующих разделах рассмотрим основные подходы для решения поставленной задачи по поиску похожих изображений.\nСвёрточные автоэнкодеры для извлечения признаков из изображения\nСверточные автоэнкодеры (CAEs) — это тип сверточных нейронных сетей. \n \nАвтоэнкодер состоит из:\nЭнкодера\n (encoder)\n, \nкоторый преобразовывает входное изображение в представление скрытого пространства с помощью серии сверточных операций.\nДекодер\n (decoder)\n — \nпытается восстановить исходное изображение из скрытого пространства с помощью серии операций свертки с повышением дискретизации / транспонирования. Также известен как деконволюция.\nПодробнее о сверточных автокодерах можно прочитать \nздесь\n.\nСам автоэнкодер строится при помощи соединения сверточных слоев и слоев пуллинга, которые уменьшают размерность изображения (сворачивают его) и извлекают наиболее важные признаки. На выходе возвращаются encoder и decoder. Для задачи кодирования изображения в вектор, нам нужен \nслой после автоэнкодера, т\n.е. векторное представление изображения, которое в дальнейшем будет использоваться для поиска похожих изображений.\nПрименение функции summary() к модели покажет описание работы модели слой за слоем. Нужно следить за тем, чтобы \nразмер изображения на входе соответствовал размеру изображения на выходе декодера\n.\nРазвернуть код\nIMG_SHAPE = x.shape[1:]\ndef build_deep_autoencoder(img_shape, code_size):\n    H,W,C = img_shape\n    # encoder\n    encoder = tf.keras.models.Sequential() # инициализация модели\n    encoder.add(L.InputLayer(img_shape)) # добавление входного слоя, размер равен размеру изображения\n    encoder.add(L.Conv2D(filters=32, kernel_size=(3, 3), activation='elu', padding='same'))\n    encoder.add(L.MaxPooling2D(pool_size=(2, 2)))\n    encoder.add(L.Conv2D(filters=64, kernel_size=(3, 3), activation='elu', padding='same'))\n    encoder.add(L.MaxPooling2D(pool_size=(2, 2)))\n    encoder.add(L.Conv2D(filters=128, kernel_size=(3, 3), activation='elu', padding='same'))\n    encoder.add(L.MaxPooling2D(pool_size=(2, 2)))\n    encoder.add(L.Conv2D(filters=256, kernel_size=(3, 3), activation='elu', padding='same'))\n    encoder.add(L.MaxPooling2D(pool_size=(2, 2)))\n    encoder.add(L.Flatten())\n    encoder.add(L.Dense(code_size))\n\n    # decoder\n    decoder = tf.keras.models.Sequential()\n    decoder.add(L.InputLayer((code_size,)))\n    decoder.add(L.Dense(14*14*256))\n    decoder.add(L.Reshape((14, 14, 256)))\n    decoder.add(L.Conv2DTranspose(filters=128, kernel_size=(3, 3), strides=2, activation='elu', padding='same'))\n    decoder.add(L.Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=2, activation='elu', padding='same'))\n    decoder.add(L.Conv2DTranspose(filters=32, kernel_size=(3, 3), strides=2, activation='elu', padding='same'))\n    decoder.add(L.Conv2DTranspose(filters=3, kernel_size=(3, 3), strides=2, activation=None, padding='same'))\n    \n    return encoder, decoder\n\n\nencoder, decoder = build_deep_autoencoder(IMG_SHAPE, code_size=32)\nencoder.summary()\ndecoder.summary()\nПараметры и обучение модели:\ninp = L.Input(IMG_SHAPE)\ncode = encoder(inp)\nreconstruction = decoder(code)\n\nautoencoder = tf.keras.models.Model(inputs=inp, outputs=reconstruction)\nautoencoder.compile(optimizer=\"adamax\", loss='mse')\nautoencoder.fit(x=train_data, y=train_data, epochs=10, verbose=1)\nВ качестве оптимизатора модель использует «adamax» (\nрусско‑язычная документация\n; \nангло‑язычная документация\n), в качестве функции потерь метрику \nmse\n. Обучение проводится 10 эпох (т. е. 10 раз).\nПолучение изображения в виде вектора с помощью сверточных автоэнкодеров происходит за счет того, что энкодер кодирует изображение, но обратное декодирование не нужно и берется слой из модели, который отвечает за кодирование изображения и сохраняется. Таким образом, сохраняются все кодированные представления изображения.\nimages = train_data\ncodes = encoder.predict(images) \nassert len(codes) == len(images)\nПостроение модели подобия изображений при помощи K-ближайших соседей (NearestNeighbours)\nПосле получения представления сжатых данных всех изображений мы можем применить \nалгоритм K‑ближайших соседей для поиска похожих изображений\n. Он основан на расчете евклидового расстояния между векторами: те расстояния, которые будут меньше всего, будут означать, что изображения похожи.\nfrom sklearn.neighbors import NearestNeighbors\nnei_clf = NearestNeighbors(metric=\"euclidean\")\nnei_clf.fit(codes)\nДля того, чтобы увидеть, какие изображения модель считает похожими, были написаны две функции, которые показывают 5 и более ближайших/похожих фотографий на ту, с которой идёт сравнение.\ndef get_similar(image, n_neighbors=5):\n    assert image.ndim==3,\"image must be [batch,height,width,3]\"\n    code = encoder.predict(image[None])    \n    (distances,),(idx,) = nei_clf.kneighbors(code,n_neighbors=n_neighbors)\n    return distances,images[idx]\ndef show_similar(image):\n    distances,neighbors = get_similar(image,n_neighbors=3)\n    plt.figure(figsize=[8,7])\n    plt.subplot(1,4,1)\n    plt.imshow(image)\n    plt.title(\"Original image\")\n    \n    for i in range(3):\n        plt.subplot(1,4,i+2)\n        plt.imshow(neighbors[i])\n        plt.title(\"Dist=%.3f\"%distances[i])\n   \t   plt.show()\nПреимущества и недостатки использования сверточных автоэнкодеров\nПреимущества:\nПодход с использованием сверточных автоэнкодеров применим для обучения модели на данных, которые были выбраны для определенной задачи, если хотим «с нуля» построить свой алгоритм под конкретную задачу и нас не устраивают способы, заложенные в предобученных моделях или готовых решениях.\nНедостатки:\nмодели нужна более точная настройка параметров для слоев и больше данных (которые измеряются не в тысячах, а миллионах);\nметод затратен по времени, в отличие от применения готовых моделей и библиотек (написание кода заняло примерно 2,5 часа, когда написание кода для других подходов занимает от 15-25 минут), т.к. нужно обучать модель.\nИспользование предобученных моделей для извлечения признаков из изображения\nПомимо использования автоэнкодеров для получения признаков из изображения можно использовать уже предобученные модели для классификации. Таких моделей очень много, и они также используют сверточные слои и слои пуллинга для получения признаков. Возникает логичный вопрос, зачем же тогда использовать автоэнкодеры? \nВо‑первых, предобученные модели могли быть созданы для других целей и могут не подойти по входным параметрам или по самой конструкции нейронной сети для вашей задачи, поэтому придется её перестраивать или строить сеть самому. \nВо‑вторых, предобученные модели на выходе могут получать не тот размер изображения, который нужен и при загрузке датасета и преобразовании изображений в вектор может не хватить памяти и мощности компьютера, а также это будет занимать много времени. Поэтому, если использовать предобученные модели, то нужно использовать метод понижения размерности PCA. При этом автоэнкодер понижает размерность и можно его настроить таким образом, чтобы на выходе получался вектор необходимого размера.\nПлюсами применения предобученных моделей является то, что нет необходимости строить нейронную сеть, настраивать сверточные слои, нужно просто взять нужный слой и использовать его для своих целей. Также такие модели были обучены на больших датасетах и имеют готовые веса (настройки) для извлечения необходимых признаков, они лучше выделяют важные области на изображении.\nДля того, чтобы использовать предобученные модели, для начала их нужно загрузить. В качестве примера, берем модель VGG16 — сверточная сеть, с 13-ю слоями, которая была обучена на датасетах с большим количеством входных данных (14 миллионов изображений, принадлежащих к 1000 классам).\nmodel = keras.applications.vgg16.VGG16(weights='imagenet', include_top=True)\nmodel.summary()\nДля загрузки изображений используем функцию:\ndef load_image(path):\n    img = image.load_img(path, target_size=model.input_shape[1:3])\n    x = image.img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)\n    return img, x\nМодель VGG16 используется для классификации изображений, т. е. класса, к которой относится изображение (самолет, вертолет и т. д.), поэтому на выходе модель использует слой для классификации. Все предыдущие слои кодируют изображение в вектор. Данную модель можно полностью скопировать с удалением последнего слоя, таким образом получим модель, которая только кодирует изображение в вектор.\nfeat_extractor = Model(inputs=model.input, outputs=model.get_layer(\"fc2\").output)\nfeat_extractor.summary()\nПосле того как модель построена, применяем её к нашим данным. Затем, получаем вектор признаков каждого изображения и используем метод понижения размерности PCA.\nimport time\ntic = time.perf_counter()\nfeatures = []\nfor i, image_path in enumerate(filelist[:200]):\n    if i % 500 == 0:\n        toc = time.perf_counter()\n        elap = toc-tic;\n        print(\"analyzing image %d / %d. Time: %4.4f seconds.\" % (i, len(images),elap))\n        tic = time.perf_counter()\n    img, x = load_image(path);\n    feat = feat_extractor.predict(x)[0]\n    features.append(feat)\nprint('finished extracting features for %d images' % len(images))\n\nfrom sklearn.decomposition import PCA\nfeatures = np.array(features)\npca = PCA(n_components=100)\npca.fit(features)\n\npca_features = pca.transform(features)\nСледующий код показывает, как случайно выбирается вектор из датасета (вектор, полученный на предыдущем этапе), сравнивается расстояние от этого вектора до всех векторов в датасете, данные расстояния сортируются по возрастанию и выбираются наиболее близкие/похожие.\nfrom scipy.spatial import distance\nsimilar_idx = [ distance.cosine(pca_features[80], feat) for feat in pca_features ]\n\nidx_closest = sorted(range(len(similar_idx)), key=lambda k: similar_idx[k])[1:6] # отображение первых 6 похожих изображений\n\nthumbs = []\nfor idx in idx_closest:\n    img = image.load_img(filelist[idx])\n    img = img.resize((int(img.width * 100 / img.height), 100))\n    thumbs.append(img)\n\n# concatenate the images into a single image\nconcat_image = np.concatenate([np.asarray(t) for t in thumbs], axis=1)\n\n# show the image\nplt.figure(figsize = (16,12))\nplt.imshow(concat_image)\nИспользование готовых библиотек\nПоставленную задачу (\nпоиск похожих изображений)\n можно также решить при помощи готовых библиотек, одной из таких является библиотека \nface_recognition\n, основанная на библиотеке \ndlib\n.\nПосле того, как мы получили изображения с лицами, нужно перевести изображения в вектор, для этого в библиотеке face_recognition есть функция face_encodings, а для сравнения векторов, и соответственно, похожих изображений используется функция \ncompare_faces\n.\nСама библиотека работает также, как и нейронные сети, т. е. был обучен датасет изображений (173 Мб в gzip‑файле), но в отличие от предыдущего способа, датасет состоял только из изображений лиц (в предыдущем способе использовались разные изображения, в т.ч. животные и транспорт). \nРазвернуть код\n# Получаем путь до изображений с вырезанными областями с лицами\nphoto = glob.glob('pdf_img/*.png')\n\n# Функция для перевода изображения в вектор\ndef get_vector(train_image):\n    diff = {}\n    bad = []\n    for image in tqdm(train_image):\n        try:\n            img = face_recognition.load_image_file(image)\n            img_enc = face_recognition.face_encodings(img)[0]\n            diff.update({image:img_enc})\n        except IndexError:\n            bad.append(image)\n    return diff, bad\n# Функция для сравнения похожих изображений\ndef compare_faces(test_image, train_images):\n    img1 = face_recognition.load_image_file(test_image)\n    img1_enc = face_recognition.face_encodings(img1)[0]\n    print('Original_image:')\n    print(Path(test_image).stem)\n    Image.fromarray(img1).show()\n    print('Compared images:')\n    differences = {}\n    for name,vec in tqdm(train_images.items()):\n        try:\n            result = face_recognition.compare_faces([img1_enc], vec, tolerance=0.49)\n            differences.update({name:result})\n        except IndexError:\n            pass            \n    new_df = {key:value for key,value in differences.items() if value == [True]}\n    fig = plt.figure(figsize=(15,len(new_df.keys())))\n    rows,cols = 1, len(new_df.keys())\n    for idx, i in enumerate(new_df.keys()):\n        fig.add_subplot(rows, cols, idx+1)\n        im = Image.open(i)\n        print(Path(i).stem)\n        plt.imshow(im)\n        plt.axis(False)\n\n# Применение функции\ncompare_faces(photo[9], r)\nДанный подход хорошо находит похожие фотографии в датасете. Для задачи сравнения изображений точность оказалась около 80%. В качестве оценки использовалась своя придуманная метрика, стоит уточнить, что данные были размечены (т. е. изображения были просмотрены и разделены на похожие и нет): если модель находит все похожие изображения и количество непохожих изображений не превышает двух, то результат оценивался как правильный.\nУ каждого pdf‑скана паспорта было свое название, и в результате получаем список из названий похожих изображений в паспортах, в виде\n \nExcel‑файла. Для этого была написана функция для сохранения названий похожих изображений:\nРазвернуть код\n# перевод изображения в вектор\ndef get_true_images(test_image, train_image):\n    names = {}\n    for t in tqdm(test_image):\n        differences = {}\n        try:\n            img1 = face_recognition.load_image_file(t)\n            img1_enc = face_recognition.face_encodings(img1)[0]\n        except IndexError:\n            print(t)\n        for name, vector in train_image.items():\n            try:\n                result = face_recognition.compare_faces([img1_enc], vector, tolerance=0.4)\n                differences.update({name:result})\n            except IndexError:\n                pass\n        new_df = {key:value for key,value in differences.items() if value == [True]}\n        names.update({t:list(new_df.keys())})\nreturn names\n\n# получение словаря со списком похожих фотографий\ndef get_names(dictionary):\n    new_list = {}\n    for idx, i in enumerate(list(dictionary.keys())):\n        b = Path(i).stem\n        stem = []\n        for j in list(dictionary.values())[idx]:\n            a = Path(j).stem\n            stem.append(a)\n        new_list.update({b:stem})\n    data = pd.DataFrame(dict([(k,pd.Series(v)) for k,v in new_list.items()]))\n    return data\n\n# Использование функции\nd = get_names(dictionary)\n\n# Сохранение функции в Excel-файл\nd.to_excel('find_faces.xlsx', sheet_name = 'Test')\nВыводы\nВ итоге были проверены подходы для поиска похожих изображений в наборе данных при помощи кодирования изображений в векторную форму. Данные алгоритмы показали, что способны решать поставленную задачу, но их всегда можно улучшить, например, путем добавления новых слоев или предварительной обработки изображений. \nДля решения нашей задачи мы прошли следующие шаги:\nШаг 1:\n Обработали изображения, преобразовали в нужный формат.\nШаг 2:\n Преобразовали изображения в вектор при помощи автоэнкодера, предобученной модели или готовых библиотек.\nШаг 3:\n Извлеченные признаки‑вектора сравнили с набором других векторов и нашли похожие изображения на основе расстояний: чем меньше расстояния, тем более похожи изображения.\nШаг 4:\n Сохранили и выгрузили результаты (Excel‑файл).\nИз трех рассмотренных нами подходов готовые библиотеки лучше всех отработали на точность (80–85%). Автоэнкодеры дали точность в 61%, а предобученные модели показали точность в 70%.\nДанные подходы позволили обнаружить поддельные и старые паспорта и запустить процесс проверки по клиентам, чьи паспорта модель определила как поддельные. \nРезультаты разработки алгоритмов по поиску похожих изображений пригодятся также для реализации следующих задач:\nобнаружение поддельных документов;\nобнаружение мошенников/подозрительных лиц (при наличии базы/ Стоп-листов и т.д.);\nконтроль при проходе в здания офисов;\nпоиск похожих изображений;\nпоиск фотографий-плагиатов;\nобнаружение копий аккаунтов.\nВесь представленный код можно найти \nпо ссылке\n.\nПолезные ссылки:\nСсылки на датасет\nО сверточных автоэнкодерах\nПро построение сверточных нейронных сетей\nБиблиотека face_recognition\n \n ",
    "tags": [
        "машинное зрение",
        "машинное обучениe",
        "machine learning",
        "computer vision",
        "распознавание объектов",
        "сезон machine learning"
    ]
}