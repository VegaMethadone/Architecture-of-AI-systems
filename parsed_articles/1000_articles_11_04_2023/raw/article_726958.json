{
    "article_id": "726958",
    "article_name": "Istio в разрезе: что умеет и не умеет самый популярный Service Mesh (обзор и видео доклада)",
    "content": "В докладе я препарирую \nIstio\n, дабы понять, как он работает, какие у него подводные камни и как им правильно пользоваться.\nЭто мой второй доклад про Istio и Service Mesh. Первый я сделал на конференции Kuber Conf 2021: \n«Что ждать от внедрения Istio?»\n. Рекомендую ознакомиться сначала с ним, будет несколько проще.\nВидеоверсию доклада \nможно посмотреть на YouTube\n (~50 минут). Ниже — основная выжимка из него в текстовом виде.\nЧто такое Service Mesh\nС помощью Service Mesh можно реализовать разные паттерны по управлению TCP-трафиком в проекте. Настраиваются они через декларативный язык с помощью API Kubernetes. Бонусом мы получаем расширенные возможности по наблюдаемости за проектом. Более ёмко определение термина я раскрыл в предыдущем докладе.\nВозможности Service Mesh\nКакие вызовы стоят перед вами, если вы решили внедрять Istio? Главных — два:\nНадежность приложения\n. Как Istio повлияет на latency и надежность компонентов ПО? Что будет, когда проект вырастет? И прочее.\nСложность Istio\n. Всякий, кто сталкивался с Istio, знает, что это сложная штука. Одних только интерфейсов, которые предоставляет Istio, наберется с десяток. А к документации есть много вопросов.\nМой доклад даёт необходимые знания о том, что ждать от Istio, к чему готовиться и как действовать в тех или иных ситуациях. Ну и самое главное: как им правильно пользоваться.\nБазовые принципы работы Istio\nРассмотрим типовой веб-проект, который работает в Kubernetes. Каждый компонент — фронтенд, бэкенд и база данных — живут каждый в своём Поде:\nМы решили внедрить Istio. Что произойдет с нашим кластером:\nтам появится контроллер istiod, который в мире Istio называется Control Plane;\nкаждому Поду в нагрузку выдадут сайдкар (sidecar), который обрабатывает пользовательские запросы, — весь массив сайдкаров называется Data Plane.\nНам же остается настраивать сетевые паттерны через API Kubernetes. Control Plane эти паттерны переваривает и рассылает по сайдкарам инструкции. \nПримерно так выглядит типичный Service Mesh, в частности Istio. И с этой схемой мы будем разбираться в подробностях.\nКак работает Istio на уровне Pod\nЗаглянем внутрь сайдкара и обнаружим там \nEnvoy\n:\nБез понимания его философии и базовых принципов изучение Istio невозможно.\nEnvoy — типичный универсальный прокси-сервер. Он принимает на вход TCP- и HTTP-запросы, переваривает их и отправляет наружу. Смысл в том, что Envoy даёт нам интерфейс для управления процессом обработки этого запроса. Этот интерфейс называется envoy API:\nEnvoy API — очень гибкая структура, которая хорошо задокументирована. Мы не полезем в дебри, а лишь познакомимся с базовыми понятиями и примитивами. \nВсё начинается с примитива listener. Сначала кажется, что это некая инструкция, которая заставляет Envoy слушать портик. Но на самом деле всё гораздо сложнее: listener — это целый конвейер по обработке входящего запроса. Он содержит цепочку фильтров, которые\nвалидируют TLS;\nкалькулируют метрики для экспорта в Prometheus;\nобеспечивают авторизацию запросов;\nи так далее.\nФильтры listener’а\nЭтот конвейер может состоять из сколь угодно большого количества фильтров, которые могут идти в любом порядке. Но как правило вся цепочка listener'а заканчивается фильтром route. \nА само решение, куда именно полетит запрос, основывается на другом фундаментальном понятии Envoy — \ncluster\n. (Я терпеть не могу этот термин: с ним связано очень много коллизий. Но имеем, что имеем.)\nCluster содержит исчерпывающую информацию о сервисе, к которому полетит запрос:\nсписок эндпоинтов с IP-адресами и портами;\nправила балансировки между IP-адресами;\nсертификаты для установки TLS-соединения;\nправила для калькуляции метрик.\nИ вот примерно по такому пути проходит запрос в Envoy:\nВернемся к нашему сайдкару. Что мы еще о нем знаем? Знаем, что сайдкар перехватывает прикладные запросы и отправляет их дальше. Возникают вопросы: \nКаким образом он перехватывает запрос?\nВ чьих интересах он обрабатывает эти запросы? Где берёт инструкции?\nДавайте разбираться. \nВ случае с перехватом трафика всё просто: в каждом Поде, который работает под управлением Istio, настроен обычный DNAT. Весь DNAT исходящего трафика направляется на порт 15001 listener'а:\nТаким образом, Envoy получает трафик, чтобы потом с ним что-то сделать. \nС входящими запросами происходит примерно то же самое, поэтому мы его рассматривать не будем.\nВ чьих интересах Envoy обслуживает запросы, откуда он берет информацию?\nНа самом деле Envoy в sidecar'е не один: вместе с ним работает istio-agent. Именно он держит связь с Control Plane, получает от него всю информацию о состоянии кластера и переводит ее на понятный для Envoy язык:\nТак Envoy получает информацию о состоянии кластера. \nДалее разберемся с цепочкой istiod — Envoy.\nКонвертация объектов Kubernetes в Envoy API\nУ каждого компонента нашего приложения есть Service. Компоненты могут жить в нескольких Подах, и у каждого — свой IP:\nВсю эту информацию Control Plane (istiod) получает от API Kubernetes, переваривает и рассылает по Envoy в сайдкарах:\nЧтобы разобраться, как устроен этот процесс, предлагаю представить себя на месте Istio и понять, какие объекты из мира K8s нам нужно преобразовать в объекты из мира Envoy:\nМы можем создать для Сервисов соответствующие кластеры, используя IP эндпоинтов и порт Сервисов:\nТеперь мы можем настроить маршрутизацию между этими объектами. Что такое маршрутизация: мы берём запрос, берём какие-то параметры этого запроса и на их основе решаем, куда далее лететь запросу. \nЧто мы видим в перехваченном запросе:\nTCP IP/port, куда летел запрос;\nзаголовок Host, если это HTTP-запрос;\nTLS SNI, если трафик зашифрован на стороне приложения (в нашем кластере таких приложений нет, поэтому этот вариант не рассматриваем).\nУ нас в кластере есть два HTTP-сервиса (front и back), для идентификации которых можно использовать заголовок Host. Также есть обычный TCP-сервис (mysql).\nЧто получается: мы должны создать таблицу маршрутизации, в которой будут в кучу сложены и HTTP-, и TCP-сервисы? Но это как-то странно. Неужели мы будем залезать в каждый MySQL-запрос в поисках HTTP-заголовка? \nВ Istio согласились, что это неразумно и решили не объединять все сервисы в одну таблицу маршрутизации. Вместо этого для обычных TCP-сервисов, которые не поддерживают «умную» маршрутизацию, создаются отдельные listener'ы с отдельной таблицей маршрутизации:\nВ новом listener'е для MySQL (10.222.0.42:3306) таблица маршрутизации содержит единственную запись и по факту это не таблица, а обычный tcp proxy. \nHTTP-сервисы мы можем смело объединить в другом listener'е, а различать их — по заголовку Host. \nИтоговая схема:\nЗдесь внимательные люди вспомнят, как я говорил, что у Envoy есть только один listener:\nЯ почти не обманул. Да, этот listener в Envoy есть, и он действительно слушает порт 15001. Разница в том, что остальные listener'ы — «воображаемые». Их не видно в netstat, если вы посмотрите в сетевое пространство имен Пода. Эти listeners существуют только в воображении Envoy и называются «виртуальными». \nБлагодаря тому, что порт 15001 Envoy слушает с флагом \nuse_original_dst\n, он может узнать оригинальный destination у ядра перед процедурой DNAT. На основе этих данных Envoy передает запрос на обработку соответствующему listener'у:\nПримерно по такой схеме работает Envoy. Сейчас мы закрепим эту информацию, изучив жизненный цикл одного запроса.\nЖизненный цикл запроса под управлением Istio\nВозвращаемся к нашему Поду с приложением и Envoy. Допустим, нашему приложению потребовалось отправить GET-запрос. Оно делает resolve IP-адреса, получает его и отправляет запрос куда-то наружу. Envoy этот запрос перехватывает:\n… и видит: запрос летел на порт 8080. Envoy понимает, что для этого запроса есть подходящий listener, и отправляет запрос ему на обработку.\nДалее:\nзапрос обрабатывается фильтрами listener'а и маршрутизируется в кластер;\nподходящий кластер выбирается на основе заголовка Host: в запросе (\nback\n).\nДалее в cluster:\nнастраиваем балансировку между существующими эндпоинтами;\nустанавливаем TCP-соединение;\nпополняем статистику для пассивного health check;\nзагружаем необходимые сертификаты для установки зашифрованного соединения;\nкалькулируем метрики;\nотправляем запрос в соседний Под.\nВ итоге получается, что когда мы включаем Istio, наш запрос летает вот по такой загогулине:\nС одной стороны, это пугает. С другой, у нас появился контроль через призму этих самых фильтров Envoy. И Istio на самом деле придуман для того, чтобы дать нам интерфейс управления этими фильтрами.\nОсобенности Istio API\nIstio даёт нам набор интерфейсов:\nPeerAuthentication — для управления аутентификацией входящих запросов. \nDestinationRule — для управления аутентификацией при исходящих запросах; он же отвечает вообще за всё, что связано с исходящими запросами.\nAuthorizationPolicy — для авторизации.\nVirtualService — для маршрутизации.\nРазберёмся с каждым из этих них.\nPeerAuthentication\nСпецификация интерфейса очень простая. Единственный флажок, который мы можем крутить, — «режим» (\nmode\n):\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: mtls-on\n  namespace: myns\nspec:\n  mtls:\n    mode: XXX # <---\nМы можем:\nвыключить входящую аутентификацию, то есть не валидировать запрос (\nDISABLE\n);\nстрого валидировать каждый входящий запрос (\nSTRICT\n);\nвключить режим «ни нашим, ни вашим»: зашифрованные запросы валидируем, незашифрованные пропускаем как есть (\nPERMISSIVE\n).\n☝️ С последним режимом, \nPERMISSIVE\n, есть нюанс. Если у вас используется FTP, SMTP, MYSQL или другой протокол, который подразумевает, что первое слово за сервером — например, сервер должен сделать приветствие, — у Envoy «сорвет башню». Он будет ждать от клиента извне хотя бы один пакетик, чтобы понять, зашифрован он или нет. А клиент будет ждать приветствия от нашего подконтрольного приложения. Получается, проблема курицы и яйца. Это очень популярная ошибка, которую тяжело выявить. Имейте в виду.\nDestinationRule\nХотя это сам по себе очень сложный объект, для настройки аутентификации нам тоже доступен только флаг \nmode\n:\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: mtls-on\n  namespace: myns\nspec:\n  host: *.myns.svc\n  trafficPolicy:\n    tls:\n      mode: XXX # <---\nМы можем:\nне шифровать исходящий запрос (\nDISABLE\n);\nиспользовать для соединения клиентские сертификаты, которые сгенерировал Istio (\nISTIO_MUTUAL\n);\nиспользовать пользовательские клиентские сертификаты (\nMUTUAL\n);\nпросто валидировать сертификат сервера и не представляться клиентским сертификатом (\nSIMPLE\n).\nЕсли мы объединим два этих ресурса — PeerAuthentication и DestinationRule, — получим тот самый Mutual TLS. \n☝️ Пугаться сложности не стоит: если вы твердо \nне\n знаете, зачем вам это надо, скорее всего, настраивать ничего не придётся. Istio делает это автоматически. Если он видит, что общение между двумя Подами идет под его управлением, — настраивает между ними Mutual TLS. Если хотя бы один из Подов не под управлением Istio, он работает в режиме \nPERMISSIVE\n, со всеми вытекающими.\nDestinationRule также отвечает за все нюансы установки исходящих соединений:\nбалансировка между эндпоинтами;\nнастройка TCP-параметров;\nпассивный health check (Circuit Breaking).\nДля примера я подготовил такую хитрую спецификацию:\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: mypolicy\nspec:\n  host: back\n  trafficPolicy:\n    loadBalancer:\n      consistentHash:\n        httpCookie:\n          name: user\n          ttl: 0s\n    connectionPool:\n      tcp:\n        maxConnections: 1\n      http:\n        maxRequestsPerConnection: 10\n    outlierDetection:\n      consecutive5xxErrors: 7\n      interval: 5m\n      baseEjectionTime: 15m\nЧто тут написано:\nIstio, если запрос полетит в сервис \nback\n, используй не стандартный алгоритм балансировки, а алгоритм \nconsistentHash\n на основе ​​\nhttpCookie\n, которая называется \nuser\n;\nне устанавливай более одного TCP-соединения в сторону сервиса, обновляй это соединение каждые 10 запросов;\nесли какой-то из эндпоинтов сервиса будет «моросить» — то есть в течение 5 минут вернет хотя бы 7 ошибок, — дай ему отдохнуть на 15 минут.\nЭто всё, на что способен DestinationRule.\n☝️ Здесь имеет место очень распространенная ошибка. В примере мы указали Istio, чтобы на сервис не летело больше одного соединения (\nmaxConnections: 1\n). И мы надеемся, что Istio так и будет работать. Действительно: если у нас один клиентский сервис, и мы настроили один DestinationRule, у нас будет одно соединение. Но стоит нам масштабировать клиента, окажется, что каждый сайдкар будет поддерживать по соединению в сторону сервиса. И этим сайдкарам до лампочки, есть у них соседи или нет. Наше приложение может быть к этому не готово. Эту особенность Istio просто нужно иметь в виду.\n☝️ И еще один нюанс. Меня несколько раз просили настроить активные health checks в Istio. Я не знаю, откуда люди берут эту информацию. Поэтому я решил вынести это в доклад: \nактивных health checks в Istio нет\n. Есть только пассивные. И есть настоявшийся \nissue\n, на который я не оставляю надежды.\nAuthorizationPolicy\nДля авторизации запроса мы можем использовать разные параметры:\nдиапазоны IP;\nпараметры Kubernetes (если клиентский Под работает под управлением Istio):\nиз какого пространства имён прилетел запрос;\nиз-под какого ServiceAccount работает Под, сделавший этот запрос.\nHTTP-параметры;\nпользовательские заголовки.\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: httpbin\n  namespace: foo\nspec:\n  selector:\n    matchLabels:\n      app: httpbin\n  action: ALLOW\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/default/sa/sleep\"]\n    - source:\n        namespaces: [\"test\"]\n    to:\n    - operation:\n        methods: [\"GET\"]\n        paths: [\"/info*\"]\n    when:\n    - key: request.auth.claims[iss]\n      values: [\"https://accounts.google.com\"]\n    - key: request.headers[X-Secret]\n      values: [\"la-resistance\"]\n\nСамое интересное — то, что мы можем взять публичный ключ нашего провайдера аутентификации и передать его Istio. А тот сможет валидировать наши прикладные JWT-токены, которые находятся в HTTP-заголовке \nAuthorization:\n. Из этих токенов можно достать лэйблы и на их основе производить авторизацию (см. поле \nwhen\n). Это очень удобно с точки зрения безопасности.\n☝️ Выше я говорил, что к документации Istio есть определенные вопросы, в том числе в случае AuthorizationPolicy. Там как-то не по-людски описан процесс принятия решения. Я постарался это уладить \nв документации модуля istio для Deckhouse\n — перевел описание процесса на более понятный, на мой взгляд, язык. Надеюсь, кому-нибудь поможет.\nЕще один важный нюанс с точки зрения безопасности — то, что авторизация работает на каждом сайдкаре, который встретится на пути запроса. Если запросу нельзя наружу, ближайший сайдкар его не выпустит. Но если запрос каким-то образом вылетел вовне и долетел до сервиса, то там его ждет еще одна проверка. Безопасность должна быть безопасной.\nVirtualService\nРазберем маршрутизацию на примере пары задач.\nДопустим, у нас есть клиентский Под, который общается с бэкенд-сервисом. Мы решили отрефакторить бэкенд и вытащили из него админку. И теперь мы хотим, чтобы все запросы, у которых location начинается с \n/admin\n, отправлялись в новый сервис — admin:\nКлассическая задача, с которой Istio прекрасно справится. Для этого берем VirtualService с нехитрой спецификацией:\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: back\nspec:\n  hosts:\n  - back\n  http:\n  - match:\n    - uri:\n        prefix: \"/admin\"\n    route:\n    - destination:\n        host: admin\n  - route:\n    - destination:\n        host: back\n☝️ При этом для \nmatch\n нам необязательно использовать какие-то locations, HTTP-методы. Можно использовать всякие регулярные выражения, хуки, пользовательские заголовки, параметры Kubernetes и пр.\n☝️ Стоит добавить, что VirtualService, как и DestinationRule, работает только на стороне клиента. Если запрос прилетел на какой-то сервис, второй раз маршрутизация производиться не будет.\nВозьмем задачу посложнее: мы написали новую версию сервиса back и хотим, чтобы она поработала на реальной нагрузке, прежде чем выкатывать ее в production. То есть хотим реализовать пресловутый канареечный деплой. Как поступим?\nМожно в лоб: выкатываем сервис в кластер в виде Deployment и добавляем его к существующему сервису. Тем самым мы расширяем список существующих endpoints еще одним:\nИ балансировка теперь происходит по одноранговым эндпоинтам. В принципе, это более-менее рабочий вариант. Для сельской местности сойдет. \nНо есть вопросы:\nКак нам выделить особые условия для нашего канареечного деплоя?\nКак выделить фокус-группу?\nКак повлиять на процент трафика, который прилетит на этот деплой?\nСделать всё это можно, но сложно. Поэтому этот вариант мы считаем скучным и не рассматриваем. Единственный плюс — для этого варианта можно даже Istio не включать.\nДавайте к нашему Deployment добавим K8s-сервис:\nТаким образом, в нашей системе появляется соответствующий кластер (в мире Istio), и мы можем маршрутизировать трафик в кластер back-canary. \nБерем VirtualService и с его помощью настраиваем, например, Weighted Load Balancer. То есть направляем 90% трафика на оригинальный сервис, а 10% — на канареечный деплой:\nК этому способу у меня лично нет вопросов. И я могу рекомендовать его каждому. \nНо у Istio свой взгляд на ситуацию. Они, видимо, решили, что создавать новый сервис для канареечного деплоя — это как-то накладно, неудобно или неправильно. Поэтому они дали пользователям возможность виртуально клонировать существующий кластер. То есть взять всю информацию из кластера и скопировать в его клон. А еще дали возможность корректировать те или иные настройки этого кластера-клона:\nТакой клон в мире Istio называется сабсет (subset). И они активно используют эту штуковину в своей документации. Поэтому я обязан был о ней рассказать. \nВоспользуемся этими сабсетами. Создадим сабсет для бэкенда, поправим список эндпоинтов и настроим маршрутизацию. \nЧтобы создать сабсет, нам нужно снова вернуться к DestinationRule — именно он отвечает за управление сабсетами. Вот его спецификация:\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: back-canary\nspec:\n  host: back\n  subsets:\n  - name: canary\n    labels:\n      version: new\nТут сказано: «возьми кластер back, создай для него сабсет с именем \"canary” и скорректируй список энжпоинтов, используя другой label selector». \nТеперь можно маршрутизировать запрос в сторону сабсета с помощью VirtualService:\nСпецификация этого VirtualService похожа на предыдущий, кроме одного нюанса: 90% трафика у нас так же направляется на оригинальный кластер back, и оставшиеся 10% — тоже на кластер back, но точнее — на его сабсет. \nВ Istio считают, что это самый правильный способ делать канареечный деплой, поэтому активно форсят его в своей документации. Но лично я ни разу не встречал кейсов, когда использование сабсета было оправдано. Почти всегда можно создать еще один Сервис, и это будет гораздо прозрачнее и понятнее для пользователя.\nЕще есть легенда, что Istio — это некое коробочное решение для «канарейки». Это не так. Istio умеет делать хитрую маршрутизацию и ничего более. То есть не более того, что умеет делать Envoy. \nТо же самое касается всех остальных интерфейсов. Istio — всего лишь интерфейс для настройки Envoy в ваших сайдкарах.\nНо зато в этом ограничении есть преимущество: если вам что-то не понятно в документации интерфейсов Istio, вы можете найти что-то похожее в документации Envoy, и там, скорее всего, гораздо лучше всё написано.\nКак Istio влияет на надежность\nКакие вопросы обычно возникают к Istio в контексте надежности:\nЧто с latency?\nЧто с безопасностью?\nЧто, если что-то сломается?\nЧто с масштабированием?\nLatency\nДля предыдущего доклада\n я сделал пару сотен синтетических замеров. Главный вывод: нужно закладывать ~2,5 мс на запрос — и с этим придется жить. \nБезопасность\nВ том же докладе я в деталях раскрыл, как работает Mutual TLS, как ротируются сертификаты, на основе каких данных и насколько это безопасно. Вывод: беспокоиться не о чем, если:\nне пускать кого попало в API K8s;\nне раздавать кому попало корневой сертификат Istio (не путать с корневым сертификатом Kubernetes);\nбережно относиться к ServiceAccount-токенам, который Kubernetes генерирует для каждого Пода.\nЛомаем компоненты\nВернемся к нашему привычному сетапу: фронтенд, бэкенд, база. Что с ним может пойти не так? Самое простое — может отвалиться какой-нибудь сайдкар внутри Data Plane. Трагедии здесь нет, просто этот Под будет висеть в \ncrash\n.\nСамое интересное — это когда отваливается Control Plane:\nТут начинаются спецэффекты. Control Plane перманентно рассылает информацию по сайдкарам. Если у нас в кластере что-то изменится — появится или сломается Под, появится Сервис и т. д. — Control Plane не узнает об этом от API K8s. Соответственно, не сможет разослать новую сводку по сайдкарам. Те будут жить по старым инструкциям, что, в принципе, иногда допустимо и катастрофы не произойдёт:\nНо если в этой же ситуации у нас вдобавок падает существующий Под, нам останется только надеяться на outlier detection, то есть на пассивные health checks. Control Plane этот эндпоинт автоматически из списка не выпилит, чтобы не дёргать его. \nДругая возможная неприятность: если у нас в кластере появится ещё один Под, которому нужно управление Istio, он зависнет в состоянии \npending\n:\nControl Plane также отвечает за инъекцию Подов, и в нашем случае он не сможет это сделать. Поэтому Под не появится в кластере. Грустно, но надо быть к этому готовым.\nМасштабирование\nНаше приложение-пример работает в пространстве имён. Допустим, оказалось, что кластер у нас коммунальный: мы выкатили туда ещё одно приложение либо компонент, с которым существующее приложение знаться не хочет. Для Control Plane дизайн нашего кластера не важен, все сайдкары он любит одинаково. Поэтому он честно рассылает полную сводку о состоянии кластера по каждому сайдкару:\nТо есть каждый сайдкар знает о существовании сервисов в соседнем пространстве имён: у них есть столько-то Подов, такие-то пользовательские настройки и пр. Это приводит к тому, что сайдкар начинает съедать больше ресурсов CPU и RAM. Но и это не катастрофа.\nНастоящие проблемы начинаются, когда дело касается Control Plane. Ведь теперь на каждый чих в кластере Control Plane обязан рассылать сводку о состоянии кластера по каждому сайдкару. Причем делает он это асинхронно. И это очень дорогая операция. Рассылается не изменение (diff), которое произошло в кластере, не какое-то сообщение о том, что что-то поменялось, — Control Plane рассылает полный пакет с данными о состоянии кластера. Получается, что Control Plane как минимум вынужден «шевелить транзисторами» и косвенно съедать ресурсы сети:\nЯ видел сетап, где такой трафик перманентно съедал 500 Мбит. \nОднако есть решение и этой проблемы. Самое правильное, что мы можем сделать, — воспользоваться ресурсом Sidecar. Он позволяет изолировать друг от друга группы Подов:\nВ спецификации Sidecar мы пишем (см. параметр \nhosts\n): «Не хочу знать никого, кроме ребят из пространства имён \nmyns\n. Еще иногда хочу общаться с Control Plane». \nЕсли мы распределим эти Sidecars по пространствам имён, область видимости сайдкаров снизится:\nТем самым снизится нагрузка на Control Plane и на сеть. \nЭто, кстати, хорошо и для безопасности: если компоненты не будут друг о друге знать, то не смогут друг друга сломать.\nДругой способ решения проблемы — масштабировать Control Plane. Нагрузка между сайдкарами размажется. Но гарантий здесь нет, поэтому такой способ я не рекомендую.\nЕсть еще один способ — «секретный»: можно использовать \nпротокол Delta xDS\n. Суть в том, что Istio на самом деле умеет рассылать не полную сводку, а только diff-ку. Это значительно сокращает нагрузку на Control Plane и сеть. Но это экспериментальная фича. Поэтому рекомендовать ее я тоже не буду. Я вам о ней не говорил.\nСамые большие надежды у меня на другой подход к организации Service Mesh, который вскоре предоставит Istio: \nAmbient Mesh\n. В Istio решили отказаться от индивидуальных сайдкаров в пользу коммунальных. Теперь сайдкары будут размазаны по узлам с помощью DaemonSet. Это снизит нагрузку на Control Plane, упростит обновления и добавит много других плюшек (и, естестенно, анти-плюшек). Сейчас — в Istio 1.16 — Ambient Mesh еще нет. Ждём с нетерпением.\nНа этом всё. Надеюсь, информация из доклада поможет вам беспроблемно использовать Istio в ваших проектах. \nВидео и слайды\nВидеозапись выступления (~50 минут):\nПрезентация:\nP.S.\nЧитайте также в нашем блоге:\n«Что ждать от внедрения Istio? (обзор и видео доклада)»\n.\n«Назад к микросервисам вместе с Istio»: \nчасть 1 (знакомство с основными возможностями)\n, \nчасть 2 (маршрутизация, управление трафиком)\n, \nчасть 3 (аутентификация и авторизация)\n.\n«Service Mesh: что нужно знать каждому Software Engineer о самой хайповой технологии»\n.\n \n ",
    "tags": [
        "istio",
        "service mesh",
        "kubernetes",
        "devops"
    ]
}