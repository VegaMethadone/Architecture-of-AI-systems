{
    "article_id": "728040",
    "article_name": "Kubernetes 1.27: обзор нововведений",
    "content": "Этой ночью \nвышла\n новая версия Kubernetes — 1.27. Среди главных изменений — переход на собственный полноценный реестр registry.k8s.io, обновление запросов и лимитов пода «на месте» — т.е. без необходимости перезапускать под или его контейнеры — и ряд усовершенствований в области безопасности (stable-статус фичи seccomp by default, in-process-валидация запросов к API-серверу и др.).\nДля подготовки статьи использовалась информация из таблицы \nKubernetes enhancements tracking\n, \nCHANGELOG-1.27\n, \nобзора Sysdig\n, а также конкретные Issues, Pull Requests и Kubernetes Enhancement Proposals (KEPs).\nВсего в новом релизе 60 изменений. Из них:\n18 новых функций (alpha); \n29 продолжают улучшаться (beta);\n13 признаны стабильными (stable).\nПримечание\nМы сознательно не переводим названия фич на русский. Они в основном состоят из специальной терминологии, с которой инженеры чаще встречаются в оригинальной формулировке.\nKEP от «Фланта»: Auth API to get self user attributes\n#3325\nKEP от «Фланта» перешел в beta-стадию. Он добавляет в группу authentication.k8s.io новый эндпоинт API — \nSelfSubjectReview\n. С помощью команды \nkubectl auth whoami\n можно обратиться к этому эндпоинту и посмотреть атрибуты текущего пользователя после завершения процесса аутентификации. Подробнее о KEP’е — в \nобзоре нововведений Kubernetes 1.26\n.\nУзлы\nAlpha-фичи\nKubelet limit of Parallel Image Pulls\n#3673\n; \nKEP\nKEP добавляет в kubelet ограничение на число параллельно скачиваемых образов на уровне узла.\nИзвлечение образов в Kubernetes ограничивается параметрами QPS и burst. Их недостаток в том, что лимитируется только число запросов, отправляемых в среду исполнения, при этом число загрузок «в процессе» не учитывается. Другими словами, даже при минимальном значении QPS параллельно может скачиваться множество образов, если процесс загрузки по тем или иным причинам занимает продолжительное время (см., например, \nIssue #112044\n). В результате число параллельно запрашиваемых образов оказывается ничем не ограничено.\nKEP добавляет параметр \nmaxParallelImagePulling\n в конфигурацию kubelet’а. Он ограничивает максимальное количество параллельно запрашиваемых образов. Любой запрос, превышающий заданный лимит, будет блокироваться до тех пор, пока не завершится скачивание одного из образов «в процессе». Подробнее о логике работы механизма можно узнать из \nпредложения по улучшению\n.\nIn-Place Update of Pod Resources\n#1287\n; \nKEP\nKEP позволяет обновлять запросы и лимиты пода «на месте» — т.е. без необходимости перезапускать под или его контейнеры. Идея состоит в том, чтобы сделать PodSpec изменяемой (mutable) в отношении определенных ресурсов. PodStatus также будет доработан — он сможет предоставлять информацию о ресурсах, выделенных для пода, а также фактических ресурсах, потребляемых подом и его контейнерами. KEP базируется на \nпредложении по вертикальному масштабированию в реальном времени и «на месте»\n и \nконцепции вертикального масштабирования ресурсов в Kubernetes\n.\nТакже планируется дополнить API Container Runtime Interface функциями, необходимыми для управления конфигурацией ресурсов CPU и памяти контейнера в среде исполнения.\nВ настоящее время изменения, связанные с ресурсами пода, требуют пересоздания последнего из-за неизменяемости (immutable) PodSpec. В то время как перезапуски подов обычно не страшны, в ряде случаев (например, если число подов невелико или рабочие нагрузки являются stateful) перезапуск нежелателен и приводит к снижению доступности. Решить эту проблему позволяет прямое изменение ресурсной конфигурации пода без необходимости пересоздавать под или контейнеры в нем.\nКроме того, вертикальное масштабирование пода «на месте» полагается на Container Runtime Interface при обновлении запросов/лимитов на CPU и память для контейнера(-ов) пода. В нынешнем виде API CRI имеет ряд недостатков, которые необходимо устранить:\nUpdateContainerResources принимает параметр, описывающий ресурсы контейнера, предназначенные для обновления, для Linux-контейнеров. Это грозит проблемами для Windows-контейнеров и контейнеров, требующих сред исполнения, не базирующихся на Linux.\nНе существует механизма CRI, позволяющего kubelet’у из среды исполнения контейнера запрашивать и обнаруживать лимиты CPU и памяти, заданные для контейнера.\nОжидаемое поведение среды исполнения, которая обрабатывает API CRI UpdateContainerResources, слабо определено и плохо документировано.\nВ рамках KEP’а спецификацию PodSpec предлагается сделать изменяемой в части ресурсов контейнеров. Первоначально правки в спецификацию будут вноситься путем ее обновления; в бета-версии для этих целей планируется создать новый подресурс \n/resize\n. В дальнейшем его можно будет применять к другим ресурсам, работающим с шаблонами подов, таким как Deployment’ы, ReplicaSet’ы, Job’ы и StatefulSet’ы. Это позволит открывать контроллерам вроде VPA частичный RBAC-доступ, не давая полного доступа на запись к спецификациям подов. Кроме того, в разделе PodSpec.Containers появится ResizePolicy со следующими значениями:\nRestartNotRequired\n — значение по умолчанию; K8s попытается изменить размер контейнера без его перезапуска, если это возможно.\nRestart\n — для применения новых значений ресурсов требуется перезапуск. Примечание: \nRestartNotRequired\n не гарантирует, что контейнер не будет перезапущен. Среда исполнения может остановить контейнер для обновления спецификации ресурсов, если это потребуется.\nПодробности реализации доступны в разделе \nProposal\n KEP’а.\nSupport memory qos with cgroups v2\n#2570\n; \nKEP\ncgroups v1 в Kubernetes поддерживает только ограничение ресурсов CPU с помощью \ncpu_shares\n / \ncpu_set\n / \ncpu_quota\n / \ncpu_period\n. Управление ресурсами памяти не поддерживается. Авторы KEP’а предлагают воспользоваться возможностями контроллера памяти в cgroups v2 для обеспечения QoS и гарантирования запросов/лимитов памяти подов/контейнеров.\nDynamic resource allocation #3063\n#3063\n; \nKEP\nKubernetes все чаще используется в качестве решения для управления новыми рабочими нагрузками (пакетная обработка) и в новых средах (граничные вычисления). Таким рабочим нагрузкам нужны не только оперативная память и процессор, но и доступ к специализированному оборудованию. Усовершенствование инфраструктуры  центров обработки данных позволяет устанавливать ускорители вне конкретных узлов и подключаться к ним динамически по мере необходимости.\nKEP реализует API, позволяющий описывать новые типы ресурсов, необходимые поду. Поддерживаются:\nРесурсы, подключаемые по сети. Существующий API плагинов устройств ограничен аппаратным обеспечением на узле.\nСовместное использование ресурсов несколькими контейнерами или подами. Существующий API диспетчера устройств вообще не поддерживает совместное использование. Его можно дополнить, после чего станет возможен обмен ресурсами между контейнерами в одном поде, но для обмена ресурсами \nмежду подами\n потребуется совершенно новый API, подобный тому, который представлен в данном KEP’е.\nРесурсы, инициализация которых в разных подах обходится слишком дорого (на данный момент оптимизировать этот процесс невозможно).\nКастомные параметры, описывающие требования к ресурсам и их инициализацию. Параметры не всегда бывают линейными и счетными. В текущей реализации API пода для работы с такими параметрами используются аннотации, а для доступа к ним из драйвера CSI или плагина устройства приходится выдумывать различные «костыли».\nНовое оборудование будет поддерживаться дополнениями от его производителей. Больше не нужно будет вносить правки в сам Kubernetes.\nKEP не заменяет другие способы запроса традиционных ресурсов (память/CPU, тома, расширенные ресурсы). Планировщик будет координировать работу дополнений, «владеющих»  ресурсами (драйвер CSI, драйвер ресурсов), и ресурсами, которыми «владеет» и  выделяет планировщик (память/CPU, расширенные ресурсы).\nИдея в том, чтобы отдать все операции, специфичные для ресурса, на откуп драйверу, управляющему этим ресурсом. Сюда входят операции на уровне управляющего слоя (отслеживание того, где в кластере доступны ресурсы, помощь в принятии решений о планировании подов, выделение ресурсов по запросу), а также на уровне узла (подготовка запуска контейнеров). Такой драйвер может быть реализован на произвольном языке программирования — главное, чтобы он поддерживал протокол распределения ресурсов и интерфейсы gRPC, определенные в данном KEP’е. Его развертывание не будет зависеть от изменения конфигурации базовых компонентов Kubernetes, таких как планировщик.\nНовые объекты API будут определять параметры запроса на ресурс (\nResourceClaim\n в API) и отслеживать его состояние. Спецификация пода будет соответствующим образом дополнена. под будет назначаться только на тот узел, на котором есть нужные ресурсы и они зарезервированы для его запросов. Это предотвратит ситуацию, когда под планируется на узел и «застревает» на нем в ожидании доступных ресурсов.\nSupport User Namespaces in stateless pods\n#127\n; \nKEP\nKEP добавляет поддержку пользовательских пространств имен в stateless-подах. \nИз документации к \nuser_namespaces(7)\n:\n«Пользовательские пространства имен изолируют связанные с безопасностью идентификаторы и атрибуты, в частности, идентификаторы пользователей и групп, корневую директорию, ключи и возможности. Идентификаторы пользователя и группы процесса могут быть разными внутри и вне пространства имен пользователя. Процесс может иметь обычный непривилегированный ID пользователя вне пользовательского пространства имен и в то же время ID = 0 внутри пространства имен; другими словами, у процесса будут полные привилегии внутри пользовательского пространства имен и ограниченные — вне его».\nИдея в том, чтобы запускать процессы в подах с другими ID пользователя и группы, нежели на хосте. В этом случае привилегированный процесс в поде будет непривилегированным на хосте. В случае, если такой процесс «вырвется» за пределы контейнера, потенциальный вред будет минимизирован, поскольку на хосте его привилегии будут ограничены.\nKEP полностью или частично устраняет ряд CVE (их список — в разделе \nMotivation\n). В спецификации пода появляется параметр \npod.spec.hostUsers: *bool\n. Если он не определен или true, используется пространство имен хоста (текущее поведение). Если false, для пода создается новое пользовательское пространство имен. По умолчанию параметр не задан.\nExtend PodResources to include resources from Dynamic Resource Allocation (DRA)\n#3695\n; \nKEP\nПредлагается дополнить API PodResources и включить в него ресурсы, распределяемые механизмом \nдинамического распределения ресурсов\n (DRA). Кроме того, предлагается расширить API, чтобы сделать его более дружественным для использования мета-плагинами CNI. Этот KEP развивает идеи в \n2043-pod-resource-concrete-assignments\n и \n2403-pod-resources-allocatable-resources\n.\nНовый API PodResources позволит агентам мониторинга узлов собирать информацию о ресурсах, выделяемых DRA. Мета-плагины CNI, такие как multus и DANM, полагаются на API PodResources при добавления ресурсов, выделяемых плагинами устройств, в качестве аргументов CNI. Данный KEP позволит этим CNI-плагинам ссылаться на ресурсы, выделяемые DRA. Дальнейшее расширение API облегчит мета-плагинам CNI доступ к ресурсам, выделенным конкретному поду — им не придется фильтровать ресурсы для всех подов на узле.\nАвторы предлагают дополнить существующий сервис gRPC PodResources kubelet’а, включив повторяющееся поле \nDynamicResource\n в сообщение \nContainerResources\n. Новое поле будет содержать информацию о классе DRA-ресурса, его ресурсных claim’ах и списке устройств CDI, распределенных драйвером DRA. Кроме того, к существующему сервису gRPC добавится метод \nGet()\n, с помощью которого можно будет опрашивать конкретные поды на предмет выделенных им ресурсов.\nBeta-фичи\nKubelet Evented PLEG for Better Performance\n#3386\nУлучшение модернизирует подход, предложенный в KEP’е \nKubelet: Pod Lifecycle Event Generator (PLEG)\n и меняет механизм работы CRI.\nТеперь kubelet получает актуальные данные о состоянии пода по модели List/Watch. В частности, прослушивает потоковые события сервера gRPC из CRI, которые необходимы для генерации событий жизненного цикла пода — то есть не опрашивает среду исполнения. В результате снижается необязательное использование ресурсов CPU kubelet’ом и нагрузка на CRI. Подробнее — в \nобзоре новинок Kubernetes 1.26\n.\nStable-фичи\nAdd downward API support for hugepages\n#2053\nНисходящий (downward) API позволяет контейнерам получать информацию о себе или кластере без использования клиента Kubernetes или сервера API. Данный KEP открывает контейнерам доступ к информации о hugepages. В спецификацию API добавляются параметры \nrequests.hugepages-<pagesize>\n и \nlimits.hugepages-<pagesize>\n, с которыми можно работать так же, как с аналогичными параметрами для CPU, памяти и эфемерного хранилища.\nKubelet option to enable seccomp by default\n#2413\nSeccomp by default: эта важная и долгожданная фича наконец переходит в статус stable. Она повышает безопасность «ванильного» Kubernetes, автоматически включая фильтрацию системных вызовов при запуске любых контейнеров (в предыдущих версиях K8s это нужно делать вручную). \nSeccomp — режим, при котором все неразрешенные системные вызовы блокируются. Это помогает избегать атак на узел или целый кластер через контейнер, а также повышает устойчивость K8s к zero day-уязвимостям. Подробнее об алгоритме работы новой фичи мы писали \nв этой новости\n.\nNode Topology Manager\n#693\nМенеджер топологии узла (Node Topology Manager) унифицирует подход к тонкой настройке распределения аппаратных ресурсов для различных компонентов в Kubernetes. Впервые он был представлен в далекой v1.16, и вот, наконец, переходит в стабильный статус. Подробнее о нем можно почитать в нашем \nобзоре нововведений Kubernetes 1.16\n.\nAdd gRPC probe to Pod.Spec.Container.{Liveness,Readiness,Startup}Probe\n#2727\nLiveness, Readiness и Startup probes — три разных типа проверки состояния пода. Сейчас они работают по протоколам HTTP(S) и TCP. Новая фича добавляет поддержку gRPC — открытого фреймворка для удаленного вызова процедур, который часто используется в микросервисной архитектуре. \nВозможность использовать встроенный gRPC среди прочего избавляет от необходимости использовать сторонние инструменты для проверки состояния контейнеров вроде grpc_health_probe(1). Подробнее — в \nобзоре Kubernetes 1.23\n.\nAdd configurable grace period to probes\n#2238\nLiveness-проверки используют параметр \nterminationGracePeriodSeconds\n как при обычном прекращении работы, так и при сбоях. Если значение параметра велико и проверка неудачна, kubelet будет ждать указанное (длительное) время, прежде чем перезапустить рабочую нагрузку.\nАвторы KEP’а предлагают добавить в API новое поле, \nprobe.terminationGracePeriodSeconds\n. Оно будет определять период ожидания для liveness- и startup-проверок, переопределяя \nterminationGracePeriodSeconds\n, но игнорироваться для readiness-проверок.\nПриложения\nBeta-фичи\nPodHealthyPolicy for PodDisruptionBudget\n#3017\nKEP добавляет поле \npodHealthyPolicy\n. С его помощью можно указать, что поды «здоровы» и, следовательно, подпадают под ограничения Pod Disruption Budget, либо их следует рассматривать как уже отключенные, и поэтому они могут быть проигнорированы PDB. Новые статусы для подов: \nstatus.currentHealthy\n, \nstatus.desiredHealthy\n и \nspec.unhealthyPodEvictionPolicy\n.\nElastic Indexed Jobs\n#3715\nИндексированные Job’ы появились в Kubernetes 1.21, чтобы облегчить планирование высокопараллелизуемых заданий.\nОднако после создания изменить количество (\nspec.completions\n) или допустимое число параллельных Job’ов (\nspec.parallelism\n) невозможно. Это довольно проблематично для некоторых рабочих нагрузок, например, связанных с глубоким обучением.\nДанный KEP делает эти поля (\nspec.completions\n и \nspec.parallelism\n) изменяемыми (mutable), но с некоторыми ограничениями (они должны быть равны).\nAllow StatefulSet to control start replica ordinal numbering\n#3335\nЦель KEP’а — разрешить миграцию StatefulSet’а между пространствами имен, между кластерами, а также разбивать его на сегменты без простоев в работе приложения.\nВ манифест StatefulSet’а добавляется новое поле \nspec.ordinals.start\n, в котором указывается стартовый номер для реплик, контролируемых StatefulSet’ом. Подробнее — в нашем \nобзоре нововведений Kubernetes 1.26\n.\nRetriable and non-retriable Pod failures for Jobs\n#3329\nЧтобы ограничить время выполнения Job’а, используются два параметра: \nactiveDeadlineSeconds\n — максимальное время выполнения Job’а;\nbackoffLimit\n — количество попыток запуска Job’а за отведенное (в предыдущем параметре) время.\nЕсли Job не запускается, и политика перезапуска установлена в \nOnFailure\n, Kubernetes попытается запустить его заново — столько раз, сколько указано в \nbackoffLimit\n.\nKEP помогает учитывать некоторые причины незапуска Job’а и, при необходимости, завершать его досрочно, игнорируя \nbackoffLimit\n. Для этого в Job API добавлено новое поле \npodFailurePolicy\n. Дополнительная информация и пример конфигурации с использованием \npodFailurePolicy\n — в \nобзоре новинок K8s 1.25\n.\nAuto remove PVCs created by StatefulSet\n#1847\nKEP вводит необязательное поле \n.spec.persistentVolumeClaimRetentionPolicy\n для контроля за тем, когда и как PVC удаляются во время жизненного цикла StatefulSet’а.\nStable-фичи\nTimeZone support in CronJob\n#3140\nCronJob создает Job’ы в соответствии с графиком, который определен пользователем. Однако часовой пояс, используемый в процессе создания Job’а, соответствует часовому поясу kube-controller-manager’а. Если пользователь забыл это учесть, Job может быть выполнен не по плану.\nKEP добавляет в CronJob API новое поле \n.spec.timeZone\n, в котором можно заранее установить часовой пояс для запуска Job’а.\nХранилище\nAlpha-фичи\nVolumeGroupSnapshot\n#3476\n; \nKEP\nKEP реализует API Kubernetes, позволяющий снимать согласованные snapshot’ы сразу с нескольких томов. Для группировки PVC используются селекторы меток. В рамках KEP 3476 добавляется поддержка snapshot’ов групп томов для драйверов томов CSI.\nСуществующий API VolumeSnapshot позволяет делать snapshot’ы томов для защиты от потери или повреждения данных. Однако часть потребностей им не покрывается. Некоторые системы хранения поддерживают согласованный snapshot, т.е. снимок с нескольких томов в один и тот же момент времени для соблюдения согласованности порядка записи. Такая функция может пригодиться для приложений, работающих сразу с несколькими томами. Например, данные такого приложения могут храниться на одном томе, логи — на другом. Если snapshot’ы с этих томов снимать в разное время, информация на них окажется рассогласована. Соответственно, приложение не сможет нормально функционировать, если эти snapshot’ы использовать для восстановления после сбоя.\nДа, можно предварительно «заморозить» (quiesce) приложение, однако это требует дополнительного времени и не всегда возможно. Кроме того, последовательное снятие snapshot’ов также может занять больше времени, нежели согласованное групповое.\nKEP вводит новые CRD \nVolumeGroupSnapshot\n, \nVolumeGroupSnapshotContent\n и \nVolumeGroupSnapshotClass\n. \nДля объединения нескольких PVC в \nVolumeGroupSnapshot\n на них навешивается соответствующая метка, которая затем указывается в селекторе \nlabelSelector\n в \nVolumeGroupSnapshot\n, если драйвер CSI поддерживает возможность \nCREATE_DELETE_GET_VOLUME_GROUP_SNAPSHOT\n. Подробнее о логике работы можно почитать в разделе \nProposal\n KEP’а. \nBeta-фичи\nReadWriteOncePod PersistentVolume Access Mode\n#2485\nВ Kubernetes нет режима доступа для PersistentVolumes, который позволяет ограничить доступ так, чтобы на одном узле мог работать только один такой под. Это чревато проблемами для некоторых рабочих нагрузок. Пример: некая рабочая нагрузка выполняет обновление устройства хранения (с помощью ReadWriteOnce) и масштабируется на несколько подов. В этом случае второй под может оказаться на том же узле и начать работать с устройством хранения параллельно с первым.\nВ случае критичных рабочих нагрузок отсутствие режима, который позволяет ограничить доступ так, чтобы на одном узле мог работать только один под, приходится обходить другими способами (например, планируя лишь один под на узел и используя ReadWriteOnce), что может приводить к неэффективному использованию ресурсов в кластере.\nKEP вводит новых режим доступа \nReadWriteOncePod\n, который позволяет ограничить доступ к PersistentVolumes одиночным подом на одиночном узле. Для сравнения, существующий режим ReadWriteOnce (RWO) ограничивает доступ к одиночному узлу, но разрешает одновременный доступ к этому узлу из нескольких подов.\nIntroduce nodeExpandSecret in CSI PV source\n#3107\nДля передачи секретов в RPC-запросах к CSI-драйверам при расширении емкости томов на узле с помощью операции \nnodeExpandVolume.\n В запрос \nnodeExpandVolume\n, который kubelet отправляет CSI-драйверу, добавлено новое поле \nsecretRef\n.\nКроме того, в объект \nCSIPersistentVolumeSource\n добавлен параметр \nNodeExpandSecretRef\n для передачи секретов в PVC-запросах на расширение тома. Подробнее — в \nобзоре нововведений Kubernetes 1.25\n.\nPrevent unauthorised volume mode conversion during volume restore\n#3141\nС помощью API-ресурса VolumeSnapshot можно создавать PVC (PersistentVolumeClaim) из снапшота. Это делается через установку в PVC параметра \nSpec.dataSource\n, который указывает на существующий ​​VolumeSnapshot. При этом нет никакого механизма проверки, соответствует ли режим доступа у вновь созданного PVC режиму доступа, заданному для исходного PVC.\nЧтобы улучшить контроль за режимом доступа к тому, авторы KEP’а предлагают следующие изменения:\nдобавить новое поле \nSourceVolumeMode\nв спецификации API \nVolumeSnapshotContent\n; в нем устанавливается режим доступа к тому, с которого сделан снапшот;\nдобавить аннотацию со ссылкой на ресурс, который может использовать доверенный пользователь для VolumeSnapshot;\nусовершенствовать методы контроля за snapshot-controller и external-provisioner.\nSpeed up recursive SELinux label change\n#1710\nНа хостах с SELinux в принудительном режиме (enforcing mode) пользователи одного контейнера не могут получить доступ к другому контейнеру или к хосту. Это обеспечивается за счет контекста, который уникален для каждого контейнера, и лейблов, которые назначаются каждому файлу в каждом томе. Злоумышленник, которому удалось выбраться из контейнера, не сможет получить доступ к данным в других контейнерах, потому что у каждого контейнера свои тома со своими лейблами.\nОднако такая защита может усложнять жизнь пользователям, которые разворачивают кластеры на базе SELinux: при запуске нового контейнера запускается рекурсивное переназначение лейблов для всех файлов в томе, привязанном к поду. Если том большой, процесс переназначения может затянуться.\nНовая функция позволяет пропустить этап переназначения лейблов и тем самым ускорить монтирование тома. Подробности см. в \nобзоре Kubernetes 1.25\n.\nRobust VolumeManager reconstruction after kubelet restart\n#3756\nЭтот KEP является частью предыдущего, #1710, поэтому сразу позиционируется как бета. Его цель — уменьшить время, необходимое kubelet’у для восстановления информации о примонтированных томах после перезапуска.\nТакая информация теряется после перезапуска kubelet’а, и для ее восстановления ему приходится сопоставлять данные сервера API и ОС хоста. Kubelet проверяет, какие поды должны быть запущены и какие тома примонтированы на самом деле.\nОднако этот процесс несовершенен и лишен некоторой ключевой информации, например, о том, какие опции для монтирования томов использовал прежний kubelet.\nKEP #3756 является своего рода работой над ошибками и предусматривает значительный рефакторинг кода. Именно поэтому он выделен из «родительского» KEP’а и снабжен отдельным переключателем функциональности (feature flag) \nNewVolumeManagerReconstruction\n. \nПланировщик\nBeta-фичи\nPod Scheduling Readiness\n#3521\nПоды считаются готовыми к планированию сразу после того, как созданы. Планировщик Kubernetes ищет узлы для размещения всех ожидающих подов. Однако в реальности некоторые поды могут долго оставаться в состоянии \nmiss-essential-resources\n («отсутствие необходимых ресурсов»). Эти поды «сбивают с толку» планировщик и компоненты типа Cluster Autoscaler.\nВ улучшении дорабатывается API и механика — так, чтобы пользователи или оркестраторы понимали, когда под полностью готов для планирования. В Pod API добавляется поле \n.spec.schedulingGates\n со значением по умолчанию \nnil\n. Поды, у которых значение этого поля не равно \nnil\n, будут «припаркованы» во внутреннем пуле \nunschedulablePods\n планировщика. Он обработает их, когда поле изменится на \nnil\n.\nMutable Pod scheduling directives when gated\n#3838\nKEP \n#3521 Pod scheduling readiness\n позволяет помечать поды как готовые к планированию. Данный KEP развивает эту идею, делая переключатели планирования (scheduling gates) изменяемыми, чтобы другие компоненты, например, кастомные контроллеры планирования, могли использовать их для расширения возможностей kube-scheduler’а. См. KEP #3521 выше.\nRespect PodTopologySpread after rolling upgrades\n#3243\nKEP решает проблему с несбалансированным распределением подов при плавающих обновлениях Deployment’а.\nВ \nTopologySpreadConstraint\n вводится новое поле \nMatchLabelKeys\n в качестве дополнения к существующему \nlabelSelector\n. В новом поле передается набор ключей для лейблов подов — их планировщик учитывает при распределении.\nФича позволяет применять ограничения \nPodTopologySpread\n только в границах новой ревизии Depolyment’а, а не во всех ревизиях.\nStable-фичи\nMutable scheduling directives for suspended Jobs\n#2926\nНачиная с Kubernetes 1.23, появилась возможность обновлять поля \nnode affinity\n, \nnode selector\n, \ntolerations\n, \nlabels\n и \nannotations\n в шаблоне пода Job’а перед его запуском. Это позволяет контролировать, где будут запущены поды, например, в одной зоне или на узлах с одинаковой моделью GPU.\nСеть\nAlpha-фичи\nCloud Dual-Stack --node-ip Handling\n#3705\n; \nKEP\nkubelet поддерживает dual-stack-значения \n--node-ip\n для необлачных кластеров (например, bare-metal-кластеров), но не для их облачных аналогов. Данный KEP исправляет это.\nВ настоящее время при передаче параметра \n--cloud-provider\n в kubelet последний ожидает, что \n--node-ip\n не будет задан или будет представлять собой один IP-адрес. Если сразу указаны и \n--cloud-provider\n, и \n--node-ip\n (при этом \n--node-ip\n отличен от \n0.0.0.0\n или \n::\n), kubelet добавит к узлу аннотацию \nalpha.kubernetes.io/provided-node-ip\n. Облачные провайдеры ожидают, что эта аннотация будет соответствовать установленному синтаксису \n--node-ip\n (т.е. значение будет единственным); в ином случае они выдадут ошибку и не уберут с узла taint \nnode.cloudprovider.kubernetes.io/uninitialized\n, в результате чего тот окажется непригодным для использования до перезапуска kubelet’а с действительным (или пустым) \n--node-ip\n.\nKEP позволит администраторам кластеров, в которых используются внешние облачные ресурсы, гибко менять IP-адреса узлов, назначаемые облаком. Администраторы смогут:\nпереопределять IP-адрес узла в dual-stack-кластере, и это не будет приводить к тому, что узел станет single-stack;\nпереопределять оба IP-адреса узла в dual-stack-кластере;\nизменять порядок IP-адресов узлов в dual-stack-кластере (т.е. задавать первичность IPv6 или IPv4);\nпереключать узлы в single-stack-режим, когда поставщик облачных услуг предлагает dual-stack-режим, без необходимости указывать конкретный IP-адрес узла в формате IPv4 или IPv6.\nReserve nodeport ranges for dynamic and static allocation\n#3668\n; \nKEP\nЭтот KEP развивает идею \nKEP 3070\n (см. \nобзор нововведений Kubernetes 1.24\n), помогая избежать конфликтов при резервировании портов для сервисов типа NodePort.\nСервис Kubernetes — это абстрактный способ открыть для внешнего трафика приложение, работающее на некотором наборе подов. У сервиса есть виртуальный ClusterIP, который позволяет балансировать трафик между подами. ClusterIP может назначаться:\nдинамически: кластер выбирает один адрес из указанного диапазона сервисных IP-адресов.\nстатически: пользователь задает один адрес в пределах указанного диапазона сервисных IP-адресов.\nВ настоящее время, создавая сервис со статическим ClusterIP, невозможно узнать, не выбрал ли этот адрес какой-либо другой сервис динамически.\nДиапазон IP-адресов сервисов можно логически разделить, избежав тем самым риска конфликтов между сервисами, использующими статическое и динамическое распределение IP-адресов.\nПри создании сервиса типа NodePort kube-proxy выделяет порт в диапазоне 30000-32767 и открывает этот порт на интерфейсе eth0 каждого узла. Подключения к этому порту затем перенаправляются на IP-адрес сервиса в кластере. KEP 3668 позволяет зарезервировать диапазон портов за определенным сервисом. \nMultiple Service CIDRs\n#1880\n; \nKEP\nПозволяет динамически увеличивать количество IP-адресов, доступных для сервисов.\nНекоторые типы сервисов — ClusterIP, NodePort и LoadBalancer — используют виртуальный IP-адрес ClusterIP, который должен быть уникальным в пределах кластера. Попытка создать сервис с адресом ClusterIP, который уже занят, приведет к ошибке.\nТекущая реализация логики распределения IP-адресов для сервисов имеет несколько ограничений:\nНельзя менять размер или увеличивать диапазоны IP, выделяемых сервисам; это приводит к проблемам, когда сети перекрываются или в кластере заканчиваются доступные IP.\nДиапазон IP-адресов сервисов не разглашается (not exposed), поэтому другие компоненты в кластере не могут его использовать.\nКонфигурация привязана к конкретному серверу API; консенсус отсутствует; в результате разные серверы API могут конкурировать между собой, удаляя IP-адреса, закрепленные друг за другом.\nДля IPv4 размер префикса для сервиса ограничен /12, однако для IPv6 он составляет /112 или меньше. Это вызывает проблемы у пользователей IPv6, поскольку /64 является стандартной и минимально рекомендуемой длиной префикса.\nFeature gate 3070 позволяет использовать только зарезервированный диапазон IP-адресов сервиса.\nДанный KEP ставит своей целью реализацию новой логики механизма распределения. Предлагается создать два объекта API — \nServiceCIDR\n и \nIPAddress\n — и позволить пользователям динамически увеличивать количество доступных для сервисов IP-адресов, задавая новые диапазоны ServiceCIDR.\nBeta-фичи\nCleaning up IPTables Chain Ownership\n#3178\nKEP оптимизирует работу с цепочками iptables, создаваемыми kubelet’ом. Опция IPTablesOwnershipCleanup запрещает kubelet’у создавать цепочки \nKUBE-MARK-DROP\n, \nKUBE-MARK-MASQ\n, \nKUBE-POSTROUTING\n и \nKUBE-KUBELET-CANARY\n (остается лишь \nKUBE-FIREWALL\n для обработки \nмарсианских пакетов\n). Кроме того, она предупреждает, что аргументы kubelet’а \n--iptables-masquerade-bit\n и \n--iptables-drop-bit\n устарели и больше не работают.\nMinimizing iptables-restore input size\n#3453\nKEP повышает производительность работы kube-proxy в режиме iptables, убирая из вызовов к \niptables-restore\n правила, которые не изменились (подробнее — в \nобзоре нововведений Kubernetes 1.26\n).\nRemove transient node predicates from KCCM's service controller\n#3458\nКонтроллер сервисов в Kubernetes cloud controller manager (KCCM) добавляет/удаляет узлы из набора узлов балансировщиков нагрузки в следующих случаях:\nКогда на узел навешивается/с узла удаляется taint \nToBeDeletedByClusterAutoscaler\n; \nКогда узел переходит в состояние Ready/NotReady.\nОднако второй случай относится только к сервисам с \nexternalTrafficPolicy: Cluster\n. При этом в обоих случаях удаление узла из набора узлов балансировщиков нагрузки приведет к тому, что все соединения на этом узле будут мгновенно прерваны. Подобное поведение представляется неоптимальным для узлов в переходном состоянии готовности или завершающих свою работу и выглядит как ошибка, поскольку в таких случаях соединениям не разрешается разрываться, даже если балансировщик нагрузки поддерживает это. \nДанный KEP предусматривает прекращение синхронизации набора узлов балансировщиков нагрузки в таких случаях. Для удобства в систему будет интегрирован feature gate \nStableLoadBalancerNodeSet\n.\nAPI\nAlpha-фичи\nCEL-based admission webhook match conditions\n#3716\n; \nKEP\nKEP добавляет так называемые «условия соответствия» (match conditions) к admission-вебхукам как расширение существующих правил для определения области применения вебхука. matchCondition представляет собой CEL-выражение. Если оно истинно, admission-запрос будет отправлен на вебхук. Если оно ложно, вебхук запрос пропустит (неявно разрешено).\nАвторы KEP’а считают, что предложенный механизм позволит оптимизировать работу с вебхуками в следующих областях:\nНадежность\n. Для многих пользователей Kubernetes admission-вебхуки остаются больным местом. Выход вебхука из строя может оказать значительное влияние на доступность кластера. Данное предложение смягчает (но не устраняет) эти проблемы, делая вебхуки более узконаправленными и целевыми.\nПроизводительность\n. Admission-вебхуки критически важны для write-запросов. Проверочные (validating) вебхуки могут выполняться параллельно, но мутирующие (mutating) веб хуки должны выполняться последовательно (до 2 раз!). Это делает их чрезвычайно чувствительными к задержкам, и даже те вебхуки, которые не выполняет никакой работы, все равно несут процесс из-за  круговых задержек (RTT).\nПоддерживаемость\n. В случае hosted- или managed-дистрибутивов Kubernetes вебхуки могут мешать запросам managed-компонентов. Существующие критерии для фильтрации запросов для многих случаев недостаточны, и их нелегко дополнить правилами провайдера.\nKEP добавляет поле \nMatchConditions\n в объекты \nValidatingWebhook\n и \nMutatingWebhook\n (в admissionregistration.k8s.io). Оценка условия  \nMatchCondition\n выполняется теми же библиотеками, которые используются для CEL \nValidatingAdmissionPolicy\n. Единственное различие в выражениях заключается в наличии переменной \nparams\n. Выражения, требующие доступа к дополнительной информации вне \nAdmissionRequest\n, должны выполняться в вебхуке и выходят за рамки данного предложения.\nCEL for Admission Control\n#3488\n; \nKEP\nKEP реализует настраиваемую in-process-валидацию запросов к API-серверу Kubernetes как альтернативу для проверочных (validating) admission-вебхуков. Предложение базируется на возможностях \nCRD Validation Rules\n (в beta, начиная с v1.25), но с акцентом на возможности применения политик при валидации контроля допуска.\nKEP снижает инфраструктурный барьер для внедрения кастомизируемых политик и реализует примитивы, способствующие применению лучших практик при работе как с K8s, так и его расширениями.\nВ настоящее время кастомные политики реализуются с помощью admission-вебхуков. Эти хуки отличаются высокой гибкостью, но имеют ряд недостатков по сравнению с применением политик «в процессе»:\nТребуют создания инфраструктуры для размещения admission-вебхуков.\nУвеличивают задержку.\nИз-за дополнительных инфраструктурных зависимостей вебхуки менее надежны, чем их in-process-аналоги. \nУправление вебхуками обременительно для администраторов кластера. Приходится заботиться о наблюдаемости и безопасности, а также формировать план выпуска/развертывания/отката вебхуков.\nKEP вводит новый kind \nValidatingAdmissionPolicy\n в группу \nadmissionregistration.k8s.io\n.\nНа высоком уровне API будет поддерживать:\nСопоставление запросов (по типу правил сопоставления для admission-вебхуков, с учетом RBAC, priority & fairness и аудита).\nОценку правил CEL (аналогично правилам проверки CRD, но с доступом к данным в AdmissionRequest).\nПреобразование версий (аналогично MatchPolicy admission-вебхуков).\nДоступ к старому объекту (аналогично правилам перехода и oldObject в AdmissionRequest)\nКонфигурируемость.\nРяд других возможностей.\nKEP разделен на два этапа, каждый из которых должен быть завершен до того, как фича перейдет в beta-версию. Сделано это для того, чтобы работу над каждым из этапов можно было завершить в течение одного релиз-цикла Kubernetes.\nAllow informers for getting a stream of data instead of chunking\n#3157\n; \nKEP\nВ некоторых случаях API-сервер Kubernetes страдает от взрывного роста потребления памяти. Эта проблема особенно очевидна в больших кластерах, где всего несколько LIST-запросов могут привести к серьезным сбоям. Неконтролируемое и неограниченное потребление памяти серверами влияет не только на кластеры, работающие в режиме HA, но и на другие программы на узле.\nKEP реализует запросы WATCH в качестве альтернативы запросам LIST. Чтобы снизить потребление памяти при получении списка данных и сделать его более предсказуемым, авторы предлагают использовать потоковую передачу из watch-cache вместо подкачки из etcd. В первую очередь изменения затронут информеры, поскольку они обычно являются наиболее активными пользователями LIST-запросов. Основная идея в том, чтобы использовать стандартную механику WATCH-запросов для получения потока отдельных объектов, но применять ее для LIST'ов. Подробнее — в \nописании KEP’а\n.\nBeta-фичи\nAggregated Discovery\n#3352\nОб операциях, которые поддерживает сервер API Kubernetes, сообщается через набор небольших документов, разделенных по группам в соответствии с номером версии. Клиенты API Kubernetes (например, kubectl) вынуждены отправлять запросы в каждую группу в «поисках» доступных API. Результат: множество запросов к кластерам, задержки и троттлинг. Когда в API добавляются новые типы, их нужно извлекать заново, что приводит к дополнительному потоку запросов. \nKEP предлагает централизовать механизм «обнаружения» в двух агрегированных документах, чтобы клиентам не нужно было отправлять множество запросов API-серверу для извлечения всех доступных операций.\nCRD Validation Expression Language\n#2876\nKEP предлагает упрощенный вариант проверки пользовательских ресурсов (custom resources), которые добавляются в CRD (Custom Resource Definition). Ранее для этого использовались только validation-вебхуки, в которых прописывались правила проверки. Такой механизм усложнял разработку и мог влиять на работоспособность CRD. Теперь правила можно писать на скриптовом языке Common Expression Language (CEL) прямо в схемах ​​\nCustomResourceDefinition\n с помощью \nx-kubernetes-validations extension\n (подробности и пример правил — в \nобзоре Kubernetes 1.23\n).\nStable-фичи\nServer Side Unknown Field Validation\n#2885\nKEP перекладывает функцию проверки неизвестных и дублирующих полей в запросах к API-серверу с клиентской части (пользователя) на серверную. Мотив: проверку на стороне клиента организовать \nзначительно сложнее\n, чем на стороне сервера.\nOpenAPI v3\n#2896\nKEP добавляет kube-apiserver’у возможность обслуживать ресурсы и типы Kubernetes как объекты OpenAPI v3. Ранее конвертация определений OpenAPI v3 проводилась через CRD; делалось это не вполне корректно.\nCLI\nAlpha-фичи\nImprove kubectl plugin resolution for non-shadowing subcommands\n#3638\n; \nKEP\nKEP позволяет использовать внешние плагины в качестве подкоманд встроенных команд, если подкоманда не существует.\nМногие пользователи Kubernetes желали бы дополнить некоторые встроенные команды kubectl новыми подкомандами. К примеру, \ncreate\n (по состоянию на v1.25) поддерживает создание 18 ресурсов в рамках простой команды (например, \nkubectl create job\n), при этом периодически возникают запросы на включение новых ресурсов. Это ожидаемо, учитывая популярность CustomResourceDefinitions, но нецелесообразно с точки зрения усилий, которые затрачиваются на поддержание всей этой функциональности.\nKEP предлагает плагины как альтернативу для подкоманд, встраиваемых в kubectl.\nВ этом случае при попытке выполнить неизвестную подкоманду kubectl будет искать ее во внешних плагинах со структурой имени, аналогичной текущему механизму поиска плагинов (например, \nkubectl-create-foo\n). Пример:\n$ kubectl create foo\n(выполняется плагин kubectl-create-foo)\nАктивировать функциональность, предлагаемую KEP’ом, можно с помощью переменной окружения:\n$ export KUBECTL_ENABLE_CMD_SHADOW=true\nApplySet : kubectl apply --prune redesign and graduation strategy\n#3659\n; \nKEP\nКоманда \nkubectl apply\n позволяет создавать или обновлять объекты из YAML-файла. C помощью флага \n--prune\n можно удалить объекты, которых больше нет в файле. Однако этот процесс несовершенен и может приводить к непредсказуемому поведению.\nЭтот KEP пытается заменить флаг \n--prune\n на новый, более перспективный и надежный подход. Пользователи смогу объединяться объекты в группы ApplySet, помогая kubectl понять, какие именно объекты подлежат удалению.\nДля этого на объекты будут навешиваться лейблы с префиксом \napplyset.k8s.io\n. Например, лейбл \napplyset.k8s.io/part-of\n будет указывать, частью какого ApplySet’а является объект. С подробностями можно ознакомиться в \nKEP’е\n.\nBeta-фичи\nAdd subresource support to kubectl\n#2590\nПроцесс извлечения подресурсов API-объектов \nstatus\n и \nscale\n через kubectl не очень удобен: приходится пользоваться \nkubectl --raw\n. А обновлять эти подресурсы можно только при помощи запросов к API через curl (или другой клиент). Это усложняет их тестирование и отладку.\nKEP добавляет новый флаг \n--subresource=[subresource-name]\n к командам \nkubectl get\n и \nkubectl patch\n. Это позволит запрашивать и обновлять подресурсы \nstatus\n, \nscale\n и др., оперируя командами kubectl. Дополнительная информация доступна в \nобзоре нововведений Kubernetes 1.24\n.\nOpenAPI v3 for kubectl explain\n#3515\nOpen API v3 расширяет возможности API Kubernetes: пользователи могут получить доступ к таким атрибутам как \nnullable\n, \ndefault\n, полям валидации \noneOf\n, \nanyOf\n и так далее. Поддержка Open API v3 в Kubernetes достигла бета-версии в 1.24.\nСейчас CRD определяют свои схемы в формате OpenAPI v3. Для обслуживания документа /openapi/v2, который использует kubectl, приходится преобразовывать формат v3 в v2. Это приводит к ошибкам, например, при попытке использовать kubectl explain c полями типа nullable.\nKEP вносит следующие усовершенствования в \nkubectl explain\n:\nисточник данных меняется с OpenAPI v2 на Open API v3;\nподдерживаются разные форматы вывода, в том числе простой текст и markdown. \nStable-фичи\nDefault container annotation that to be used by kubectl\n#2227\nВ поды была добавлена аннотация \nkubectl.kubernetes.io/default-container\n для определения контейнера по умолчанию.\nЭто упрощает использование таких инструментов, как \nkubectl logs\n или \nkubectl exec\n на подах с sidecar-контейнерами.\nРазное\nAlpha-фичи\nNode log query\n#2258\n; \nKEP\nЧтобы посмотреть логи сервисов на узле, например, containerd, kubelet’а и т.п., администратор кластера Kubernetes должен подключиться к узлу по SSH и смотреть файлы логов напрямую. Более простым и элегантным способом было бы задействовать CLI kubectl для просмотра логов на узле по аналогии с тем, как это делается при других взаимодействиях с кластером.\nKEP предлагает команду \nkubectl node-logs\n для работы с объектами узлов, а также клиент для работы с просмотрщиком эндпоинта \n/var/log/\n kubelet’а. Дополнительную информацию о специфике реализации для разных ОС можно узнать из раздела \nProposal\n KEP’а. \nKEP for adding webhook hosting capability to the CCM framework\n#2699\n; \nKEP\nKEP подробно описывает усовершенствование структуры CCM с целью обеспечить поддержку вебхуков, специфичных для облачных провайдеров. Цель состоит в том, чтобы упростить генерацию исполняемого кода или усовершенствовать CCM, чтобы тот поддерживал такие кастомные вебхуки.\nCloud Controller Manager (CCM) — сервис, который содержит специфические для облака контроллеры, необходимые для  правильной работы кластера Kubernetes. В некоторых случаях желательно, чтобы все эти кастомные ресурсы применялись синхронно, во время обработки запросов API-сервера, а не асинхронно после того, как изменения уже были применены.\nPublishing Kubernetes packages on community infrastructure\n#1731\n; \nKEP\nВ KEP’е подробно описаны усилия и текущие изменения, внесенные в релизный процесс и инфраструктуру сборки пакетов. Конечная цель — перейти от принадлежащего Google репозитория пакетов к управляемому сообществом репозиторию Open Build Service (OBS).\nBeta-фичи\nKMS v2 Improvements\n#3299\nKEP направлен на улучшение сервиса \nKey Management Service\n (KMS), который используется для шифрования данных etcd по схеме envelope encryption. Глобальная цель фичи — упростить работу с KMS. Дополнительная информация — в \nобзоре Kubernetes 1.25\n.\nReduction of Secret-based Service Account Tokens\n#2799\nKEP избавляет от необходимости использовать токен из секрета с учетными данными для доступа к API. Данные передаются напрямую из TokenRequest API и монтируются в под с помощью защищенного тома.\nAPI Server Tracing\n#647\nKEP расширят возможности трассировки API-сервера с помощью стандарта распределенного трейсинга и мониторинга OpenTelemetry. Коллектор OpenTelemetry умеет собирать метрики и трейсы из множества источников и транслировать их в нужный пользователю формат: Prometheus, Jaeger, VictoriaMetrics и т. Д.\nKubernetes Component Health SLIs\n#3466\nKEP позволяет отправлять данные SLI в структурированном виде и последовательно — так, чтобы агенты мониторинга могли их использовать с более высокими интервалами очистки и создавать SLO и алерты на основе этих SLI.\nВ компоненты Kubernetes добавляется новый endpoint \n/metrics/sli\n, который возвращает данные SLI в формате Prometheus. Kubernetes получает стандартный формат для запроса данных о состоянии его компонентов (бинарников и т. п.). Вместе с этим отпадает необходимость в Prometheus exporter’е. Подробнее — в \nобзоре Kubernetes 1.26\n.\nExtend metrics stability\n#3498\nПорядок стабилизации метрик\n первоначально был введен для защиты значимых метрик от проблем при использовании в downstream’е. Метрики могут быть \nalpha\n или \nstable\n. Гарантировано стабильны только stable-метрики.\nKEP вводит дополнительные классы стабильности — в первую очередь, чтобы синхронизировать этапы стабилизации метрик с этапами стабилизации релизов Kubernetes.\nК полям метрик добавляются статусы \nInternal\n (метрики для внутреннего использования) и \nBeta\n (более зрелая стадия метрики с бóльшими гарантиями стабильности, нежели alpha или internal, но менее стабильная, чем stable).\nAuto-refreshing Official CVE Feed\n#3203\nАвторы KEP’а предлагают создать автоматически обновляемый список Kubernetes CVEs и разместить его по адресу https://kubernetes.io/docs/reference/issues-security/official-cve-feed со ссылкой на странице \nKubernetes Security and Disclosure Information\n. Список будет пополняться issues, которым присвоен лейбл \nofficial-cve-feed\n.\nContainer Resource based Pod Autoscaling\n#1610\nВ настоящее время Horizontal Pod Autoscaler (HPA) может масштабировать рабочие нагрузки на основе ресурсов, используемых их подами, т.е. учитывается совокупное использование ресурсов всеми контейнерами пода.\nДанный KEP позволяет HPA масштабировать рабочие нагрузки на основе использования ресурсов отдельными контейнерами.\nStable-фичи\nExpose metrics about resource requests and limits that represent the pod model\n#1748\nKEP создает «из коробки» достаточное количество метрик, необходимых для планирования ресурсов кластера и простого представления модели ресурсов Kubernetes. Для её реализации в kube-scheduler появились новые метрики по запрашиваемым ресурсам (\nkube_pod_resource_requests\n) и желаемым лимитам (\nkube_pod_resource_limits\n) для всех запущенных подов.\nFreeze k8s.gcr.io image registry\n#3720\nВ прошлом году с выходом версии Kubernetes 1.25 проект перешел на свой полноценный реестр registry.k8s.io, однако значительная часть трафика по-прежнему была направлена на предыдущий эндпоинт k8s.gcr.io. Переход данного KEP’а на stable-стадию знаменует окончание затянувшегося переезда на новый реестр.\nStay on supported Go versions\n#3744\nKubernetes написан на Go. В рамках минорных версий его разработчики стараются не менять версию Go, чтобы избежать дестабилизирующих изменений. Это означает, что к концу жизненного цикла конкретный релиз может сильно отстать от исправлений безопасности, реализованных в Go.\nKEP описывает процесс обновления поддерживаемых ветвей Kubernetes минорных версий release-1.x для сборки и выпуска с новыми минорными версиями Go, с упором на сохранение нормальной работоспособности и совместимости патч-релизов Kubernetes.\nСписок устаревших или удаленных фичей\nНиже приведен список переключателей функциональности, флагов и т.п., которые признаны устаревшими в Kubernetes 1.27:\nУстаревшие версии API:\nKubeadm v1beta2; миграцию на v1beta3 можно провести с помощью \nkubeadm config migrate\n.\nresource.k8s.io/v1alpha1.PodScheduling: переход на resource.k8s.io/v1alpha2.PodSchedulingContext.\nDynamicResourceManagement v1alpha1: переход на v1alpha2.\nCSIStorageCapacity: Storage.k8s.io/v1beta1: переход на v1.\nУстаревшие примитивы — перейти на альтернативу до выхода следующего релиза:\nАннотации seccomp.security.alpha.kubernetes.io/pod и container.seccomp.security.alpha.kubernetes.io: вместо них используйте поле securityContext.seccompProfile.\nПлагин допуска SecurityContextDeny.\nАннотация service.kubernetes.io/topology-aware-hints: переход на service.kubernetes.io/topology-mode.\nУдаленные примитивы — перейти на альтернативу до обновления:\nВместо k8s.gcr.io используйте registry.k8s.io.\nПереключатели функциональности:\nIPv6DualStack\nExpandCSIVolumes\nExpandInUsePersistentVolumes\nExpandPersistentVolumes\nControllerManagerLeaderMigration\nМиграция CSI\nCSIInlineVolume\nEphemeralContainers\nLocalStorageCapacityIsolation\nNetworkPolicyEndPort\nStatefulSetMinReadySeconds\nIdentifyPodOS\nDaemonSetUpdateSurge\nappProtocol: kubernetes.io/grpc.\nФлаг kube-apiserver’а: \n--master-service-namespace\n.\nФлаги CLI: \n--enable-taint-manager\n и \n--pod-eviction-timeout\n.\nФлаги kubelet’а: \n--container-runtime\n, \n--master-service-namespace\n.\nIn-tree-плагин дискового хранилища Azure.\nПоставщик учетных данных kubelet’а AWS: используйте ecr-credential-provider.\nМетрики:\nnode_collector_evictions_number заменена на node_collector_evictions_total\nscheduler_e2e_scheduling_duration_seconds заменена на scheduler_scheduling_attempt_duration_seconds\nДругие изменения\nСписок изменений, под которые необходимо адаптировать конфигурации:\nkubelet: \n--container-runtime-endpoint\n и \n--image-service-endpoint\n перенесены в kubelet config.\nИмена StatefulSet’ов должны быть DNS метками, а не поддоменами.\nПоле resourceClaims было изменено с \nset\n на \nmap\n.\nПлагин NodeAffinity Filter: фильтр не запускается, если PreFilter возвращает Skip.\nПланировщик: больше не выполняет определенные методы, когда они не нужны:\nФильтр \nNodeAffinity\n;\nФильтр \nInterPodAffinity\n.\nScore\nresource.k8s.io/v1alpha1/ResourceClaim теперь отклоняет повторно используемые UID.\nkubelet: больше не создает некоторые legacy-правила iptables по умолчанию.\nkubelet: значение по умолчанию для MemoryThrottlingFactor теперь равно 0.9.\nAPI пода: поле \n.spec.schedulingGates[].name\n \nтребует квалифицированных имен. Правила проверки для него теперь такие же, как для\n \n.spec.readinessGates[].name\n.\nPodSpec: отклоняет недопустимые имена ResourceClaim и ResourceClaimTemplate.\nAPI resource.k8s.io: несовместимое изменение в структуре AllocationResult.\nПеред обновлением рекомендуем ознакомиться с \nCHANGELOG’ом\n.\nP.S.\nЧитайте также в нашем блоге:\n«\nKubernetes 1.26: обзор нововведений, включая первый KEP „Фланта“\n»\n;\n«Kubernetes 1.25: обзор нововведений»\n;\n«Kubernetes 1.24: обзор нововведений»\n.\n \n ",
    "tags": [
        "kubernetes"
    ]
}