{
    "article_id": "726998",
    "article_name": "Прыжок сквозь время или Как обновиться с Asterisk 11 до 18",
    "content": "Однажды мы в Интерсвязи решили обновить Asterisk с 11 версии до 18. История получилась интересной и поучительной. Меня зовут Глеб, я инженер группы управления телефонией оператора связи. И вот наш опыт.\nОт коллег из других компаний знаем, что у многих сложилась схожая ситуация: используемая версия Asterisk отстает от актуальной уже на несколько лет. У кого-то даже версии 1.4 или 1.8. Почему не обновляют? У всех свои причины, наши же связаны с тем, что из-за рисков нарушить работу огромной инфраструктуры наши предшественники не решались пойти на такой шаг. А мы решились. \nЗачем мы все-таки решились на обновление\nИнфраструктура телефонии Интерсвязи пишется уже с давних времен. От предков нам достался кластер Asterisk 11 версии и многочисленные FastAGI скрипты, написанные на языке Perl. Казалось бы, что может быть хуже? Когда этого очень много.\nВот таких кружочков AGI я мог здесь расположить около 50, если показывать реальную схему. Со всем этим мы весьма успешно существовали вплоть до 2022 года, разрабатывая и поддерживая скрипты и FastAGI, реализующие всю нашу разностороннюю бизнес-логику.\nВремя шло, с выхода Asterisk 11 на момент 2022 года прошло уже 10 лет. Последующие версии оснащались новыми функциями, закрывали проблемы старых релизов, а вместе с этим появлялись причины и формировались цели на обновление.\nЦель №1\n — избавиться от устаревших Asterisk 11 из исходников и получить актуальную версию из пакетов, что позволит нам приобрести актуальные функции и облегчить следующие обновления.\nЦель №2\n — оптимизировать нашу инфраструктуру. Изначально это не рассматривали, она сформировалась в процессе обновления, когда мы разбирали возникшие трудности.\nЦель №3\n — внедрить использование ARI для модернизации VoiP-разработки, использования новых фич и современных приложений.\nРасскажу обо всех целях подробнее, потому что на пути к каждой из них возникали свои трудности и нюансы.\nКак мы обновляли Asterisk\nНам не хотелось все сводить к банальному обновлению. Хотелось получить инструмент, с которым мы могли бы одной командой сконфигурировать сервер с Asterisk 18 под свои задачи.\nМы используем кластер из четырех серверов с Asterisk, один из них — в резерве. Балансировка между ними производится с помощью Kamailio, поэтому автоматизация этого процесса для нас более актуальна. Если бы мы вручную обновляли серверы, это могло занять по два-три дня на каждый из четырех. Еще возрастал риск что-то забыть и произвести обновление не унифицировано.\nВ качестве инструмента выбрали Ansible. Произвели рефакторинг проекта, убрали лишние неиспользуемые элементы инфраструктуры, оптимизировали то, что работало не лучшим образом. Как итог — разработали роль Ansible, которая устанавливает свежий Asterisk и конфигурирует сервер примерно за 20 минут. В дальнейшем этой ролью можно проводить абсолютно любые манипуляции с сервером: конфигурирование iptables, systemd, обновление ПО и т.д.\nС какими проблемами столкнулись:\n1. Неподдерживаемая библиотека для работы с AMI. \nРабота AMI состоит в том, что какое бы событие не произошло на Asterisk, будь то начало или конец звонка, задание переменной или что либо еще — все это инициализирует AMI-событие с сообщением, которое содержит в себе информацию и параметры для обработки этого события.\nЯдро нашей телефонии было написано с использованием языка Perl. Мы использовали Metacpan-библиотеку Asterisk::AMI, которая вышла аж в 2011 году. Однако с выходом Asterisk 13 «выхлоп» сообщений AMI изменился, и эта библиотека перестала поддерживаться.\nИз рассматриваемых вариантов решения проблемы — реализовать на серверах Asterisk собственное API, например, на Python с использованием Panoramisk, к которому мы бы обращались из наших скриптов, а он бы уже соответственно работал с AMI локально. \nДругой вариант решения проблемы — переписать все скрипты на REST-интерфейс Asterisk (ARI) и работать с Asterisk напрямую. Такой вариант, мягко говоря, очень замедлил бы процесс обновления, учитывая количество скриптов для переписывания. \nВ итоге решение проблемы нашлось не без везения — методом быстрого гугления  наткнулись в GitHub на готовое решение от Жени Гостькова, который, дописав обработку изменившихся сообщений, «научил» работать Perl c Asterisk 16+. Как показала практика, в том числе и с 18 версией. Проблему решили.\n2. Изменение некоторых событий и как следствие — наши скрипты не принимали и не знали, как работать с новыми событиями. \nИзменились не только имена, но и наполнение событий. Взгляните на самый явный пример.\nЭто событие Bridge, 11 версия Asterisk:\nEvent: Bridge\nBridgestate: <value>\nBridgetype: <value>\nChannel1: <value>\nChannel2: <value>\nUniqueid1: <value>\nUniqueid2: <value>\nCallerID1: <value>\nCallerID2: <value>\nИ вот, что оно из себя представляет уже в 18 версии:\nEvent: BridgeEnter\nBridgeUniqueid: <value>\nBridgeType: <value>\nBridgeTechnology: <value>\nBridgeCreator: <value>\nBridgeName: <value>\nBridgeNumChannels: <value>\nChannel: <value>\nChannelState: <value>\nChannelStateDesc: <value>\nCallerIDNum: <value>\nCallerIDName: <value>\nConnectedLineNum: <value>\nConnectedLineName: <value>\nLanguage: <value>\nAccountCode: <value>\nContext: <value>\nExten: <value>\nPriority: <value>\nUniqueid: <value>\nLinkedid: <value>\nSwapUniqueid: <value>\nКроме того, изменилось также и поведение некоторых событий. Если раньше событие Bridge вызывалось один раз на два канала одновременно, то теперь заменившее его событие BridgeEnter вызывается два раза за звонок и на каждый канал. Нам пришлось изучить все подобные изменившиеся события и переписать их обработку.\n3. Приложение Queue. \nТретья проблема состояла в том, что глубоко под капотом у нас используется приложение диалплана Queue, и оно между 11 и 18 версией также претерпело немало изменений. У нас нарушилась логика при переводе звонка между очередями нашего кол-центра: наши Asterisk перестали понимать, что звонок ушел из очереди. По всей видимости, ранее в приложении существовал баг, который служил нам ориентиром, сигнализирующим о том, что звонок покидает очередь. Возможно, с выходом новых версий баг был устранен, из-за чего этого ориентира мы лишились. \nМы посмотрели исходники приложения, решили, что не будем их править, а вместо этого реализуем логику при переводе звонка из одной очереди в другую на базе UserEvent, имена которых не зависят от версии Asterisk.\nUserEvent — это приложение диалплана, которое позволяет создать кастомизированное AMI-событие с необходимыми нам параметрами, которые могут состоять, например, из текущих переменных канала.\nПример из диалплана:\n[redirect]\nexten => 1,Verbose(Перевод звонка)\n  . . .\n  . . .\n  same => n,UserEvent(Redirect,MemberFrom:\\ ${MEMBER_FROM}, MemberTo:\\ ${MEMBER_TO})\n  . . .\n  . . .\nКак мы им воспользовались? Помимо приложения Queue, наш кол-центр реализован за счет диалплана и различных скриптов, что позволяет нам легко конфигурировать любой этап звонка. На примере перевода звонка: когда оператор совершает его, вызываем в диалплане соответствующий UserEvent, который отлавливается нашим слушателем AMI-сообщений и обрабатывается соответствующим скриптом. А в этом скрипте фиксируется статистика по разговору с текущим оператором и останавливается запись разговора с ним. \nКак мы оптимизировали инфраструктуру\nВ процессе обновления мы обнаружили необходимость в оптимизации и в некоторых переработках реализации нашей инфраструктуры. В изначальной реализации скрипты делали прямой коннект с AMI, что, как показала практика, нестабильно работает под высокой нагрузкой. При большом количестве звонков, либо же ввиду каких-то сетевых проблем, коннект с AMI обрывается, и теряются все данные по звонкам. Мы теряем статистику, нарушается обработка звонков, возникают двойные и тройные звонки у одного оператора.\nЧтобы решить эту проблему, мы начали складывать события с серверов Asterisk в кэш, предотвращая их потерю, а уже скрипты, в свою очередь, будут вычитывать события из этого кэша для дальнейшей обработки. \nМы изолировали работу прослушивания AMI-событий в отдельный скрипт, который вычитывает AMI-сообщения и складывают их в кэш в виде Redis, а скрипты, работающие с обработкой этих событий, просто забирают их из кэша и работают как с обычным JSON-объектом.\nКакие еще преимущества мы получили от этой схемы?\n1. Мы добились повышения отказоустойчивости.\nУ нас есть кластер из четырех Asterisk, а также сервера, на которых крутятся наши скрипты. Скрипт, прослушивающий события, находится локально с каждым Asterisk, вычитывает сообщения и пытается их поместить в Redis, который находится на сервере с приложениями. Приложения слушают очередь этих событий в Redis и обрабатывают их. Что же произойдет, если Redis по какой-то причине, например, при сетевой проблеме, окажется недоступен? Listener это поймет и начнет отправлять сообщения на другой аналогичный сервер, на котором расположены те же приложения.\nТаким образом, данные по звонку не потеряются даже в случае выхода из строя одного из серверов. \n2. Мы оптимизировали обработку наших звонков. \nДавайте взглянем на кусочек этой схемы отдельно:\nЭти два скрипта работают параллельно, один связан с обработкой хождения самого звонка, другой связан с работой со статистикой. Мы выяснили, что они работают с одними и теми же событиями AMI. Поэтому решили, что достаточно отказаться от древнего скрипта по обработке хождения звонка, перенеся его логику в более молодой по работе со статистикой. Старый скрипт для своей работы порождал огромное количество форков, и следить за ним было уже неудобно. В итоге получили вот такую лаконичную схему. \n3. Мы организовали фильтрацию событий.\nВ ходе дальнейших оптимизаций выяснили, что для работы нам нужны не все события, которые генерирует Asterisk, а лишь малая их часть — примерно 2,5%\nНапример, есть событие NewExten, которое вызывается, когда звонок переходит от экстена к экстену. Они составляли почти половину от всего потока событий, но при этом мы ими не пользовались.\nДругой пример — события VarSet, которые генерируются всякий раз, когда в звонке происходит задание какой-то переменной. Для работы наших скриптов достаточно отлавливать только 10-15 таких событий.\nКак мы можем это оптимизировать? Возможно кто-то не знал, но в Asterisk есть функция фильтрации AMI-сообщений в файле конфигурации manager.conf. В нем можем указать только необходимый набор сообщений. Так и сделали. \nНапример, вот так мы указали только те события, которые нам нужны, исключив отсюда NewExten и др.:\neventfilter=Event: AgentConnect\neventfilter=Event: AgentComplete\neventfilter=Event: AgentRingNoAnswer\neventfilter=Event: Bridge\neventfilter=Event: BridgeEnter\neventfilter=Event: DialBegin\neventfilter=Event: Hangup\neventfilter=Event: Hold\neventfilter=Event: MusicOnHoldStart\neventfilter=Event: MusicOnHoldStop\neventfilter=Event: NewCallerid\neventfilter=Event: Newchannel\neventfilter=Event: Unhold\neventfilter=Event: UserEvent\neventfilter=Event: QueueCallerAbandon\nА по такому шаблону можно указать только те переменные для VarSet, которые нам необходимы:\neventfilter=Variable: QUEUE_NAME\neventfilter=Variable: BRIDGEPEER\neventfilter=Variable: ALARM_QUEUE_ID\neventfilter=Variable: LOCAL_CALL_TO\neventfilter=Variable: AUTODIAL_NUM\neventfilter=Variable: SUBSCRIBER_DIAL\neventfilter=Variable: SUBSCRIBER_CALLERID\neventfilter=Variable: TICKET\neventfilter=Variable: DIALSTATUS\neventfilter=Variable: AUTO_MONITOR\neventfilter=Variable: REVERSE_DIAL\neventfilter=Variable: REALNUMBER\neventfilter=Variable: DAMAGE_EXISTS\neventfilter=Variable: PLANNED_KTV\neventfilter=Variable: DEBT_EXISTS\nЕсли в будущем какие-то названия событий поменяются или добавятся новые, мы просто изменим фильтрацию и реализуем их обработку. Результаты выполнения цели по оптимизации представлены ниже. Данные получены с нашего мониторинга очереди событий в Redis. \nЕсли раньше мы в среднем имели 20-50 событий в очереди, а в пиках нагрузка могла доходить до 200 событий, то теперь у нас в среднем 1-2 события в очереди, а пиковые состояния доходят до 4.\nКак мы внедряли ARI\nМы начали использовать ARI в нашей инфраструктуре. Например, ARI Proxy — модуль для Golang, позволяющий обеспечить горизонтальное масштабирование ARI-приложений и упрощающий работу с внешними сервисами.\nРанее мы работали с внешними API напрямую через приложение диалплана CURL. Из-за этого нам приходилось везде по-разному, в зависимости от запроса, парсить ответ от API, тратить строки диалплана на валидацию и обработку. Это было неудобно. \nТеперь же с использованием приложения на ARI Proxy, в которое мы проваливаемся одной строчкой через Stasis, можем лаконично и единообразно обрабатывать запросы и ответы.\nПри этом мы получили возможность засылать разные виды запросов, с различными хедерами, с кастомным боди.\nВторой пример: мы начали снимать данные по каналам на предмет джиттера для отображения возможных проблем с голосом и оперативного принятия мер по их устранению. Для этого используем приложение на Golang с использованием CyCoreSystems/ARI.\nТакже постепенно уходим от AMI. Все новые приложения пишем уже на ARI, а старое стараемся разбирать и переписывать.\nВыводы\nПодводя итог, осуществленное обновление позволило нам достичь следующих успехов:\nРазработали инструмент для настройки серверов с актуальной версией Asterisk из пакетов. Напомню, этим инструментом стала роль Ansible.\nСоздали отказоустойчивую систему унифицированной и расширяемой обработки звонков посредством оптимизации и изолирования работы с AMI.\nНачали внедрение ARI, что помимо удобств в нашей работе позволяет снизить порог входа для новых разработчиков, которые будут взаимодействовать с Asterisk с помощью привычного им REST-интерфейса.\nНа что обратить внимание при обновлении версии Asterisk:\nсовместимость используемых вами библиотек в приложениях; \nнюансы при работе с AMI, например, смена имен событий.\nУдачных обновлений! Надеюсь, наш опыт поможет вам. \nСпасибо за внимание!\n \n ",
    "tags": [
        "asterisk",
        "обновление"
    ]
}