{
    "article_id": "728194",
    "article_name": "Восемь признаков недо-yield вашего проекта на Python",
    "content": "\r\n\nKandinsky 2.1: Умпалумпы программируют python код без yield\n\r\nИногда говорят, что \nкод имеет запах\n. Это относится к стилистике написания, выбору переменных и т.п. Однако, когда речь идет про циклы, я предпочитаю использовать термин «недо-yield», характеризующий стиль работы программиста в циклах и с массивами данных.\n\r\n\n\r\nПредставим себе, что Пупа и Лупа взялись писать код на Python. Но Лупа заболел, и Пупе пришлось писать код за… него. Код, который у них в итоге получился, используется во множестве репозиториев и был тепло оценен Python-сообществом в форме нескольких PEP-соглашений. Предлагаю вам пройтись по такому коду, принюхаться и обратить внимание на некоторые строки.\n\r\n\n\r\n\nDisclaimer:\nВ статье я критично высказываюсь о недоиспользовании в Python коде генераторов и конструкций на их основе. Если вам, по каким-то причинам, комфортнее использовать другие циклические конструкции языка Python, прошу не воспринимать эту статью как причину для конфронтации.\n\r\n\nПервый признак «недо-yield» — \nВальтруизм\n (Whiletruism).\n\r\nНу конечно, кто ж не знает старину \n«while»\n!\n\r\n\n\r\n\n# Synchronous put. For threads.\n    def put(self, item):\n        while True:\n            fut = self._put(item)\n            if not fut:\n                break\ncurio, queue.py\n\r\nМне не нравится этот стиль написания, поскольку \nwhile\n по идее должен проверять условие, которого нет. Я предпочитаю использовать более декларативную конструкцию.\n\r\n\n\r\nДля этого мне понадобится built-in \niter\n из \nPEP 234 – Iterators\n:\n\r\n\n\r\n\niter(func, sentinel) \n\r\nКогда мы используем функцию \niter\n(\nfunc, sentinel\n), каждое обращение к итератору будет вызывать функцию \nfunc()\n и возвращать её результат, до тех пор, пока он не станет равным \nsentinel\n. Зная это — можно легко получить вечный генератор:\n\r\n\n\r\n\niter(int, 1)  # самый известный мне вечный генератор.\n\r\nC помощью вечного генератора можно написать такой вариант вечного цикла:\n\r\n\n\r\n\ninfinity = iter(int, 1)  # infinity generator\nfor _moment_ in infinity:\n    ...  # Carpe diem\nЛови момент, так сказать.\n \n\r\nВсе известные мне варианты перебора бесконечностей из \nitertools\n проигрывают предложенному варианту, но вы можете использовать:\n\r\n\n\r\n\n\r\n\ncount\n(\nstart=0, step=1\n): 0, 1, 2, 3, 4,… раньше был не бесконечный, говорят, уже поправили.\n\r\n\ncycle\n(\np\n): p[0], p[1], ..., p[-1], p[0], ...\n\r\n\nrepeat\n(\nx, times=∞\n): x, x, x, x, ...\n\r\n\n\r\nЕсли вы не эстет, то пока никакой выгоды от такой замены кода вы не получите. \n\r\n\n\r\n\nСледующий признак «недо-yield» — \nПрерванный Вальтруизм\n(Breakable Whiletruism).\n\r\nВот пример этого родственника бытового вальтруизма:\n\r\n\n\r\n\ndef build_values(self, args, kwargs):\n        values = {}\n        if args:\n            arg_iter = enumerate(args)\n            while True:\n                try:\n                    i, a = next(arg_iter)\n                except StopIteration:\n                    break\n               arg_name = self.arg_mapping.get(i)\n               ....\npydantic, decorators.py\n\r\nЯ предложил бы написать этот код с использованием генераторного выражения. Для этого нам пригодится \nPEP 289 – Generator Expressions\n.\n\r\n\n\r\n\ndef build_values(self, *args, **kwargs):\n    finc = self.arg_mapping.get\n    gen = (func(idx), arg for idx, arg in enumerate(args))\n    for arg_name, arg in gen:\n        # do something\n\r\nС этого момента начинает проявляться важность использования генераторов:\n\r\n\n\r\n\n\r\n\nКод с генератором выполняет то же самое, только код короче.\n\r\n\nМы избежали объявления дополнительных переменных.\n\r\n\nПо моему мнению, когнитивная сложность этой части кода ниже. Я делал \nдоклад на тему сложности кода на PyCon DE 2022\n, кому сложно понимать мой немглийский, этот же \nдоклад на русском\n.\n\r\n\n\r\n\n\r\n\nВершина вальтруизма — блок классического For-loop вкупе с break. (Breakable Looping)\n\r\nВы можете встретить подобный пугающий код:\n\r\n\n\r\n\nhost_header = None\nfor key, value in scope[\"headers\"]:\n    if key == b\"host\":\n        host_header = value.decode(\"latin-1\")\n        break\nstarlette, datastructires.py\n \n\r\n\n— А что такого ужасного в моем коде\n, возмутился Пупа, увидев, что я пишу эту статью.\n\r\n\n— Смотри, ты ранее уже положил в цикле информацию в список «headers», и после ищешь ключ host. Что мешает тебе сразу создать словарь, ведь потом все равно список используется как словарь. Значение получить проще: \nheaders.get\n(\n'host'\n). Кстати, ты можешь получить \nhost\n еще на этапе создания «headers»...\n\r\nКонечно, у такой задачи много решений. Я бы использовал функцию \nnext\n:\n\r\n\n\r\n\nhost_header = next((val.decode(\"latin-1\") for key, val in scope[\"headers\"] if key == b\"host\"), None)\n\n\r\nПупа, скорее всего, покрутит пальцем у виска и побежит за… своим коллегой. А я попробую объяснить, почему наличие \nbreak\n в цикле \nfor\n — это одно из диких недоразумений, которые могут встретиться в коде. Я считаю, что такой паттерн также характеризует «недо-yield» кода.\n\r\n\n\r\nПредставьте себе, что вам нужно создать список с миллионом объектов. Вы хотите получить доступ к одному из объектов, пропустив все предшествующие, и остановиться. У меня есть вопросы:\n\r\n\n\r\n\n\r\n\nЧто мешает выявить нужный объект на этапе создания списка?\n\r\n\nСоздаваемый список явно планируется итерировать, а как же его еще можно использовать. Почему тогда это список, а не генератор?\n\r\n\n\r\nОтвечая на эти вопросы, приходим к следующему признаку «недо-yield»:\n\r\n\n\r\n\nСоздание List в циклах. Loop for List\n\r\nРассмотрим пример:\n\r\n\n\r\n\nif isinstance(obj, (list, set, frozenset, GeneratorType, tuple)):\n        encoded_list = []\n        for item in obj:\n            encoded_list.append(jsonable_encoder(item, *args, **kwargs))\n        return encoded_list\n\nfastapi, encoders.py\n\r\nЕсли смотреть на код fastapi дальше, то видно, что возвращаемое значение \nencoded_list\n позже будет проитерировано. В таком случае, действительно, имеет смысл использовать генератор:\n\r\n\n\r\n\nif isinstance(obj, (list, set, frozenset, GeneratorType, tuple)):\n    return (\n        jsonable_encoder(item, *args, ** kwargs) for item in obj\n    )\n\r\n\n— \nНу как же\n, начнет возмущаться Лупа, увидев мой код. — \nУ тебя же не так очевидно, как в моем варианте\n.\n\r\n— \nВ принципе да, если нет опыта работы с генераторами, очевидность теряется\n, отвечаю я.\n\r\n— \nНо подождите, это же еще и тестировать невозможно!\n — восклицает Пупа за… своим другом.\n\r\nИ здесь я соглашусь:\n\r\n\n\r\n\nВ Python не так много средств для отладки генераторных выражений\n\r\n\n\r\n\nДля отладки можно написать простую обертку-генератор:\n\r\n\n\r\n\ndef genreport(gen):\n    return ((print(item), item)[-1] for item in gen)  # это может быть и logging.log\n\r\n\n\r\nОбертку используем в необходимых местах, где нужно следить за генерируемыми значениями.\n\r\n\n\r\n\n\r\n\nКроме того, есть библиотека \nitertools\n\r\n\n\r\n\nitertools.tee(iterable, n=2)\n# Return n independent iterators from a single iterable.\n\r\n\n\r\nПревращает ваш итератор в два независимых итератора. Один можно использовать для тестирования, второй — для продолжения выполнения. Ознакомьтесь с ограничениями использования.\n\r\n\n\r\n\n\r\n\nИ есть еще \nmore_itertools\n\r\n\n\r\n\nmore_itertools.spy(iterable, n=1)\n# Return a 2-tuple with a list containing the first n elements of iterable, and an iterator with the same items as iterable.\n#This allows you to “look ahead” at the items in the iterable without advancing it.\nПозволяет «взглянуть вперед» на элементы итерируемого объекта, не «продвигая» его. Читайте ограничения использования.\n\r\n\nПро \nmore_itertools\n я узнал от глубоко почитаемого мной \nkesn\n, хотя фраза в его \nнедавней статье\n «\nГенераторы всем хороши, кроме одного: они откладывают выполнение кода, и в реальности узнать, когда ваш код выполнится, бывает затруднительно...\n» показывает неприятие величайшей сути генератора: выполняться только когда нужно, а когда не нужно — не выполняться.\n\r\nУ меня есть на это пример с множественным \ncontinue\n в цикле, конечно же, как признак «недо-yield»:\n\r\n\n\r\n\nПродолжающая Форлупнутость. Continued Forlooperty\n \n\r\nПредставим себе код:\n\r\n\n\r\n\ndef remove_cookie_by_name(cookiejar, name, domain=None, path=None):\n    \"\"\"Unsets a cookie by name, by default over all domains and paths. Wraps CookieJar.clear(), is O(n).\"\"\"\n    clearables = []\n    for cookie in cookiejar:\n        if cookie.name != name:\n            continue\n        if domain is not None and domain != cookie.domain:\n            continue\n        if path is not None and path != cookie.path:\n            continue\n        clearables.append((cookie.domain, cookie.path, cookie.name))\n        ... \n\nrequest, cookies.py\n\r\nТут мне кажется странным следующее: если при создании множественного списка элементов позже многие из них будут пропущены, то зачем было создавать такой список?\n\r\n\n\r\nПомочь нам в этой ситуации может David Beazley с презентацией \ncoroutines\n, начинаем читать со слайда 34 про генераторы в цепочке \ngenerators pipeline\n:\n\r\n\n\r\n\ndef remove_cookie_by_name(cookiejar, name, domain=None, path=None):\n    \"\"\"Unsets a cookie by name, by default over all domains and paths \"\"\"\n    clearables = (cookie for cookie in cookiejar if cookie.name == name)\n    if domain is not None:\n        clearables = (cookie for cookie in clearables if domain == cookie.domain)\n    if path is not None:\n        clearables = (cookie for cookie in clearables if path == cookie.path)\n   ...\n\r\nОбратите внимание, что ещё до создания и наполнения списка мы убрали проверки, которые не надо выполнять. Мы просто декларировали, как должен работать генератор будущих значений. И конечно же, он отработает не в момент его объявления, а позже, когда генератор будет итерироваться.\n\r\n\n\r\nЕще один пример для тренировки, как работать с генераторами в цепочке, мне, кстати, кажется, что в исходнике есть небольшая ошибка:\n\r\n\n\r\n\n    def _read_file(self, file_name):\n        file_values = {}\n        with open(file_name) as input_file:\n            for line in input_file.readlines():\n                line = line.strip()\n                if \"=\" in line and not line.startswith(\"#\"):\n                    key, value = line.split(\"=\", 1)\n                    key = key.strip()\n                    value = value.strip().strip(\"\\\"'\")\n                    file_values[key] = value\n        return file_values\nstarlette, config.py\n\r\nМоя рекомендация по улучшению этого кода остается неизменной: строим трубопровод (pipeline):\n\r\n\n\r\n\ndef _read_file(self, file_name):\n        with open(file_name) as input_file:\n            lines = (line for line in input_file.readlines())\n            lines = (line.strip() for line in lines if line or lines.close())  # спасибо Пупе и Лупе\n            lines = (line.partition(\"=\") for line in lines if \"=\" in line and not line.startswith(\"#\"))\n            return {key.rstrip() : value.lstrip().strip(\"\\\"'\") for key, __, val in lines}\n\r\nВ ранних версиях Python этот код выглядит более элегантным, но Пупа и Лупа быстро подсуетились и внесли \nPEP 479 – Change StopIteration handling inside generators\n. \n\r\nИменно поэтому мне приходится использовать \ngenerator.close()\n из \nPEP 342 – Coroutines via Enhanced Generators \n, чтобы остановить работу генератора внутри генератора.\n\r\n\n\r\nЗавершим наше исследование финальным признаком «недо-yield»:\n\r\n\n\r\n\nЛюбовь к изменению списков. List-changes Love.\n\r\nЭта часть — мой основной аргумент для всех пупалупов. Замена генерации списков/словарей итераторами не спасет вас. Многократное использование одного и того же списка — это тот самый супер-пупер-лупер паттерн, способный убить производительность даже самой отлаженной и отрефакторенной библиотеки!\n\r\n\n\r\nВот вам пример кода, написанного Пупой за… его напарника, который имеет огромное количество положительных оценок на \nstackowerflow\n, попал в \nдокументацию DRF\n и уже \nопубликован на страницах HABR\n.\n\r\n\n\r\n\nclass DynamicFieldsModelSerializer(serializers.ModelSerializer):\n    def __init__(self, *args, **kwargs):\n        # Don't pass the 'fields' arg up to the superclass\n        fields = kwargs.pop('fields', None)\n        # Instantiate the superclass normally\n        super().__init__(*args, **kwargs)\n        if fields is not None:\n            # Drop any fields that are not specified in the `fields` argument.\n            allowed = set(fields)\n            existing = set(self.fields)\n            for field_name in existing - allowed:\n                self.fields.pop(field_name)\n\r\nИ ни одна Пупа или Лупа, а с ними и множество других разработчиков не видят проблемы, которая, на поверку, оказывается довольно ощутимой в production.\n\r\n\n\r\nРасскажу вам, в чем тут косяк. Я обнаружил его в 2017 году, когда заметил, что мой сериализатор \ndynamicFieldsModel\n для «толстой» модели работал медленнее обычного при сериализации только трех запрошенных полей. В приведенном выше примере происходят изменения сериализатора в коде после \nsuper().__init_()\n, и, возможно, причина замедления именно в этом коде.\n\r\n\n\r\nИ, да, так оно и оказалось — причиной было пупалупное решение сначала создать ВСЕ поля модели, а затем выполнить проход по списку полей и удалить ненужные. Там вообще-то dict-like object, но не будем портить такую хорошую притчу. Ад многократных проходов по спискам внутри самой DRF я еще упомяну.\n\r\n\n\r\nДля нормального решения этой задачи я предлагаю перестать безмозгло следовать документации и посмотреть, что вообще происходит с сериализатором.\n\r\n\n\r\nОказывается, что создание полей после инициализации сериализатора происходит в ленивом \nproperty\n \nfields\n, которое в первую очередь вызывает \nget_field_names\n:\n\r\n\n\r\n\n# Methods for determining the set of field names to include...\n    def get_field_names(self, declared_fields, info):\n        \"\"\" Returns the list of all field names that should be created when\n        instantiating this serializer class. This is based on the default\n        set of fields, but also takes into account the `Meta.fields` or\n        `Meta.exclude` options if they have been specified. \"\"\"\n        fields = getattr(self.Meta, 'fields', None)\n        exclude = getattr(self.Meta, 'exclude', None)\n        ...\n\nrest_framework, serializers.py\n\r\nКак видим, информацию о полях \nget_field_names\n берет из \nself.Meta\n.\n\r\n\n\r\nТолько не поддавайтесь первому пупалупному желанию переопределить \nself.Meta.fields/self.Meta.extra\n. Этим вы сломаете все и сразу: Meta — это синглтон для всех объектов этого класса.\n\r\n\n\r\nА вот так — уже можно:\n\r\n\nclass DynamicFieldsModelSerializer(serializers.ModelSerializer):\n    \"\"\"A ModelSerializer that takes an additional `fields` argument that controls which fields should be displayed.\"\"\"\n    def __init__(self, *args, **kwargs):\n        self.Meta = type('Meta', (self.Meta,) {'fields' : kwargs.pop('fields', self.Meta.fields)})\n        super(DynamicFieldsModelSerializer, self).__init__(*args, **kwargs)\n\n\r\nРазумеется, переданные \nfields\n проверены на наличие в модели, и в декларации класса сериализатора определены \nMeta.fields\n. Попробуйте, возможно, результат вас удивит.\n\r\n\n\r\nВ этом примере я хочу выразить еще один признак «недо-yield». Создание списка объектов и его изменение в дальнейшем — это ужасное программное решение. Хуже может быть только многократное изменение этого же списка.\n\r\n\n\r\nПодводя итог моим размышлениям над «пупалупским» кодом, соберу все воедино:\n\r\n\n\r\n\nУ вас лютый «недо-yield» в коде, если часто встречаются следующие признаки:\n\r\n\n\r\n\nОсновной «недо-yield» в проекте характеризуется соотношением количества yield с количеством циклов на количество файлов проекта.\n\r\n\nWhiletruism. Измеряется количеством бесконечных циклов в проекте.\n\r\n\nBreakable Whiletruism. Измеряется количеством бесконечных циклов с break в проекте.\n\r\n\nBreakable Looping. Измеряется количеством for циклов с break в проекте.\n\r\n\nContinuable Looping. Измеряется количеством for циклов с continue в проекте.\n\r\n\nLoop for List. Измеряется количеством объявлений \"= [ ]\" перед циклом с .append в проекте.\n\r\n\nLooped List. Измеряется количеством «For… in [...» в проекте.\n\r\n\nList-Change Love. Измеряется количеством .extend/.pop/.append и т.п. в циклах в проекте.\n\r\n\n\r\nИспользование вышеупомянутых паттернов в коде не обязательно ошибочно, и надежнее сделать замеры времени. Мой способ позволяет мне предположить наличие «пупалупных» кусков кода еще до запуска. Когда полученные цифры выглядят странно, становится ясно, что Пупа и Лупа где-то рядом.\n\r\n\n\r\nДавайте проверим несколько библиотек:\n\r\n\n\r\n\n\r\n\nDRF оказалась рекордсменом «пупалупия»: 590 циклов, 8 yield, 72 файла, 5 while, 23 break, 24 continue.\n\r\n\nPydantic, та еще «пупалупа»: 436 циклов и 107 yield на 26 файлов, и 13 while, 6 break, 38 continue.\n\r\n\nСравните с fastapi: 70 циклов и 38 yield на 42 файла, и ни одного while или break, только 8 continue. Я раньше реально недооценивал качество кода этой библиотеки.\n\r\n\n\r\nМоя любимая библиотека \ndjango.contrib.admin\n показала: 326 циклов и 38 yield на 29 файлов.\n\r\nОб этих и других моих исследованиях Django.admin я докладывал на \nDjango Con EU 2022\n и после на \nDjango Con US 2022\n. Это был интересный опыт, не уверен только, что после моих заявлений о ежегодном многокилометровом недоелде Django.admin меня позовут выступить там еще раз.\n\r\n\n\r\nВ завершение предлагаю вам попробовать посмотреть характеристики «недо-yield» вашего проекта, и, может быть, вы захотите поделиться своими результатами в комментариях. Также буду рад услышать истории о том, как вы используете и тестируете генераторы в коде.\n\r\n\n\r\n\nP.S. Те, кто еще не знаком с Пупой и Лупой, это два \nумпалумпа\n и про их приключения есть \nмножество историй\n. \n\r\n\n\r\nP.P.S. На вопрос, откуда такие \nгалимые\n примеры — все блоки кода для статьи взяты из публичных репозиториев. В каждом примере указан источник. \n\r\n\n\r\nМогу добавить, что в приватных проектах ситуация не лучше. С 2017 года я в роли Code-Ментора для Python-разработчиков повидал множество репозиториев. Если проект большой и сменил несколько разработчиков, то каждая доработка наслаивается на предыдущий код и, например, паттерн looped-list мог объединять десятки повторов.\n\r\n\n\r\nP.P.P.S. Для генерации картинок в статью я использовал рекламируемый сейчас на Habr \nKandinsky 2.1\n. Генератор так себе, но иногда он попадает в цель. Смотрите, как и выглядит и пахнет Python-код без генераторов.\n\r\n\n\r\n\n\r\n\nKandinsky 2.1: Пупа и Лупа пишут код без yield\n\r\n\n\n                        \nХочу больше Кандинского!\n\n                        \n\r\n\nKandinsky 2.1: Лютый недо-yield в программном коде Python без генераторов\n\r\n\n\r\n\n\r\n\nKandinsky 2.1: Недо-yield в программном коде Python, --малевич\n\r\n\n\r\n\n\r\n\nKandinsky 2.1: Многокилометровый недо yield программного кода\n\r\n\n\r\n\n\r\n\nKandinsky 2.1: недо yield программного кода Python надо устранять!\n\r\n\n\r\n\n\r\n\nKandinsky 2.1: Пупа и Лупа программируют недо-yield\n\r\n\n\r\n\n\r\n\nKandinsky 2.1: бытовой вальтруизм не так ужасен, как его близкий родственник прерванный вальтруизм(Breakable Whiletruism)\n\r\n\n\r\n\n\r\n\nKandinsky 2.1: пупа и лупа пишут код -ренессанс\n\r\n\n\r\n\n\r\n\nKandinsky 2.1: пупа и лупа пишут код на Python\n\r\n\n\n                    \n \n ",
    "tags": [
        "python",
        "ненормальное программирование",
        "рефакторинг",
        "генераторы"
    ]
}