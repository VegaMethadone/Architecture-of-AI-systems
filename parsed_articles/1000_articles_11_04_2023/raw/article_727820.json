{
    "article_id": "727820",
    "article_name": "Managed Kubernetes the hard way",
    "content": "Всем привет. Меня зовут Путилин Дмитрий (Добрый Кот) \nTelegram\n.\nОт коллектива \nFR-Solutions\n и при поддержке \n@irbgeo\n \nTelegram\n : Продолжаем серию статей о K8S.\nВ этой статье мы поделимся своим опытом разработки Managed K8S под Yandex Cloud и расскажем, как мы создали конфигурацию, которую можно легко адаптировать для запуска в любом облаке или on-premises решении, изменяя только некоторые настройки. Если вы заинтересованы в построении гибких и масштабируемых Kubernetes-кластеров, то этот материал обязательно для вас.\nВ предыдущих статьях\nБазовая организации сертификатов в kubeadm — \nСертификаты K8S или как распутать вермишель Часть 1\n.\nКак начать использовать внешний PKI сторедж Vault для хранения и выписывания сертификатов для k8s control‑plane — \nСертификаты K8S или как распутать вермишель Часть 2\n.\nКак развернуть Kubernetes кластер по принципу Hard Way — \nKubernetes the hard way\n.\nПроблема\nИз моего личного опыта могу сказать, что Managed решения в облаках или в онпрем‑серверах — это отличный инструмент для создания своего продукта, и зачастую этого достаточно. Однако, бывают ситуации, когда нужно больше гибкости и возможностей настройки, а Managed решение предоставляет ограниченный набор функций.\nДля нас было критично использовать сетевой плагин Cilium с нашими настройками, также нам требовались флаги feature‑gates, которых по дефолту нет в Yandex K8S API.\nВ принципе, не беда, мы всегда можем развернуть стационарный K8S и закастомизировать его как нам угодно. Возникли следующие вопросы: какие инструменты взять, какой выстроить процесс и как сделать так, что бы создаваемые кластера были одинаковыми?\nВыбор инструментов\nДля данной задачи однозначно требуются cloud native инструменты, поэтому выбор пал на Terraform. Остались вопросы: как настраивать узлы, нужен ли нам Ansible, Puppet, SaltStack? После 3 месяцев поиска золотой пилюли мы поняли, что для создания кластера нам потребуется только Terraform и cloud‑init.\nАрхитектура\nТак как в основе нашего продукта лежит Terraform, то одно из условий работы с ним — Сервисно‑ресурсная модель (СРМ). \nРесурсами выступают все его компоненты, от балансировщика нагрузки до конфигураци cloud‑init для нашего кластера.\nТакже CPM позволяет менять одинаковые типы ресурсов без потребности в смене процесса деплоя кластера, таким образом, описав модули создания инфраструктуры под Yandex Cloud, VK Cloud и т. п., и, поменяв намеример модуль Yandex cloud на модуль VK Cloud, получим тот же результат, но в другом окружении.\nСертификаты\nНаиболее значимым и сложным этапом было разработать подход работы с сертификатами, проблема была упомянута в предыдущих статьях. Мы определили основные спецификации для сертификатов и описали ресурсы Vault, которые создаются на основе содержимого \nспецификации\n. Однако возник вопрос доставки ключей/токенов на мастер-узлы, чтобы клиент на узле мог запросить сертификаты, указанные в спецификации. Было рассмотрено несколько вариантов решения этой проблемы:\nДля получения secret_id и role_id от Approle можно использовать временный токен, который имеет ограниченный доступ. Для этого токен должен иметь достаточно длительный срок жизни, чтобы виртуальная машина успела запустить клиента, или можно указать, что использование токена допустимо только один раз.\nИспользование сервиса IAM от облачного провайдера для сохранения secret_id/role_id для каждой машины в облаке. Затем, можно использовать cloud-cli для получения необходимых секретов прямо с хоста.\nМы предпочли второй вариант и выбрали его, так как он лучше подходил для нашего случая. Однако первый вариант может быть полезен в тех облаках, где нет поддержки сервиса IAM.\nПеременные окружения\nПри написании кода мы поняли, что описывать каждый модуль с его входными и выходными переменными - это трудоемкий процесс, особенно когда возникают повторы. Через некоторое время мы решили, что имеет смысл выделить отдельный модуль, содержащий переменные, которые используются в нескольких модулях. Таким образом, мы смогли уменьшить объем входных аргументов каждого модуля и привести их к более компактному формату: \nкаталог\nvariable \"k8s_global_vars\" {\n  description = \"module:K8S-GLOBAL VARS\"\n  type        = any\n  default     = {}\n}\n\nПри создании структуры этого модуля мы также уделяли внимание принципу \"записал - забыл\" - это означает, что если мы хотим добавить только переменную, но нехотим добавлять соответствующий вывод в OUTPUT, нам нужно использовать структурные массивы, в которые мы добавляем только нужные нам переменные, а глобальный вывод остается единым на блок. Например:\nlocals {\n\n  k8s-addresses = {\n    local_api_address           = format(\"%s.1\",  join(\".\", slice(split(\".\",local.k8s_network.service_cidr), 0, 3)) )\n    dns_address                 = format(\"%s.10\", join(\".\", slice(split(\".\",local.k8s_network.service_cidr), 0, 3)) )\n\n    idp_provider_fqdn           = format(\"auth.%s\"          , local.cluster_metadata.base_domain)\n    base_cluster_fqdn           = format(\"%s.%s\"            , local.cluster_metadata.cluster_name, local.cluster_metadata.base_domain)\n    wildcard_base_cluster_fqdn  = format(\"%s.%s.%s\", \"*\"    , local.cluster_metadata.cluster_name, local.cluster_metadata.base_domain)\n    etcd_server_lb_fqdn         = format(\"%s.%s.%s\", \"etcd\" , local.cluster_metadata.cluster_name, local.cluster_metadata.base_domain)\n  }\n}\n\noutput \"k8s-addresses\" {\n  value = local.k8s-addresses\n}\nCloud init\nГенерация cloud-init конфигурации является не менее важным аспектом, поскольку эта конфигурация передается виртуальной машине при ее создании.\nВ первых версиях мы были вынуждены описывать каждый файл, создавать шаблоны для них и выносить их в отдельные модули по логическому смыслу, например, модуль containerd\" включал в себя конфигурационные файлы и шаблоны для systemd сервисов. Однако, такой подход был слишком трудоемким в поддержке из-за большого количества модулей.\nМы решили использовать подход, подобный kubeadm. Сначала мы попытались развернуть кластер с помощью kubeadm, но выяснилось, что он не может выполнить первоначальную настройку системы, такую как установка пакетов, добавление конфигурационных файлов и запуск сервисов. Поэтому мы начали разработку инструмента, который бы мог настроить систему до требуемого состояния. Результатом этой работы стал fraimctl - инструмент, который заменил множество шаблонов одной командой \nfraimctl init\n.\nТаким образом, нам оставалось описать: \nбазовый \nконфиг\n fraimctl (устанавливает все компоненты и готовит конфиги к ним);\nбазовый \nконфиг\n kubeadm (генерит статик под манифесты и чекает, что кластер поднят);\nбазовый \nконфиг\n key-keeper (клиент который запрашивает сертификаты).\nУ нас есть несколько задач, которые мы должны выполнить, чтобы полностью отказаться от kubeadm. Мы планируем перенести этап создания конфигурационных файлов key-keeper, kubeconfig и static pod manifests в fraimctl. Кроме того, мы добавим функционал для проверки готовности сертификатов и кластера, а также этап маркировки узлов. Это позволит нам полностью отказаться от использования kubeadm и не зависеть от этого инструмента.\nFraimctl\nКак уже упоминалось ранее, этот инструмент создан для возможности полного отказа от использования kubeadm и настройки кластеров без его использования.\nПример конфигурациооного файла:\nfraimctl.conf\n- apiVersion: fraima.io/v1alpha\n  kind: Containerd\n  spec:\n\n    service:\n      extraArgs:\n        # This document provides the description of the CRI plugin configuration. \n        # The CRI plugin config is part of the containerd config\n        # Default: /etc/containerd/config.toml\n        config: /etc/kubernetes/containerd/config.toml\n\n    configuration:\n      extraArgs:\n        version: 2\n        plugins:\n          io.containerd.grpc.v1.cri:\n            containerd:\n              runtimes:\n                runc:\n                  # Runtime v2 introduces a first class shim API for runtime authors to integrate with containerd. \n                  # The shim API is minimal and scoped to the execution lifecycle of a container.\n                  runtime_type: \"io.containerd.runc.v2\"\n                  options:\n                    # While containerd and Kubernetes use the legacy cgroupfs driver for managing cgroups by default, \n                    # it is recommended to use the systemd driver on systemd-based hosts for compliance of the \"single-writer\" rule of cgroups. \n                    # To configure containerd to use the systemd driver, set the following option:\n                    SystemdCgroup: true\n\n    downloading:\n      - name: containerd\n        src: https://github.com/containerd/containerd/releases/download/v1.6.6/containerd-1.6.6-linux-amd64.tar.gz\n        checkSum:\n          src: https://github.com/containerd/containerd/releases/download/v1.6.6/containerd-1.6.6-linux-amd64.tar.gz.sha256sum\n          type: \"sha256\"\n        path: /usr/bin/\n        owner: root:root\n        permission: 0645\n        unzip:\n          status: true\n          files: \n            - bin/containerd\n            - bin/containerd-shim\n            - bin/containerd-shim-runc-v1\n            - bin/containerd-shim-runc-v2\n            - bin/containerd-stress\n            - bin/ctr\n\n      - name: runc\n        src: https://github.com/opencontainers/runc/releases/download/v1.1.3/runc.amd64\n        path: /usr/bin/\n        owner: root:root\n        permission: 0645\n\n    starting:\n      - systemctl enable containerd\n      - systemctl start containerd\n\nКаждый компонент имеет четыре стадии:\ndownloading\n  (загружает бинарные файлы, проверяет контрольные суммы, распаковывает необходимые компоненты и размещает их в соответствующих папках.)\nservice\n (генерирует службу systemd, и с помощью параметра extraArgs можно настроить ее поведение под свои нужды.)\nconfiguration\n  (генерирует конфигурацию для службы systemd, и с помощью параметра extraArgs можно настроить ее поведение под свои нужды.)\nstarting\n (выполняет необходимые команды после первых трех этапов.)\nОдной из ключевых особенностей этого инструмента является этап загрузки (Downloading), который загружает бинарные файлы компонентов. Это позволяет не зависеть от производителя операционной системы и разворачивать единым подходом на любом хосте, не нужно думать о множестве условий (if else) и о том какая операционная система в основе.\nТакже предусмотрены отдельные конфигурационные файлы для настройки sysctl и modprobe.\nfraimctl.conf\n- apiVersion: fraima.io/v1alpha\n  kind: Sysctl\n  spec:\n    configuration:\n      extraArgs:\n        net.ipv4.ip_forward: 1\n    starting:\n      - sudo sysctl --system\n\n- apiVersion: fraima.io/v1alpha\n  kind: Modprob\n  spec:\n    configuration:\n      extraArgs:\n      - br_netfilter\n      - overlay\n    starting:\n      - sudo modprobe overlay\n      - sudo modprobe br_netfilter\n      - sudo sysctl --system\nИнфраструктура\nКаждый кубик в Terraform представляет собой ресурс и логически определяется как класс в языке программирования. Мы можем определить класс, например, loadBalancer, который принимает определенный набор аргументов и возвращает структуру, которая также заранее определена. Это означает, что мы можем изменять кубики по нашему усмотрению, а при смене облака все компоненты будут взаимодействовать друг с другом благодаря структуре входных и выходных параметров.\nБлагодаря этой архитектуре мы можем обновлять операционные системы без проблем и даже менять производителя операционной системы на лету.\nkubectl get no -o wide\nroot@master-2-cluster-2:/home/dkot# kubectl get nodes -o wide\nNAME                 STATUS   ROLES                  AGE     VERSION    INTERNAL-IP   EXTERNAL-IP     OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nmaster-1-cluster-2   Ready    control-plane,master   2m50s   v1.23.12   10.1.0.11     51.250.66.122   Ubuntu 20.04.4 LTS   5.4.0-124-generic   containerd://1.6.8\nmaster-2-cluster-2   Ready    control-plane,master   2m53s   v1.23.12   10.2.0.33     84.201.139.95   Ubuntu 20.04.4 LTS   5.4.0-124-generic   containerd://1.6.8\nmaster-3-cluster-2   Ready    control-plane,master   2m55s   v1.23.12   10.3.0.21     51.250.40.244   Ubuntu 20.04.4 LTS   5.4.0-124-generic   containerd://1.6.8\n\nroot@master-2-cluster-2:/home/dkot# kubectl get nodes -o wide\nNAME                 STATUS   ROLES                  AGE   VERSION    INTERNAL-IP   EXTERNAL-IP     OS-IMAGE                       KERNEL-VERSION    CONTAINER-RUNTIME\nmaster-1-cluster-2   Ready    control-plane,master   37s   v1.23.12   10.1.0.12     62.84.119.244   Debian GNU/Linux 10 (buster)   4.19.0-18-amd64   containerd://1.6.8\nmaster-2-cluster-2   Ready    control-plane,master   12m   v1.23.12   10.2.0.16     51.250.27.187   Debian GNU/Linux 10 (buster)   4.19.0-18-amd64   containerd://1.6.8\nmaster-3-cluster-2   Ready    control-plane,master   12m   v1.23.12   10.3.0.13     51.250.45.49    Debian GNU/Linux 10 (buster)   4.19.0-18-amd64   containerd://1.6.8\n\nДля каждого облака необходимо написать модуль, который повторяет структуру выше, чтобы у нас всегда была одинаковая архитектура на всех кластерах.\nРеализация\nДавайте рассмотрим базовый проект и то, как можно начать использовать этот инструмент.\nСкачиваем репозиторий \nhttps://github.com/fraima/kubernetes\nВ этом репозитории есть несколько разделов\ninfrastructure-vault\n (создает рут PKI в Vault)\ninfrastructure-yandex\n (создает базовую конфигурацию в YC, которая включает в себя создание VPC, таблицы маршрутизации и создание сервисных аккаунтов по умолчанию)\ninfrastructure-keycloak\n (устанавливает базовую конфигурацию для Keycloak, которая позволяет авторизоваться в кластере через этот инструмент)\nk8s-yandex-cluster\n (проект-шаблон, который используется для создания кластера.)\nЗаходим в каждый раздел по очереди и применяем, что прописано в Readme.\nПодготовка\nДля начала работы вам понадобятся переменные для подключения к Vault, YC и Keycloak.\nenvironments\nexport TF_VAR_YC_CLOUD_ID=\"\"\nexport TF_VAR_YC_FOLDER_ID=\"\"\nexport TF_VAR_YC_TOKEN=\"\"\nexport TF_VAR_YC_ZONE=\"\"\nexport TF_VAR_VAULT_TOKEN=\"\"\nexport TF_VAR_VAULT_ADDR=\"\"\nexport TF_VAR_KEYCLOAK_REALM=\"\"\nexport TF_VAR_KEYCLOAK_CLIENT_ID=\"\"\nexport TF_VAR_KEYCLOAK_USER=\"\"\nexport TF_VAR_KEYCLOAK_PASSWORD=\"\"\nexport TF_VAR_KEYCLOAK_URL=\"\"\nЭтот подход позволяет использовать Terraform в контейнере через инструмент CI/CD, не указывая реальные значения переменных в провайдерах.\nЕсли вы работаете с чистым Terraform, не забывайте выделять каждый кластер в отдельный workspace.\nterraform workspace new example\n\nterraform plan    -var-file vars/example.tfvars\nterraform apply   -var-file vars/example.tfvars\nterraform destroy -var-file vars/example.tfvars\nИнит конфиг \nОсновная конфигурация зависит от двух файлов в проекте.\nlocals.defaults.tf - базовые значения, которые определены для всех наших кластеров.\nvars/${cluster_name}.tf - переменные, которые специально указаны для конкретного кластера.\nvars/${cluster_name}.tf \nglobal_vars = {\n    cluster_name    = \"example\"\n    pod_cidr        = \"10.102.0.0/16\"\n\n    serviceaccount_k8s_controllers_name = \"yandex-k8s-controllers\"\n\n    kube_apiserver_flags = {\n        oidc-issuer-url         = \"https://auth.dobry-kot.ru/auth/realms/master\"\n        oidc-client-id          = \"kubernetes-clusters\"\n        oidc-username-claim     = \"sub\"\n        oidc-groups-claim       = \"groups\"\n        oidc-username-prefix    = \"-\"\n    }\n\n    kube_controller_manager_flags = {\n        cluster-name = \"kubernetes\"\n    }\n\n    kube_scheduler_flags = {\n        \n    }\n\n    addons = {\n        cilium = {\n            enabled = true\n            extra_values = {\n                cluster = {\n                    name = \"example\"\n                    id = 12\n                }\n            }\n        }\n\n        vault-issuer = {\n            enabled = true\n            extra_values = {}\n        }\n\n        coredns = {\n            enabled = true\n            extra_values = {}\n        }\n\n        gatekeeper = {\n            enabled = true\n            extra_values = {}\n        }\n\n        certmanager = {\n            enabled = true\n            extra_values = {}\n        }\n\n        machine-controller-manager = {\n            enabled = true\n            extra_values = {}\n        }\n\n        yandex-cloud-controller = {\n            enabled = true\n            extra_values = {}\n        }\n\n        yandex-csi-controller = {\n            enabled = true\n            extra_values = {}\n        }\n\n        compute-instance = {\n            enabled = true\n            custom_values = {\n                subnet_id   = \"e9bndv0b3c5asheadg09\"\n                zone        = \"ru-central1-a\"\n                image_id    = \"fd8ingbofbh3j5h7i8ll\"\n                replicas    = 1\n            }\n            extra_values = {\n                metadata = {\n                    nodeLabels = {\n                        \"node-role.kubernetes.io/worker\" = \"\"\n                        \"provider\" = \"yandex\"    \n                    }\n                    cloudLabels = {\n                        tair = \"critical\"\n                    }\n                }\n            }\n        }\n    }\n\n}\n\ncloud_metadata = {\n    cloud_name  = \"cloud-uid-vf465ie7\"\n    folder_name = \"example\"\n}\n\nmaster_group = {\n    name                = \"master\"\n    count               = 3\n\n    default_subnet      = \"10.0.0.0/24\"\n    default_zone        = \"ru-central1-a\"\n\n    metadata = {\n        # user_data_template = \"fraima-hbf\"\n        user_data_template = \"fraima\"\n    }\n}\n\nЭтот ENV-параметр дает возможность изменить значения, которые будут использованы в конфигурационных файлах или ресурсах в будущем.\nНапример, мы можем изменить или добавить флаги Kube-apiserver с помощью переменной \"kube_apiserver_flags\".\nВ файле с переменными на данный момент определены три группы.\n\"master_group\" определяет, какие мастера следует заказать, в какой подсети они будут находиться, в какой зоне, будут ли они в разных зонах или нет, а также количество мастер-нод (это значение можно определить только один раз, изменить его с 1 на 3 в настоящее время невозможно).\n\"global_vars\" определяет будущую конфигурацию кластера, включая его имя, подсети для подов, флаги для компонент, которые будут использоваться, а также какие аддоны будут добавлены.\n \"cloud_metadata\" содержатся указатели на облачный провайдер, такие как cloud_name\" и \"folder_name\".\nЗапускаем\ntime terraform apply -var-file vars/example.tfvars  -auto-approve\nПо умолчанию будет развернут кластер с тремя мастер-нодами, каждая из которых имеет 6 CPU, 12 ГБ оперативной памяти и 100 ГБ дискового пространства, а также 10 ГБ для ETCD.\nДля каждого кластера будет создан внешний балансер, к которому вы сможете подключиться. Также будут созданы аддоны, которые настроят сеть, базовые интеграции с YC, такие как CSI driver, Cloud Controller и Machine Controller Manager для заказа воркер-нод в облаке.\nЧерез шесть минут вы получите полностью готовый кластер и инструкции о том, как подключиться к нему.\nApply complete! Resources: 86 added, 0 changed, 0 destroyed.\n\nOutputs:\n\nLB-IP = \"kubectl config set-cluster  cluster --server=https://158.160.63.64:443 --insecure-skip-tls-verify\"\n\nreal    6m4,698s\nuser    0m24,182s\nsys     0m1,582s\n\ndk@dobry-kot-system:~/workspace/fraima/kubernetes/k8s-yandex-cluster-naked$ kubectl get nodes -o wide\nNAME                STATUS   ROLES                  AGE     VERSION    INTERNAL-IP   EXTERNAL-IP      OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nmaster-50d858e0-1   Ready    control-plane,master   9m34s   v1.23.12   10.0.0.12     158.160.51.95    Ubuntu 20.04.4 LTS   5.4.0-124-generic   containerd://1.6.6\nmaster-50d858e0-2   Ready    control-plane,master   9m33s   v1.23.12   10.0.0.6      158.160.38.139   Ubuntu 20.04.4 LTS   5.4.0-124-generic   containerd://1.6.6\nmaster-50d858e0-3   Ready    control-plane,master   9m34s   v1.23.12   10.0.0.19     158.160.42.200   Ubuntu 20.04.4 LTS   5.4.0-124-generic   containerd://1.6.6\nВы можете заметить, что кластер успешно запущен и функционирует. Кроме того, у узлов теперь есть внешние IP-адреса и свидетельствует о том, что интеграция с YC работает.\nЕсли вы используете keycloak для подключения, не забудьте установить плагин \"kubectl login\" и воспользоваться универсальным kubeconfig.\nkubeconfig\napiVersion: v1\nclusters:\n- cluster:\n    insecure-skip-tls-verify: true\n    server: https://158.160.63.64:443\n  name: cluster\ncontexts:\n- context:\n    cluster: cluster\n    namespace: kube-fraima-machine-controller-manager\n    user: cluster\n  name: cluster\ncurrent-context: cluster\nkind: Config\npreferences: {}\nusers:\n- name: cluster\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1beta1\n      args:\n      - oidc-login\n      - get-token\n      - --oidc-issuer-url=https://$KEYCLOAK-SERVER/auth/realms/master\n      - --oidc-client-id=kubernetes-clusters\n      - --oidc-client-secret=kube-client-secret\n      - --certificate-authority=/usr/local/share/ca-certificates/oidc-ca.pem\n      - --skip-open-browser\n      - --grant-type=password\n      - --username=$USERNAME\n      - --password=$PASSWORD\n      command: kubectl\n      env:\n      - name: context\n        value: $(kubectl config current-context)\n      interactiveMode: IfAvailable\n      provideClusterInfo: false\n\nНаполнение\nNAME                                     STATUS   AGE\ndefault                                  Active   11m\nkube-fraima-certmanager                  Active   8m48s # CERTMANAGER\nkube-fraima-dns                          Active   9m57s # COREDNS\nkube-fraima-machine-controller-manager   Active   8m55s\nkube-fraima-opa                          Active   9m44s # GATEKEEPER\nkube-fraima-sdn                          Active   10m   # CILIUM\nkube-fraima-yandex-cloud-controller      Active   11m\nkube-fraima-yandex-csi-controller        Active   9m54s\nkube-node-lease                          Active   11m\nkube-public                              Active   11m\nkube-system                              Active   11m\nВнимание\nОдной из важных особенностей этих кластеров является отсутствие приватных ключей от СА на мастерах, так как они хранятся в VAULT. Однако, такой подход приводит к определенным проблемам.\nВы можете добавить любую ноду в кластер через csr bootstraping, где нода генерирует запрос на сертификат и отправляет его в API, а затем вы подтверждаете этот запрос и нода получает свои сертификаты и добавляется в кластер. Однако, в данной инсталляции это нельзя сделать стандартными средствами.\nПоскольку kube-controller-manager занимается выдачей сертификатов для узлов, то без доступа к приватному ключу CA этот функционал теряется. Однако, мы нашли способ получить сертификаты, установив Certmanager и Gatekeeper, а затем настроив ClusterIssuer в Certmanager для интеграции с VAULT. С помощью этого ClusterIssuer можно будет выписывать сертификаты только для worker/master узлов. Затем в Gatekeeper настраиваем мутацию ресурса CSR, который изменит базовый SIGNERNAME с \"\nkubernetes.io/kubelet-serving\n\" на \"\nclusterissuers.cert-manager.io/vault-issuer\n\". Таким образом, мы сможем получить необходимые сертификаты.\ndk@dobry-kot-system:~/Downloads$ kubectl get csr\nNAME                                                   AGE     SIGNERNAME                                    REQUESTOR                       REQUESTEDDURATION   CONDITION\ncsr-52cx7                                              12m     kubernetes.io/kubelet-serving                 system:node:master-50d858e0-3   <none>              Pending\ncsr-lbhqf                                              12m     kubernetes.io/kubelet-serving                 system:node:master-50d858e0-1   <none>              Pending\ncsr-n27p4                                              12m     kubernetes.io/kubelet-serving                 system:node:master-50d858e0-2   <none>              Pending\nnode-csr-3l5VT-i7YinQWaTvbCY467d27GQLnSqnT_BYgk_PFII   8m17s   clusterissuers.cert-manager.io/vault-issuer   system:bootstrap:663273         <none>              Pending\n\nКак вы можете заметить, новый узел запросил сертификат через CSR, но SIGNERNAME у него установлен как \"\nclusterissuers.cert-manager.io/vault-issuer\n\". После подтверждения этого\n запроса Certmanager выдаст сертификат, который будет храниться во внешнем хранилище Vault.\nkubectl certificate approve node-csr-3l5VT-i7YinQWaTvbCY467d27GQLnSqnT_BYgk_PFII\nПосле этого появится еще один запрос на сертификат, который также нужно подтвердить, и после этого узел будет добавлен в кластер.\ndk@dobry-kot-system:~/Downloads$ kubectl get no -o wide\nNAME                                         STATUS   ROLES                  AGE   VERSION    INTERNAL-IP   EXTERNAL-IP      OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME\nmaster-50d858e0-1                            Ready    control-plane,master   24m   v1.23.12   10.0.0.12     158.160.51.95    Ubuntu 20.04.4 LTS   5.4.0-124-generic   containerd://1.6.6\nmaster-50d858e0-2                            Ready    control-plane,master   24m   v1.23.12   10.0.0.6      158.160.38.139   Ubuntu 20.04.4 LTS   5.4.0-124-generic   containerd://1.6.6\nmaster-50d858e0-3                            Ready    control-plane,master   24m   v1.23.12   10.0.0.19     158.160.42.200   Ubuntu 20.04.4 LTS   5.4.0-124-generic   containerd://1.6.6\nworker-yandex-compute-instance-68ffc-f2sd2   Ready    worker                 11m   v1.23.12   10.154.0.11   51.250.72.216    Ubuntu 22.04.1 LTS   5.15.0-46-generic   containerd://1.6.6\n\nПланы\nРасширить функционал Fraimctl, чтобы отказаться от использования Kubeadm.\nНаписать инфраструктурные модули для AWS и VK-Cloud.\nПокрыть Terraform тестами.\nОрганизовать модули более четко и удалить ненужное.\nПерейти с использования Terraform + Helm на Terraform + Flux.\nНаписать расширение для K8S API для добавления нашего кастомного функционала.\nДобавить инструмент для настройки узлов как Day2 операций.\nУ нашего коллектива амбициозные планы и мы нацелены на получение статуса CNCF. \nЕсли вы оценили наш контент, присоединяйтесь к нашему чату, где вы сможете задать любые интересующие вас вопросы. Мы также будем рады любой помощи в нашем проекте.\nКонтакты\nterraform modules: \nhttps://github.com/fraima/terraform-modules\nterraform cluster: \nhttps://github.com/fraima/kubernetes\ntelegram community:  \nhttps://t.me/fraima_ru\ntelegram me\n: \nhttps://t.me/Dobry_kot\n \n ",
    "tags": [
        "Linux",
        "devops",
        "sre",
        "k8s",
        "kubernetes"
    ]
}