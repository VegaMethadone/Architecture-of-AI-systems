{
    "article_id": "726954",
    "article_name": "Приложения в Kubernetes: быстрый запуск Kafka с KRaft",
    "content": "Автор статьи: Рустем Галиев\nIBM Senior DevOps Engineer & Integration Architect  \nПривет Хабр!\nСегодня у нас будет возможность установить Kafka с одной из самых простых конфигураций. Эта установка не оптимизирована для производственных сред, но идеально подходит для быстрой и локальной разработки.\nApache Kafka — это фреймворк, реализующий программную шину, использующую потоковую обработку. Это программная платформа с открытым исходным кодом, разработанная Apache Software Foundation и написанная на Scala и Java. Проект направлен на предоставление унифицированной платформы с высокой пропускной способностью и малой задержкой для обработки потоков данных в реальном времени.\nИными же словами распределенная система обмена сообщениями между серверными приложениями в режиме реального времени.\nЭта установка Kafka использует проект ранней версии под названием Apache Kafka Raft (KRaft). Это проект, в котором устранена зависимость от ZooKeeper.\nСперва нам будет полезно иметь реестр на этапах билда, пуша и деплоймента. Нет необходимости передавать частные образы через Интернет. Вместо этого мы сохраняем все в локальном реестре.\nУстановим реестр\nСуществует множество вариантов создания реестра образов контейнеров. Мы предпочитаем чистое решение Kubernetes и поэтому устанавливаем реестр через Docker Registry Helm Chart.\nДобавим репозиторий для установки диаграммы \nHelm\n:\nhelm repo add twuni https://helm.twun.io && helm repo list\nУстановите чарт для частного реестра контейнеров:\nhelm install registry twuni/docker-registry \\\n  --version 2.1.0 \\\n  --namespace kube-system \\\n  --set service.type=NodePort \\\n  --set service.nodePort=31500\nРеестр теперь доступен как услуга. Его можно вывести:\nkubectl get service --namespace kube-system\nНе забудьте назначить переменную среды общему расположению реестра:\nУ меня было\nexport REGISTRY=4f809e0c89d3406b8e4ccc59da3d2223-2887247877-31500-kira01.environments.vmoops.com\nПройдет несколько секунд, прежде чем развертывание реестра сообщит, что оно доступно:\nkubectl get deployments registry-docker-registry --namespace kube-system\nКак только реестр станет доступен, проверим содержимое пустого реестра:\ncurl $REGISTRY/v2/_catalog | jq -c\nВы увидите этот ответ реестра с ожидаемым пустым массивом: \n{\"repositories\":[]}\nЗапустим Кафка брокеры\nKafka — это распределенная система, в которой реализованы основные функции системы публикации-подписки. На каждом хосте в кластере Kafka работает сервер, называемый брокером, который хранит сообщения, отправленные в топики, и обслуживает запросы консьюмеров. В настоящее время Kafka использует ZooKeeper для отслеживания состояния брокеров в кластере Kafka и ведения списка топиков и сообщений Kafka.\nНа сегодня мы будем использовать ранний доступ и, возможно, будущую реализацию Kafka, использующую KRaft. Вместо того, чтобы полагаться на ZooKeeper, управление метаданными реализовано в ядре Kafka в виде набора контроллеров кворума. Как и ZooKeeper, они основаны на алгоритме консенсуса Raft, поэтому реализация является надежной и отказоустойчивой, и она обещает улучшить производительность и безопасность Kafka. Конфигурация KRaft также хорошо подходит для быстрой разработки.\nВ настоящее время нет официального образа контейнера, поддерживающего реализацию KRaft. Однако IBM предоставляет два файла — \nDockerfile\n и \nentrypoint.sh\n, которые были загружены в домашний каталог нашей ВМ.\nDockerfile:\nFROM openjdk:11\n\nENV KAFKA_VERSION=3.3.2\nENV SCALA_VERSION=2.13\nENV KAFKA_HOME=/opt/kafka\nENV PATH=${PATH}:${KAFKA_HOME}/bin\n\nLABEL name=\"kafka\" version=${KAFKA_VERSION}\n\nRUN wget -O /tmp/kafka_${SCALA_VERSION}-${KAFKA_VERSION}.tgz https://downloads.apache.org/kafka/${KAFKA_VERSION}/kafka_${SCALA_VERSION}-${KAFKA_VERSION}.tgz \\\n && tar xfz /tmp/kafka_${SCALA_VERSION}-${KAFKA_VERSION}.tgz -C /opt \\\n && rm /tmp/kafka_${SCALA_VERSION}-${KAFKA_VERSION}.tgz \\\n && ln -s /opt/kafka_${SCALA_VERSION}-${KAFKA_VERSION} ${KAFKA_HOME} \\\n && rm -rf /tmp/kafka_${SCALA_VERSION}-${KAFKA_VERSION}.tgz\n\nCOPY ./entrypoint.sh /\nRUN [\"chmod\", \"+x\", \"/entrypoint.sh\"]\nENTRYPOINT [\"/entrypoint.sh\"]\nentrypoint.sh:\nNODE_ID=${HOSTNAME:6}\nLISTENERS=\"PLAINTEXT://:9092,CONTROLLER://:9093\"\nADVERTISED_LISTENERS=\"PLAINTEXT://kafka-$NODE_ID.$SERVICE.$NAMESPACE.svc.cluster.local:9092\"\n\nCONTROLLER_QUORUM_VOTERS=\"\"\nfor i in $( seq 0 $REPLICAS); do\n\tif [[ $i != $REPLICAS ]]; then\n    \tCONTROLLER_QUORUM_VOTERS=\"$CONTROLLER_QUORUM_VOTERS$i@kafka-$i.$SERVICE.$NAMESPACE.svc.cluster.local:9093,\"\n\telse\n    \tCONTROLLER_QUORUM_VOTERS=${CONTROLLER_QUORUM_VOTERS::-1}\n\tfi\ndone\n\nmkdir -p $SHARE_DIR/$NODE_ID\n\nif [[ ! -f \"$SHARE_DIR/cluster_id\" && \"$NODE_ID\" = \"0\" ]]; then\n\tCLUSTER_ID=$(kafka-storage.sh random-uuid)\n\techo $CLUSTER_ID > $SHARE_DIR/cluster_id\nelse\n\tCLUSTER_ID=$(cat $SHARE_DIR/cluster_id)\nfi\n\nsed -e \"s+^node.id=.*+node.id=$NODE_ID+\" \\\n-e \"s+^controller.quorum.voters=.*+controller.quorum.voters=$CONTROLLER_QUORUM_VOTERS+\" \\\n-e \"s+^listeners=.*+listeners=$LISTENERS+\" \\\n-e \"s+^advertised.listeners=.*+advertised.listeners=$ADVERTISED_LISTENERS+\" \\\n-e \"s+^log.dirs=.*+log.dirs=$SHARE_DIR/$NODE_ID+\" \\\n/opt/kafka/config/kraft/server.properties > server.properties.updated \\\n&& mv server.properties.updated /opt/kafka/config/kraft/server.properties\n\nkafka-storage.sh format -t $CLUSTER_ID -c /opt/kafka/config/kraft/server.properties\n\nexec kafka-server-start.sh /opt/kafka/config/kraft/server.properties\nИз этих двух файлов можно создать надежный образ Kafka на основе KRaft. Создадим образ KRaft:\ndocker build -t $REGISTRY/kafka-kraft .\nСоздание образа контейнера займет несколько минут. Отправим образ KRaft в реестр образов контейнеров в локальном кластере Kubernetes, который был настроен ранее:\ndocker push $REGISTRY/kafka-kraft\nIBM также предоставляет манифест \nStatefulSet\n для запуска KRaft в Kubernetes \nkafka.yaml\n:\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: kafka-kraft\n---\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: kafka-pv-volume\n  labels:\n\ttype: local\nspec:\n  storageClassName: manual\n  capacity:\n\tstorage: 10Gi\n  accessModes:\n\t- ReadWriteOnce\n  hostPath:\n\tpath: '/mnt/data'\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: kafka-pv-claim\n  namespace: kafka-kraft\nspec:\n  storageClassName: manual\n  accessModes:\n\t- ReadWriteOnce\n  resources:\n\trequests:\n  \tstorage: 3Gi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kafka-svc\n  labels:\n\tapp: kafka-app\n  namespace: kafka-kraft\nspec:\n  clusterIP: None\n  ports:\n\t- name: '9092'\n  \tport: 9092\n  \tprotocol: TCP\n  \ttargetPort: 9092\n  selector:\n\tapp: kafka-app\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: kafka\n  labels:\n\tapp: kafka-app\n  namespace: kafka-kraft\nspec:\n  serviceName: kafka-svc\n  replicas: 3\n  selector:\n\tmatchLabels:\n  \tapp: kafka-app\n  template:\n\tmetadata:\n  \tlabels:\n    \tapp: kafka-app\n\tspec:\n  \tvolumes:\n    \t- name: kafka-storage\n      \tpersistentVolumeClaim:\n        \tclaimName: kafka-pv-claim\n  \tcontainers:\n    \t- name: kafka-container\n      \timage: $REGISTRY/kafka-kraft\n      \tports:\n        \t- containerPort: 9092\n        \t- containerPort: 9093\n      \tenv:\n        \t- name: REPLICAS\n          \tvalue: '3'\n        \t- name: SERVICE\n          \tvalue: kafka-svc\n        \t- name: NAMESPACE\n          \tvalue: kafka-kraft\n        \t- name: SHARE_DIR\n          \tvalue: /mnt/kafka\n      \tvolumeMounts:\n        \t- name: kafka-storage\n          \tmountPath: /mnt/kafka\nОдин из манифестов — это объявление \nPersistentVolume\n для хранения данных Kafka. Для этого создадим локальную директорию:\nmkdir /mnt/kafka\nИмея KRaft в реестре образов контейнеров, мы можем запустить KRaft:\nenvsubst < kafka.yaml | kubectl apply -f -\nenvsubst\n используется для ввода значения \n$REGISTRY в YAML\n. KRaft устанавливается в пространство имен kafka-kraft; переключим контекст на это пространство имен, чтобы последующие команды предполагали этот контекст:\nkubectl config set-context --current --namespace=kafka-kraft\nПроверим статус установки:\nkubectl get services,statefulsets,pods,pv,pvc\nКогда хотя бы первый брокер Kafka запущен, проверьте его логи:\nkubectl logs kafka-0\nЕсли вы видите ошибки в логах, такие как \nError connecting to node \n(Ошибка подключения к узлу), в настоящее время являются нормальной активностью, поскольку брокеры пытаются соединиться друг с другом при запуске.\nЧерез несколько секунд поды будут сообщать о том, что они запущены, а \nStatefulSet\n сообщит о готовности 3/3:\nkubectl get services,statefulsets,pods | grep -z 'Running\\|3/3\\|kafka-svc\\|9092'\nОбратите внимание, что служба Kafka доступна в кластере по адресу \nkafka-svc:9092\n.\nВ завершение хочу пригласить вас на \nбесплатный урок\n, где мои коллеги из OTUS расскажут как устроен мониторинг кластера, его компоненты и приложения в кластере. Вы изучите различные подходы мониторинга, подходы к мониторингу как приложения так и компонентов кластера, основные метрики Kubernetes. Также узнаете про кластеризацию/федерацию Prometheus, дополнительные хранилища метрик для prometheus (victoria metrics; thanos, cortex).\nЗарегистрироваться на бесплатный вебинар\n \n ",
    "tags": [
        "kafka",
        "kraft",
        "kubernetes"
    ]
}