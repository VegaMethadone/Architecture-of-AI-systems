{
    "article_id": "727318",
    "article_name": "Снижаем размерность эмбеддингов предложений для задачи определения семантического сходства",
    "content": "Привет, Хабр!\nМеня зовут Николай Шукан, я Data Scientist и участник \nпрофессионального сообщества NTA\n. Сегодня речь пойдет о методах снижения размерности эмбеддингов для задач определения семантического сходства предложений.\nДля чего это необходимо?\n С каждым годом растет сложность моделей, решающих вопросы семантически‑ и контекстно‑ориентированной обработки естественного языка (\nNLP\n). Также нельзя забывать и про проблемы мультиязычности моделей. Все это сильно сказывается на увеличении их размеров и системных требований к железу для их обучения, дообучения, да и просто запуска. Задачи NLP сегодня — это прикладные задачи, их хочется решать на доступном оборудовании и за разумное время.\nА если поконкретней? \nПеред мной стояла задача найти и обобщить текстовые данные, представляющие собой массив предложений. Я точно знал, что среди них есть семантически схожие фразы. Однако прямой подход для определения семантического сходства наборов фраз требовал много памяти и времени. Чтобы решить эту проблему, я попытался уменьшить размерность векторов признаков предложений, но как понять, когда остановиться и что это даст?\nНу и как понять? \nВ рамках данного поста посмотрим, как меняется оценка семантического сходства от изменения размерностей эмбеддингов разными классическими методами их уменьшения.\nНавигация по посту\nВведение\nВ объятиях Hugging Face\nРазберёмся с датасетом\nНачинаем снижение\nИтог\nВведение\nМера степени семантического сходства между парами предложений — любопытный аспект NLP. Отлично справляется с группировкой схожих по смыслу предложений и предикатов, выискивает повторы, убрав которые можно снизить нагруженность модели, которая будет обрабатывать наши текстовые данные.\nДля решения задачи семантического сходства между предложениями нужно преобразовать их в вектор. Для этого воспользуемся эмбеддингом: на вход подается набор предложений (в нашем случае — два для попарного сравнения), а на выходе преобразуется в числовые векторы этих предложений. Собственно, под эмбеддингом будем понимать результат преобразования текстовой сущности в числовой вектор. Модель преобразования выглядит так:\nНа сегодня самый простой и эффективный способ создания контекстуализированных эмбеддингов — использовать \nтрансформер\n, модель машинного обучения, отлично зарекомендовавшую себя в сфере NLP.\nАрхитектурно трансформеры схожи с RNN (система кодировщик‑декодировщик). Кроме того, они также предназначены для обработки последовательностей. Архитектура трансформера выглядит следующим образом:\nИсточник\nВ отличие от RNN трансформеры не требуют обработки последовательностей по порядку: если входные данные это текст, то трансформеру не требуется обрабатывать конец текста после обработки его начала. Благодаря этому трансформеры распараллеливаются легче чем RNN и оттого выигрывают в скорости обучения. Также важной особенностью данной архитектуры является механизм внимания — он фокусируется на важных с точки зрения контекста словах и отдает их напрямую в обработку. Благодаря этому трансформеры обладают хорошей долгосрочной памятью и лучшим умением учитывать контекст.\nВсе это делает трансформеры выбором номер один для решения нашей задачи семантического сходства.\nТаким образом, получим следующую модель определения семантического сходства:\nФункция метрики будет заключаться в определении близости полученных векторов. Для этого можно использовать разные инструменты, мы остановимся на косинусном сходстве:\nгде A и B — n‑мерные вектора, θ — угол между ними, A∙B — скалярное произведение векторов A и B, ||A|| и ||B|| — длины векторов в евклидовом пространстве, A\ni\n и B\ni \n— i‑ые компоненты векторов A и B соответственно.\nДля уменьшения количества признаков предложений, воспользуемся классическими методами снижения размерностей. Цель — уменьшить количество слабо информативных признаков для облегчения модели, но не потерять в качестве информации, т. е. получить значение семантического сходства как минимум не значительно хуже, чем до уменьшения размерности.\nСхема снижения размерности будет выглядеть следующим образом:\nПолученные эмбеддинги пониженной размерности также отправляются на вход в функцию метрики, где уже вычисляется семантическое сходство. \nПриступим к реализации описанной схемы. \nВ объятиях Hugging Face\nДля подбора датасета и создания модели семантического сходства я обратился к замечательной платформе \nHugging Face\n.\nSTSb Multi MT\n — это набор мультиязычных переводов и англоязычный оригинал классического STSbenchmark. Датасет состоит из трех колонок: первое предложение, второе предложение и метрика их схожести от 0 до 5 (далее — эталонная оценка). Датасет разбит на 3 части — train, test и dev. Так как в рамках поста вопросы точной донастройки рассматриваться не будет, то ограничимся dev сплитом в 1,5 тыс. строк русскоязычной части датасета.\n# загрузим датасет\ndf_dev = load_dataset(\"stsb_multi_mt\", name=\"ru\", split=\"dev\")\nПервые пять строк датасета:\nsentence1\nsentence2\nsimilarity_score\n\"Человек в твердой шляпе   танцует.\"\n\"Мужчина в твердой шляпе танцует.\"\n5\n\"Маленький ребенок едет верхом на   лошади.\"\n\"Ребенок едет на лошади.\"\n4.75\n\"Мужчина кормит мышь змее.\"\n\"Человек кормит змею мышью.\"\n5\n\"Женщина играет на гитаре.\"\n\"Человек играет на гитаре.\"\n2.4\n\"Женщина играет на флейте.\"\n\"Человек играет на флейте.\"\n2.75\nСреди моделей мой выбор пал на \ndistiluse‑base‑multilingual‑cased‑v1\n из семейства \nsentence‑transformers\n. \nАрхитектура модели выглядит следующим образом:\nНа вход в модель подается предложение. Оно проходит через слой трансформера (DistilBertModel) и преобразуется в эмбеддинг, который через слой пулинга попадает на полносвязный слой Dense с вектором смещения (bias) и тангенсальной активационной функцией. На выходе получаем эмбеддинг с размерностью 512.\nДанная модель отображает предложения в 512-мерное векторное пространство.\n# загрузим модель\nmodel = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")\nРазберёмся с датасетом\nОбработаем наш датасет. Для чего нормализуем эталонную оценку, приведя ее к значениям от 0 до 1. Из модели вытащим эмбеддинги и рассчитаем косинусное сходство, которое и послужит основой для сравнения с целевой метрикой датасета и измененной метрикой после снижения размерности.\nres = []\nF = True\nfor df in df_dev:\n    score = float(df['similarity_score'])/5.0 # нормализация эталонной оценки\n    embeddings = model.encode([df['sentence1'], df['sentence2']])\n    semantic_sim = 1 - cosine(embeddings[0], embeddings[1]) # косинусное сходство между парами предложений\n    res.append([df['sentence1'], df['sentence2'], score, semantic_sim])\n    if F == True:\n        mas_embed = embeddings\n        F = False\n    else:    \n        mas_embed = np.concatenate((mas_embed, embeddings), axis=0)\nСоберем все в единый датафрейм:\ndf = pd.DataFrame(res, columns=['senetence1', 'sentence2', 'score', 'semantic_sim'])\nНачинаем снижение\nТеперь можем приступить к применению методов снижения размерности.\nЯ выбрал четыре метода доступных в модуле scikit‑learn.decomposition — Матричная декомпозиция:\nPCA\n — Метод главных компонент.\nFastICA\n — Быстрый алгоритм для Анализа независимых компонент.\nFactor Analysis\n (FA) — Факторный анализ.\nTruncatedSVD\n — Усеченное сингулярное разложение.\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import FactorAnalysis\nfrom sklearn.decomposition import TruncatedSVD\nДля сравнения методов между собой и с эталонным значением воспользуемся евклидовым расстоянием: \nгде x — вектор эталонных оценок, y\nm\n — вектор приближений при сокращении размерности до m, n — число пар предложений, для которых надо рассчитать косинусное сходство, x\ni \nи y\ni\n — i‑ые элементы соответствующих векторов.\nЭталонная оценка\nСемантическое   сходство\nСемантическое   сходство с размерностью 50 методом ICA\n1.0000\n0.958966\n0.953331\n0.9500\n0.903258\n0.909175\n1.0000\n0.938772\n0.916701\n0.4800\n0.828721\n0.835421\n0.5500\n0.805219\n0.771535\n0.5230\n0.783895\n0.762820\nТаким образом, получим вектор эталонных оценок, основной вектор приближений (семантическое сходство), вектора приближений n‑ой размерности и i‑го метода (например, размерность 50 и метод ICA).\nПриведем пример кода для метода ICA:\neucl_dis_ica = []\nfor el in dims:\n    ica = FastICA(n_components = el)\n    mas_embed_fit = ica.fit_transform(mas_embed)\n    \n    # семантическое сходство\n    tmp_res = []\n    for i in range (0, 3000, 2):\n        semantic_sim = 1 - cosine(mas_embed_fit[i], mas_embed_fit[i+1])\n        tmp_res.append(semantic_sim)\n    \n    # евклидово расстояние \n    df[f'reduce_sim_ica_{el}'] = tmp_res\n    df['eucl_dis_ica'] = (df['score'] - df[f'reduce_sim_ica_{el}'])**2\n    eucl_dis_ica.append(df['eucl_dis_ica'].sum() ** 0.5)\nИтого, получим функцию зависимости евклидового расстояния и числа размерностей. Найдя локальный минимум евклидового расстояния до эталонной оценки на интересующем нас интервале [50; 450] размерностей, получим оптимальное количество размерностей, где нет существенных потерь информации.\nВизуализируем рассчитанные данные:\nplt.figure(figsize=(12,7.5), dpi= 80)\nplt.plot(dims, eucl_dis_pca, color='tab:red', label='PCA')\nplt.text(dims[-1], eucl_dis_pca[-1], 'PCA', fontsize=12, color='tab:red')\nplt.plot(dims, eucl_dis_ica, color='tab:blue', label='ICA')\nplt.text(dims[-1], eucl_dis_ica[-1], 'ICA', fontsize=12, color='tab:blue')\nplt.plot(dims, eucl_dis_fa, color='tab:green', label='FA')\nplt.text(dims[-1], eucl_dis_fa[-1], 'FA', fontsize=12, color='tab:green')\nplt.plot(dims, eucl_dis_tsvd, color='tab:green', label='TSVD')\nplt.text(dims[-1], eucl_dis_tsvd[-1], 'TSVD', fontsize=12, color='tab:green')\nplt.plot(dims, targ, color='tab:orange', label='Target', linestyle='dashed')\n\nplt.ylabel('Евклидово расстояние')\nplt.xlabel('Размерность')\nplt.legend(loc='upper right', ncol=2, fontsize=12)\n\nplt.show()\nОранжевая пунктирная линия на графике (target) — это значение евклидового расстояния между вектором эталонных оценок и вектором семантического сходства (т. е. основным вектором приближения без уменьшения размерностей). С этим значением мы и будем сравнивать получившиеся функции методов снижения размерностей.\nИз графика видно, что:\nАлгоритмы ICA и FA отработали лучше всего и приблизились к эталонной оценке даже больше, чем target, с локальным минимум около 200 размерностей (что в 2.5 раза меньше начальных 512).\nАлгоритм PCA показал себя чуть хуже, но при этом при 200 размерностях уже совпал с target.\nАлгоритм TSVC в чистом виде не позволяет эффективно снизить количество размерностей.\nИтог\nРассмотренный алгоритм позволяет упростить модель за счет уменьшения числа избыточных, слабо информативных признаков, и это позволяет:\nСнизить объем обрабатываемых многомерных эмбеддингов\n. Это также уменьшает объем задействуемой памяти и увеличивает скорость работы дальнейшей обработки этих данных. На конкретном примере сокращение объема данных составило около 60%.\nПовысить эффективность модели\n. После подбора оптимального сочетания метода снижения размерности и целевого количества измерений удалось добиться сохранения качества данных и даже получить результат лучше, чем при исходных параметрах.\nПосмотреть весь код можно под спойлером или на \nGitHub\n.\nРазвернуть код\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.spatial.distance import cosine\nfrom sentence_transformers import SentenceTransformer\nfrom datasets import load_dataset\n\n# загрузим модель\nmodel = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")\n\n# загрузим датасет\ndf_dev = load_dataset(\"stsb_multi_mt\", name=\"ru\", split=\"dev\")\n\n# конвертнем датасет в датафрейм\nres = []\nF = True\nfor df in df_dev:\n    score = float(df['similarity_score'])/5.0 # нормализация эталонной оценки\n    embeddings = model.encode([df['sentence1'], df['sentence2']])\n    semantic_sim = 1 - cosine(embeddings[0], embeddings[1]) # косинусное сходство между парами предложений\n    res.append([df['sentence1'], df['sentence2'], score, semantic_sim])\n    if F == True:\n        mas_embed = embeddings\n        F = False\n    else:    \n        mas_embed = np.concatenate((mas_embed, embeddings), axis=0)\n\ndf = pd.DataFrame(res, columns=['sentence1', 'sentence2', 'score', 'semantic_sim'])\n        \nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import FastICA\nfrom sklearn.decomposition import FactorAnalysis\nfrom sklearn.decomposition import TruncatedSVD\n\n# создадим список размерностей\ndims = [x for x in range(50, 451, 50)]\n\n# рассчитаем Евклидово расстояние для базовой модели\ndf['eucl_dis'] = (df['score'] - df['semantic_sim'])**2\ntmp_targ = df['eucl_dis'].sum() ** 0.5\ntarg = [tmp_targ for _ in range(len(dims))]\n\n# для каждого метода уменьшения размерности\n# найдем эмбеддинги новых размерностей\n# и для каждой пары предложений косинусное сходство\n# для каждой размерности найдем евклидово расстояние до эталонной оценки \n\n# ICA\neucl_dis_ica = []\nfor el in dims:\n    ica =  FastICA(n_components = el)\n    mas_embed_fit = ica.fit_transform(mas_embed)\n    \n    # семантическое сходство\n    tmp_res = []\n    for i in range (0, 3000, 2):\n        semantic_sim = 1 - cosine(mas_embed_fit[i], mas_embed_fit[i+1])\n        tmp_res.append(semantic_sim)\n    \n    # евклидово расстояние \n    df[f'reduce_sim_ica_{el}'] = tmp_res\n    df['eucl_dis_ica'] = (df['score'] - df[f'reduce_sim_ica_{el}'])**2\n    eucl_dis_ica.append(df['eucl_dis_ica'].sum() ** 0.5)\n\n# PCA\neucl_dis_pca = []\nfor el in dims:\n    pca =  PCA(n_components = el)\n    mas_embed_fit = pca.fit_transform(mas_embed)\n    \n    # семантическое сходство\n    tmp_res = []\n    for i in range (0, 3000, 2):\n        semantic_sim = 1 - cosine(mas_embed_fit[i], mas_embed_fit[i+1])\n        tmp_res.append(semantic_sim)\n    \n    # евклидово расстояние\n    df[f'reduce_sim_pca_{el}'] = tmp_res\n    df['eucl_dis_pca'] = (df['score'] - df[f'reduce_sim_pca_{el}'])**2\n    eucl_dis_pca.append(df['eucl_dis_pca'].sum() ** 0.5)\n\n# FA\neucl_dis_fa = []\nfor el in dims:\n    fa =  FactorAnalysis(n_components = el)\n    mas_embed_fit = fa.fit_transform(mas_embed)\n    \n    # семантическое сходство\n    tmp_res = []\n    for i in range (0, 3000, 2):\n        semantic_sim = 1 - cosine(mas_embed_fit[i], mas_embed_fit[i+1])\n        tmp_res.append(semantic_sim)\n    \n    # евклидово расстояние\n    df[f'reduce_sim_fa_{el}'] = tmp_res\n    df['eucl_dis_fa'] = (df['score'] - df[f'reduce_sim_fa_{el}'])**2\n    eucl_dis_fa.append(df['eucl_dis_fa'].sum() ** 0.5)\n\n# TSVD\neucl_dis_tsvd = []\nfor el in dims:\n    tsvd =  TruncatedSVD(n_components = el)\n    mas_embed_fit = tsvd.fit_transform(mas_embed)\n    \n    # семантическое сходство\n    tmp_res = []\n    for i in range (0, 3000, 2):\n        semantic_sim = 1 - cosine(mas_embed_fit[i], mas_embed_fit[i+1])\n        tmp_res.append(semantic_sim)\n    \n    # евклидово расстояние\n    df[f'eucl_dis_tsvd_{el}'] = tmp_res\n    df['eucl_dis_tsvd'] = (df['score'] - df[f'eucl_dis_tsvd_{el}'])**2\n    eucl_dis_tsvd.append(df['eucl_dis_tsvd'].sum() ** 0.5)\n\n# нарисуем график\nplt.figure(figsize=(12,7.5), dpi= 80)\n\nplt.plot(dims, eucl_dis_pca, color='tab:red', label='PCA')\nplt.text(dims[-1], eucl_dis_pca[-1], 'PCA', fontsize=12, color='tab:red')\nplt.plot(dims, eucl_dis_ica, color='tab:blue', label='ICA')\nplt.text(dims[-1], eucl_dis_ica[-1], 'ICA', fontsize=12, color='tab:blue')\nplt.plot(dims, eucl_dis_fa, color='tab:green', label='FA')\nplt.text(dims[-1], eucl_dis_fa[-1], 'FA', fontsize=12, color='tab:green')\nplt.plot(dims, eucl_dis_tsvd, color='tab:green', label='TSVD')\nplt.text(dims[-1], eucl_dis_tsvd[-1], 'TSVD', fontsize=12, color='tab:green')\nplt.plot(dims, targ, color='tab:orange', label='Target', linestyle='dashed')\n\nplt.ylabel('Евклидово расстояние')\nplt.xlabel('Размерность')\n\nplt.legend(loc='upper right', ncol=2, fontsize=12)\n\nplt.show()\n \n ",
    "tags": [
        "эмбеддинги",
        "эмбеддинг",
        "обработка текстов",
        "nlp",
        "natural language processing",
        "обработка текста"
    ]
}