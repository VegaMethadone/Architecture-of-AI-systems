{
    "article_id": "729048",
    "article_name": "Регрессионный анализ в DataScience. Часть 3. Аппроксимация",
    "content": "  АКТУАЛЬНОСТЬ ТЕМЫ\n В предыдущих обзорах (\nhttps://habr.com/ru/articles/690414/\n, \nhttps://habr.com/ru/articles/695556/\n) мы рассматривали линейную регрессию. Пришло время переходить к нелинейным моделями. Однако, прежде чем рассматривать полноценный нелинейный регрессионный анализ, остановимся на аппроксимации зависимостей.\nПро аппроксимацию написано так много, что, кажется, и добавить уже нечего. Однако, кое-что добавить попытаемся.\nПри выполнении анализа данных может возникнуть потребность оперативно построить аналитическую зависимость. Подчеркиваю - речь не идет о полноценном регрессионном анализе со всеми его этапами, проверкой гипотез и т.д., а только лишь о подборе уравнения и оценке ошибки аппроксимации. Например, мы хотим оценить характер зависимости между какими-либо показателями в датасете и принять решение о целесообразности более глубокого исследования. Подобный инструмент предоставляет нам тот же Excel - все мы помним, как добавить линию тренда на точечном графике:\nТакой же инструмент необходим и при работе в Python, причем желательно, сохранив главное достоинство - оперативность и удобство использования, избавиться от недостатков, присущих Excel:\nограниченный набор аналитических зависимостей и метрик качества аппроксимации;\nневозможность построения нескольких зависимостей для одного набора данных;\nневозможность установления ограничений на значения параметров зависимостей;\nневозможность устранить влияние выбросов.\nИспользованием подобных инструментов мы и рассмотрим в данном обзоре.\nПрименение пользовательских функций\nКак и в предыдущих обзорах, здесь будут использованы несколько пользовательских функций для решения разнообразных задач. Все эти функции созданы для облегчения работы и уменьшения размера программного кода. Данные функции загружается из пользовательского модуля \nmy_module__stat.py\n, который доступен в моем репозитории на GitHub (\nhttps://github.com/AANazarov/MyModulePython\n).\nВот перечень данных функций:\ngraph_lineplot_sns\n - функция позволяет построить линейный график средствами seaborn и сохранить график в виде png-файла;\ngraph_scatterplot_sns\n - функция позволяет построить точечную диаграмму средствами seaborn и сохранить график в виде png-файла;\nregression_error_metrics\n  - функция возвращает ошибки аппроксимации регрессионной модели;\nПользовательскую функцию \nsimple_approximation\n мы создаем в процессе данного обзора (она тоже включена в пользовательский модуль \nmy_module__stat.py\n).\nИСХОДНЫЕ ДАННЫЕ\nВ качестве примера в данном обзоре продолжим рассматривать задачу нахождения зависимости \nсреднемесячного расхода топлива автомобиля (л/100 км) (FuelFlow)\n от \nсреднемесячного пробега (км) (Mileage)\n и \nсреднемесячной температуры (Temperature)\n (этот же датасет я использовал в своих статьях: \nhttps://habr.com/ru/post/683442/\n, \nhttps://habr.com/ru/post/695556/\n).\n# Общий заголовок проекта\nTask_Project = \"Analysis of fuel consumption of a car\"\n\n# Заголовок, фиксирующий момент времени\nAsOfTheDate = \"\"\n\n# Заголовок раздела проекта\nTask_Theme = \"\"\n\n# Общий заголовок проекта для графиков\nTitle_String = f\"{Task_Project}\\n{AsOfTheDate}\"\n\n# Наименования переменных\nVariable_Name_T_month = \"Monthly data\"\nVariable_Name_Y = \"FuelFlow (liters per 100 km)\"\nVariable_Name_X1 = \"Mileage (km)\"\nVariable_Name_X2 = \"Temperature (°С)\"\nЗагрузим исходные данные из csv-файла. Это уже обработанный датасет, готовый для анализа (первичная обработка данных выполненная в отдельном файле \nPreparation of input \ndata.py\n, который также доступен в моем репозитории на GitHub).\nСтолбцы таблицы\n:\nMonth\n — месяц (в формате Excel)\nMileage\n - месячный пробег (км)\nTemperature\n - среднемесячная температура (°C)\nFuelFlow\n - среднемесячный расход топлива (л/100 к\ndataset_df = pd.read_csv(filepath_or_buffer='data/dataset_df.csv', sep=';')\nСохранение данных\nСохраняем данные в виде отдельных переменных (для дальнейшего анализа).\nСреднемесячный расход топлива (л/100 км) / Fuel Flow (liters per 100 km):\nY = np.array(dataset_df['Y'])\nПробег автомобиля за месяц (км) / Mileage (km):\nX1 = np.array(dataset_df['X1'])\nСреднемесячная температура (°С) / Temperature (degrees celsius):\nX2 = np.array(dataset_df['X2'])\nВизуализация\nГраницы значений переменных (при построении графиков):\n(X1_min_graph, X1_max_graph) = (0, 3000)\n(X2_min_graph, X2_max_graph) = (-20, 25)\n(Y_min_graph, Y_max_graph) = (0, 30)\nПостроение графика:\ngraph_scatterplot_sns(\n    X1, Y,\n    Xmin=X1_min_graph, Xmax=X1_max_graph,\n    Ymin=Y_min_graph, Ymax=Y_max_graph,\n    color='orange',\n    title_figure=Title_String, title_figure_fontsize=14,\n    title_axes='Dependence of FuelFlow (Y) on mileage (X1)', title_axes_fontsize=16,\n    x_label=Variable_Name_X1,\n    y_label=Variable_Name_Y,\n    label_fontsize=14, tick_fontsize=12,\n    label_legend='', label_legend_fontsize=12,\n    s=80)\nОСНОВЫ ТЕОРИИ \nКратко остановимся на некоторых теоретических вопросах.\nИнструменты Python для аппроксимации\nPython\n предоставляет нам большой набор инструментов для аппроксимации (подробнее см. \nhttps://docs.scipy.org/doc/scipy/reference/optimize.html\n). Мы остановимся на некоторых из них:\nфункция \nscipy.optimize.leastsq\n (\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.leastsq.html#scipy.optimize.leastsq\n)\nфункция \nscipy.optimize.least_squares\n (\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares\n)\nфункция \nscipy.optimize.curve_fit\n (\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n)\nфункция \nscipy.optimize.minimize\n (\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize\n)\nбиблиотека \nlmfit\n (\nhttps://lmfit.github.io/lmfit-py/\n)\n Основные виды моделей для аппроксимации\nНе будем углубляться в классификацию моделей, об этом написаны сотни учебников, монографий, справочников и статей. Однако, в целях унификации в таблице ниже все-таки приведем основные виды моделей для аппроксимации - разумеется, это список не является исчерпывающим, но охватывает более или менее достаточный набор моделей для практического использования:\nНаименование\nУравнение\nлинейная\n'\nквадратическая\nкубическая\nполиномиальная\nстепенная\nэкспоненциальная I типа\nэкспоненциальная II типа\nлогарифмическая\nобратная логарифмическая\nгиперболическая I типа\nгиперболическая II типа\nгиперболическая III типа\nлогистическая кривая I типа\nлогистическая кривая II типа (Перла-Рида)\nкривая Гомперца\nмодифицированная экспонента I типа\nмодифицированная экспонента II типа\nобобщенная логистическая кривая I типа\nобобщенная логистическая кривая II типа\nПримечания:\nНелинейные модели в ряде источников принято делить на 2 группы: \nмодели, нелинейные по факторам\n - путем замены переменных могут быть линеаризованы, т.е. приведены к линейному виду, и для оценки их параметров может применяться классический метод наименьших квадратов;\nмодели, нелинейные по параметрам\n - не могут быть линеаризованы, и для оценки их параметров необходимо применять итерационные методы.\nК моделям, нелинейным по параметрам, относятся, например, модифицированные кривые (модифицированная экспонента, обобщенная логистическая кривая), кривая Гомперца и пр. Оценка их параметров средствами Python требует особых приемов (предварительная оценка начальных значений). В данном разборе мы не будем особо останавливаться на этих моделях.\n Оценка ошибок и доверительных интервалов для параметров моделей аппроксимации\nЭкспоненциальные модели могут быть выражены в 2-х формах:\nс помощью собственно \nэкспоненциальной функции\n \n;\nв \nпоказательной форме\n \nНе будем особо распространяться на опасности увлечения полиномиальными моделями для аппроксимации данных, отметим только, что использовать такие модели нужно очень осторожно, с особым обоснованием и содержательным анализом задачи, а полиномы выше 3-й степени вообще использовать не рекомендуется.\n Метрики качества аппроксимации\nРассмотрим основные метрики качества:\nMean squared error (MSE)\n  - среднеквадратическая ошибка и \nRoot mean square error (RMSE)\n - квадратный корень из среднеквадратической ошибки \nMSE\n:\nОсобенности: тенденция к занижение качества модели, чувствительность к выбросам.\n 2. \nMean absolute error (MAE)\n - средняя абсолютная ошибка:\nОсобенность: гораздо менее чувствительна к выбросам, чем \nRMSE\n.\n 3. \nMean squared prediction error (MSPE)\n - среднеквадратическая ошибка прогноза (среднеквадратическая ошибка в процентах):\nОсобенности: нечувствительность к выбросам; нельзя использовать для наблюдений, в которых значения выходной переменной равны нулю.\n 4. \nMean absolute percentage error (MAPE)\n - средняя абсолютная ошибка в процентах:\nОсобенности: нельзя использовать для наблюдений, в которых значения выходной переменной равны нулю.\n 5. \nRoot Mean Square Logarithmic Error (RMSLE)\n - cреднеквадратичная логарифмическая ошибка, применяется, если разность между фактическим и предсказанным значениями различается на порядок и выше:\nОсобенности: нечувствительность к выбросам; смещена в сторону меньших ошибок (наказывает больше на недооценку, чем за переоценку); к значениям добавляется константа 1, так как логарифм 0 не определен.\n 6. \n - коэффициент детерминации:\nОсобенность: значение \n находится в пределах [0; 1], но иногда может принимать отрицательные значения (если ошибка модели больше ошибки среднего значения).\n Разумеется, приведенный перечень метрик качества аппроксимации не является исчерпывающим.\nСравнительный анализ метрик качества приведен в \nhttps://machinelearningmastery.ru/how-to-select-the-right-evaluation-metric-for-machine-learning-models-part-1-regrression-metrics-3606e25beae0/\n, \nhttps://machinelearningmastery.ru/how-to-select-the-right-evaluation-metric-for-machine-learning-models-part-2-regression-metrics-d4a1a9ba3d74/\n, \nhttps://loginom.ru/blog/quality-metrics\n.\nОценка ошибок и доверительных интервалов для параметров моделей аппроксимации\nИнструменты аппроксимации \nPython\n позволяют получить довольно обширный набор различных данных, в том числе тех, которые нам дают возможность оценить ошибки параметров аппроксимации и построить для них доверительные интервалы. Не вдаваясь глубоко в математическую теорию вопроса, кратко разберем, как это реализовать на практике.\nВ общем виде доверительный интервал для параметров аппроксимации можно представить в виде \n, где \n - значение j-го параметра модели, а \n - стандартная ошибка этого параметра.\nДля определения \n нам необходима матрица ковариации оценок параметров \n, диагональные элементы которой представляют собой дисперсию оценок параметров, то есть:\n имеет размерность \n, где \n - число оцениваемых параметров модели аппроксимации.\nТеперь разберем, как получить \n, используя различные инструменты аппроксимации \nPython\n:\nФункция \nscipy.optimize.curve_fit\n в стандартном наборе возвращаемых данных непосредственно содержит расчетную ковариационную матрицу \n (The estimated covariance of popt), которая обозначается в программном коде как \npcov\n (см. \nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html\n). Расчетная стандартная ошибка параметров может быть определена очень просто: \nperr = np.sqrt(np.diag(pcov))\n.\nФункция \nscipy.optimize.leastsq\n в стандартном наборе возвращаемых данных содержит матрицу, обратную матрице Гессе (The inverse of the Hessian) \n, которая обозначается в программном коде как \ncov_x\n. Чтобы получить \n, необходимо умножить \n на величину остаточной дисперсии\n:\nили в программном коде: \npcov = cov_x*MSE\n.\nВеличина \nSSE\n, необходимая для расчета \nMSE\n, может быть определена в программном коде как \n.\nФункция \nscipy.optimize.least_squares\n вместо \n возвращает модифицированную матрицу Якоби (Modified Jacobian matrix) \n, которая обозначается в программном коде как \nresult.jac\n (см. \nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html#scipy.optimize.least_squares\n, \nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html#scipy.optimize.OptimizeResult\n). Получить \n можно следующим образом:\nили в программном коде: \ncov_x = np.linalg.inv(np.dot(result.jac.T, result.jac))\n.\nФункция \nscipy.optimize.minimize\n имеет свои специфические особенности: возвращает обратную матрицу Гессе (Inverse of the objective function’s Hessian), которая обозначается в программном коде как \nres.hess_inv\n (см. \nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize\n, \nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html#scipy.optimize.OptimizeResult\n), но для определения \ncov_x\n необходимо воспользоваться приближенным выражением \ncov_x = 2*res.hess_inv\n (подробнее об этом см. \nhttps://stackoverflow.com/questions/40187517/getting-covariance-matrix-of-fitted-parameters-from-scipy-optimize-least-squares\n).\nБиблиотека \nlmfit\n позволяет непосредственно в стандартном наборе возвращаемых данных получить ошибки и доверительные интервалы параметров аппроксимации.\nБолее подробно с вопросом можно ознакомиться здесь:\nhttps://question-it.com/questions/9466217/poluchenie-kovariatsionnoj-matritsy-podobrannyh-parametrov-iz-metoda-scipy-optimizeleast_squares\nhttps://stackoverflow.com/questions/40187517/getting-covariance-matrix-of-fitted-parameters-from-scipy-optimize-least-squares\nhttps://stackoverflow.com/questions/14854339/in-scipy-how-and-why-does-curve-fit-calculate-the-covariance-of-the-parameter-es\nhttps://stackovergo.com/ru/q/4167751/in-scipy-how-and-why-does-curvefit-calculate-the-covariance-of-the-parameter-estimates\nhttps://math.stackexchange.com/questions/2349026/why-is-the-approximation-of-hessian-jtj-reasonable\nhttps://mmas.github.io/least-squares-fitting-numpy-scipy\nhttps://www.nedcharles.com/regression/Nonlinear_Least_Squares_Regression_For_Python.html\nАППРОКСИМАЦИЯ ЗАВИСИМОСТЕЙ\nОпределим набор зависимостей, которые будем использовать для аппроксимации. В данном обзоре мы рассмотрим набор наиболее широко распространенных зависимостей, не требующих особого подхода к вычислениям. Более сложные случаи - например, зависимости с асимптотами, зависимости нелинейные по параметрам и пр. - тема для отдельного рассмотрения.\n# equations\nlinear_func = lambda x, b0, b1: b0 + b1*x\nquadratic_func = lambda x, b0, b1, b2: b0 + b1*x + b2*x**2\nqubic_func = lambda x, b0, b1, b2, b3: b0 + b1*x + b2*x**2 + b3*x**3\npower_func = lambda x, b0, b1: b0 * (x**b1)\nexponential_type_1_func = lambda x, b0, b1: b0*np.exp(b1*x)\nexponential_type_2_func = lambda x, b0, b1: b0*np.power(b1, x)\nlogarithmic_func = lambda x, b0, b1: b0 + b1*np.log(x)\nhyperbolic_func = lambda x, b0, b1: b0 + b1/x\nДалее будем собственно выполнять аппроксимацию: оценивать параметры моделей, строить графики, причем так, чтобы на графиках отображались уравнения моделей и метрики качества (по аналогии с тем, как это позволяет сделать Excel - это очень удобно), рассмотрим возможность определения ошибок и доверительных интервалов для параметров аппроксимации.\nОтдельно довольно кратко остановимся на библиотеке \nlmfit\n, которая предоставляет очень широкий инструментарий для аппроксимации.\nАппроксимация с использованием scipy.optimize.leastsq\nОсобенности использования функции:\nРеализация метода наименьших квадратов, \nбез возможности\n введения ограничений на значения параметров.\nВозможность выбора метода оптимизации не предусмотрена.\nДанный пакет считается устаревшим по сравнению, с \nleast_squares\n.\nСсылка на документацию\n.\nfrom scipy.optimize import leastsq\n\n# list of model\nmodel_list = [\n    'linear', 'quadratic',\n    'qubic', 'power',\n    'exponential type 1', 'exponential type 2',\n    'logarithmic',\n    'hyperbolic']\n\n# model reference\nmodels_dict = {\n        'linear':             linear_func,\n        'quadratic':          quadratic_func,\n        'qubic':              qubic_func,\n        'power':              power_func,\n        'exponential type 1': exponential_type_1_func,\n        'exponential type 2': exponential_type_2_func,\n        'logarithmic':        logarithmic_func,\n        'hyperbolic':         hyperbolic_func}\n\n# initial values\np0_dict = {\n        'linear':             [0, 0],\n        'quadratic':          [0, 0, 0],\n        'qubic':              [0, 0, 0, 0],\n        'power':              [0, 0],\n        'exponential type 1': [0, 0],\n        'exponential type 2': [1, 1],\n        'logarithmic':        [0, 0],\n        'hyperbolic':         [0, 0]}\n\n# titles (formulas)\nformulas_dict = {\n        'linear':             'y = b0 + b1*x',\n        'quadratic':          'y = b0 + b1*x + b2*x^2',\n        'qubic':              'y = b0 + b1*x + b2*x^2 + b3*x^3',\n        'power':              'y = b0*x^b1',\n        'exponential type 1': 'y = b0*exp(b1^x)',\n        'exponential type 2': 'y = b0*b1^x',\n        'logarithmic':        'y = b0 + b1*ln(x)',\n        'hyperbolic':         'y = b0+b1/x'}\n\n# actual data\nX_fact = X1\nY_fact = Y\n\n# function for calculating residuals\nresidual_func = lambda p, x, y: y - func(x, *p)\n\n# return all optional outputs\nfull_output = True\n\n# variables to save the calculation results\ncalculation_results_df = pd.DataFrame(columns=['func', 'p0', 'popt', 'cov_x', 'SSE', 'MSE', 'pcov', 'perr', 'Y_calc', 'error_metrics'])\nerror_metrics_results_df = pd.DataFrame(columns=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE', 'R2'])\n\n# calculations\nfor func_name in model_list:\n    print(f'{func_name.upper()} MODEL: {formulas_dict[func_name]}')\n    func = models_dict[func_name]\n    calculation_results_df.loc[func_name, 'func'] = func\n    p0 = p0_dict[func_name]\n    print(f'p0 = {p0}')\n    calculation_results_df.loc[func_name, 'p0'] = p0\n    if full_output:\n        (popt, cov_x, infodict, mesg, ier) = leastsq(residual_func, p0, args=(X_fact, Y_fact), full_output=full_output)\n        integer_flag = f'ier = {ier}, the solution was found' if ier<=4 else f'ier = {ier}, the solution was not found'\n        print(integer_flag, '\\n', mesg)\n        calculation_results_df.loc[func_name, 'popt'] = popt\n        print(f'parameters = {popt}')\n        calculation_results_df.loc[func_name, 'cov_x'] = cov_x\n        print(f'cov_x = \\n{cov_x}')\n        #SSE = (residual_func(popt, X_fact, Y_fact)**2).sum()\n        SSE = (infodict['fvec']**2).sum()\n        calculation_results_df.loc[func_name, 'SSE'] = SSE\n        print(f'SSE = {SSE}')\n        MSE = SSE / (len(Y_fact)-len(p0))\n        calculation_results_df.loc[func_name, 'MSE'] = MSE\n        print(f'MSE = {MSE}')\n        pcov = cov_x * MSE\n        calculation_results_df.loc[func_name, 'pcov'] = pcov\n        print(f'pcov =\\n {pcov}')\n        perr = np.sqrt(np.diag(pcov))\n        calculation_results_df.loc[func_name, 'perr'] = perr\n        print(f'perr = {perr}\\n') \n    else:\n        popt = leastsq(residual_func, p0, args=(X_fact, Y_fact), full_output=full_output)\n        calculation_results_df.loc[func_name, 'popt'] = popt\n        print(f'parameters = {popt}\\n')\n    Y_calc = func(X_fact, *popt)\n    calculation_results_df.loc[func_name, 'Y_calc'] = Y_calc\n    (error_metrics_dict, error_metrics_df) = regression_error_metrics(Yfact=Y_fact, Ycalc=Y_calc, model_name=func_name)\n    calculation_results_df.loc[func_name, 'error_metrics'] = error_metrics_dict.values()\n    error_metrics_results_df = pd.concat([error_metrics_results_df, error_metrics_df])        \nLINEAR MODEL: y = b0 + b1*x\np0 = [0, 0]\nier = 3, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000 and the relative error between two consecutive iterates is at \n  most 0.000000\nparameters = [ 1.1850e+01 -2.1760e-03]\ncov_x = \n[[ 8.7797e-02 -7.3041e-05]\n [-7.3041e-05  7.6636e-08]]\nSSE = 84.42039151684142\nMSE = 1.5928375757894608\npcov =\n [[ 1.3985e-01 -1.1634e-04]\n [-1.1634e-04  1.2207e-07]]\nperr = [3.7396e-01 3.4938e-04]\n\nQUADRATIC MODEL: y = b0 + b1*x + b2*x^2\np0 = [0, 0, 0]\nier = 1, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000\nparameters = [ 1.4343e+01 -7.0471e-03  1.8760e-06]\ncov_x = \n[[ 2.8731e-01 -4.6282e-04  1.5012e-07]\n [-4.6282e-04  8.3812e-07 -2.9328e-10]\n [ 1.5012e-07 -2.9328e-10  1.1295e-13]]\nSSE = 53.26102533463595\nMSE = 1.0242504872045375\npcov =\n [[ 2.9428e-01 -4.7404e-04  1.5376e-07]\n [-4.7404e-04  8.5844e-07 -3.0039e-10]\n [ 1.5376e-07 -3.0039e-10  1.1569e-13]]\nperr = [5.4247e-01 9.2652e-04 3.4013e-07]\n\nQUBIC MODEL: y = b0 + b1*x + b2*x^2 + b3*x^3\np0 = [0, 0, 0, 0]\nier = 1, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000\nparameters = [ 1.4838e+01 -8.8040e-03  3.5207e-06 -4.1264e-10]\ncov_x = \n[[ 5.7657e-01 -1.4894e-03  1.1111e-06 -2.4112e-10]\n [-1.4894e-03  4.4815e-06 -3.7039e-09  8.5573e-13]\n [ 1.1111e-06 -3.7039e-09  3.3058e-12 -8.0107e-16]\n [-2.4112e-10  8.5573e-13 -8.0107e-16  2.0099e-19]]\nSSE = 52.41383349470524\nMSE = 1.0277222253863771\npcov =\n [[ 5.9256e-01 -1.5307e-03  1.1419e-06 -2.4780e-10]\n [-1.5307e-03  4.6057e-06 -3.8066e-09  8.7945e-13]\n [ 1.1419e-06 -3.8066e-09  3.3974e-12 -8.2328e-16]\n [-2.4780e-10  8.7945e-13 -8.2328e-16  2.0656e-19]]\nperr = [7.6978e-01 2.1461e-03 1.8432e-06 4.5449e-10]\n\nPOWER MODEL: y = b0*x^b1\np0 = [0, 0]\nier = 1, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000\nparameters = [33.4033 -0.1841]\ncov_x = \n[[ 1.2667e+01 -5.7225e-02]\n [-5.7225e-02  2.6284e-04]]\nSSE = 49.87968652194349\nMSE = 0.9411261607913867\npcov =\n [[ 1.1922e+01 -5.3856e-02]\n [-5.3856e-02  2.4737e-04]]\nperr = [3.4528 0.0157]\n\nEXPONENTIAL TYPE 1 MODEL: y = b0*exp(b1^x)\np0 = [0, 0]\nier = 1, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000\nparameters = [ 1.2624e+01 -2.7834e-04]\ncov_x = \n[[ 1.5566e-01 -1.1829e-05]\n [-1.1829e-05  1.1127e-09]]\nSSE = 75.19491109027253\nMSE = 1.4187719073636327\npcov =\n [[ 2.2085e-01 -1.6783e-05]\n [-1.6783e-05  1.5787e-09]]\nperr = [4.6994e-01 3.9733e-05]\n\nEXPONENTIAL TYPE 2 MODEL: y = b0*b1^x\np0 = [1, 1]\nier = 1, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000\nparameters = [12.6242  0.9997]\ncov_x = \n[[ 1.5566e-01 -1.1826e-05]\n [-1.1826e-05  1.1121e-09]]\nSSE = 75.19491108954423\nMSE = 1.4187719073498912\npcov =\n [[ 2.2085e-01 -1.6778e-05]\n [-1.6778e-05  1.5778e-09]]\nperr = [4.6994e-01 3.9722e-05]\n\nLOGARITHMIC MODEL: y = b0 + b1*ln(x)\np0 = [0, 0]\nier = 3, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000 and the relative error between two consecutive iterates is at \n  most 0.000000\nparameters = [23.9866 -2.1170]\ncov_x = \n[[ 2.1029 -0.3106]\n [-0.3106  0.0463]]\nSSE = 49.34256729321431\nMSE = 0.9309918357210247\npcov =\n [[ 1.9578 -0.2891]\n [-0.2891  0.0431]]\nperr = [1.3992 0.2075]\n\nHYPERBOLIC MODEL: y = b0+b1/x\np0 = [0, 0]\nier = 1, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000\nparameters = [  9.2219 315.7336]\ncov_x = \n[[ 2.2732e-02 -2.5920e+00]\n [-2.5920e+00  1.4765e+03]]\nSSE = 78.69038671392151\nMSE = 1.4847242776211607\npcov =\n [[ 3.3751e-02 -3.8485e+00]\n [-3.8485e+00  2.1922e+03]]\nperr = [ 0.1837 46.8207]\nТеперь сформируем отдельный DataFrame, в котором соберем все результаты аппроксимации - параметры моделей (\n, \n, \n, \n), метрики качества (\nMSE\n, \nRMSE\n, \nMAE\n, \nMSPE\n, \nMAPE\n, \nRMSLE\n, \n) и начальные условия (\n, \n, \n, \n).\nДля улучшения восприятия используем возможности Python по визуализации таблиц (\nhttps://pandas.pydata.org/pandas-docs/stable/user_guide/style.html\n):\nзаменим \nnan\n на \"-\";\nзначения параметров моделей, величина которых сравнима с установленным пределом точности отображения (\nDecPlace\n), выделим цветом (orange);\nдля параметра \n установим повышенную точность отображения, а для параметров \n, \n установим экспоненциальный формат;\nдля метрик качества установим выделение цветом наилучшего (green) и наихудшего (red) результата.\nПри этом нужно обратить внимание на следующее - пользовательская функция \nregression_error_metrics\n возвращает результат как в виде DataFrame, так и в виде словаря (dict):\nесли мы используем данные в виде DataFrame, то необходимо откорректировать значения признаков \nMSPE\n и \nMAPE\n, так как пользовательская функция \nregression_error_metrics\n возвращает значения этих признаков в строковом формате (из-за знака '%') и при определении наилучшего и наихудшего результатов могут возникнуть ошибки (значение '10%' будет считаться лучшим результатом, чем '9%' из-за того, что первым символом является единица), поэтому следует удалить знак '%' и изменить тип данных на float;\nесли мы используем данные в виде словаря (dict), то подобная проблема не возникает.\nresult_df = pd.DataFrame(\n                list(calculation_results_df.loc[:,'p0'].values),\n                columns=['p0', 'p1', 'p2', 'p3'],\n                index=model_list)\n\nresult_df = result_df.join(pd.DataFrame(\n                                list(calculation_results_df.loc[:,'popt'].values),\n                                columns=['b0', 'b1', 'b2', 'b3'],\n                                index=model_list))\n\nresult_df = result_df.join(pd.DataFrame(\n                                list(calculation_results_df.loc[:,'perr'].values),\n                                columns=['std(b0)', 'std(b1)', 'std(b2)', 'std(b3)'],\n                                index=model_list))\n\nresult_df = result_df.join(pd.DataFrame(\n                                list(calculation_results_df.loc[:,'error_metrics'].values),\n                                columns=['MSE', 'RMSE',\t'MAE', 'MSPE', 'MAPE', 'RMSLE', 'R2'],\n                                index=model_list))\n\ndisplay(result_df)\n\n# settings for displaying the DataFrame: result_df\nresult_df_leastsq = result_df.copy()\n'''for elem in ['MSPE', 'MAPE']:\n    result_df_leastsq[elem] = result_df_leastsq[elem].apply(lambda s: float(s[:-1]))'''\nprint('scipy.optimize.leastsq:'.upper())\ndisplay(result_df_leastsq\n        .style\n            .format(\n                precision=DecPlace, na_rep='-',\n                formatter={\n                    'b1': '{:.' + str(DecPlace + 3) + 'f}',\n                    'b2': '{:.' + str(DecPlace) + 'e}',\n                    'b3': '{:.' + str(DecPlace) + 'e}',\n                    'std(b1)': '{:.' + str(DecPlace + 3) + 'f}',\n                    'std(b2)': '{:.' + str(DecPlace) + 'e}',\n                    'std(b3)': '{:.' + str(DecPlace) + 'e}'})\n            .highlight_min(color='green', subset=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE'])\n            .highlight_max(color='red', subset=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE'])\n            .highlight_max(color='green', subset='R2')\n            .highlight_min(color='red', subset='R2')\n            .applymap(lambda x: 'color: orange;' if abs(x) <= 10**(-(DecPlace-1)) else None, subset=['b0', 'b1', 'b2', 'b3', 'std(b0)', 'std(b1)', 'std(b2)', 'std(b3)']))    \nСформируем DataFrame, содержащий ошибки и доверительные интервалы параметров моделей аппроксимации:\nresult_df_with_perr = pd.DataFrame()\n\nparameter_name_list = ['b0', 'b1', 'b2', 'b3']\nresult_df_with_perr[parameter_name_list] = result_df[parameter_name_list]\n\nparameter_format_dict = {\n    'b0': '{:.' + str(DecPlace) + 'f}',\n    'b1': '{:.' + str(DecPlace + 3) + 'f}',\n    'b2': '{:.' + str(DecPlace) + 'e}',\n    'b3': '{:.' + str(DecPlace) + 'e}',\n    'std(b1)': '{:.' + str(DecPlace + 3) + 'f}',\n    'std(b2)': '{:.' + str(DecPlace) + 'e}',\n    'std(b3)': '{:.' + str(DecPlace) + 'e}'}\n\nfor parameter in parameter_name_list:\n    for model in result_df.index:\n        if not result_df.isna().loc[model, parameter]:\n            result_df_with_perr.loc[model, parameter] = \\\n                str(parameter_format_dict[parameter].format(result_df.loc[model, parameter])) + \\\n                    ' ' + u\"\\u00B1\" + ' ' + \\\n                    str(parameter_format_dict[parameter].format(result_df.loc[model, 'std('+parameter+')']))\n            relative_errors = abs(result_df.loc[model, 'std('+parameter+')'] / result_df.loc[model, parameter])\n            result_df_with_perr.loc[model, 'error('+parameter+')'] = '{:.3%}'.format(relative_errors)\n\nresult_df_with_perr_leastsq = result_df_with_perr.copy()\nprint('scipy.optimize.leastsq:'.upper())\ndisplay(result_df_with_perr_leastsq)\nXY_calc_df = pd.DataFrame({\n    'X_fact': X_fact,\n    'Y_fact': Y_fact})\nfor i, model in enumerate(model_list):\n    XY_calc_df[model] = calculation_results_df.loc[model, 'Y_calc']\ndisplay(XY_calc_df.head(), XY_calc_df.tail())\nПостроим графики аппроксимации (по аналогии с Excel):\n# setting colors for graphs\ncolor_dict = {\n        'linear':                      'blue',\n        'quadratic':                   'darkblue',\n        'qubic':                       'slateblue',\n        'power':                       'magenta',\n        'exponential type 1':          'cyan',\n        'exponential type 2':          'black',\n        'logarithmic':                 'orange',\n        'inverse logarithmic':         'gold',\n        'hyperbolic':                  'grey',\n        'hyperbolic type 2':           'darkgrey',\n        'hyperbolic type 3':           'lightgrey',\n        'modified exponential type 1': 'darkcyan',\n        'modified exponential type 2': 'black',\n        'logistic type 1':             'green',\n        'logistic type 2':             'darkgreen',\n        'Gompertz':                    'brown'\n        }\n\nlinewidth_dict = {\n        'linear':                      2,\n        'quadratic':                   2,\n        'qubic':                       2,\n        'power':                       2,\n        'exponential type 1':          5,\n        'exponential type 2':          1,\n        'logarithmic':                 2,\n        'inverse logarithmic':         2,\n        'hyperbolic':                  2,\n        'hyperbolic type 2':           2,\n        'hyperbolic type 3':           2,\n        'modified exponential type 1': 2,\n        'modified exponential type 2': 2,\n        'logistic type 1':             2,\n        'logistic type 2':             2,\n        'Gompertz':                    2\n        }\n\n# changing the Matplotlib settings\nplt.rcParams['axes.titlesize'] = 14\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['xtick.labelsize'] = 12\nplt.rcParams['ytick.labelsize'] = 12\n\n# bounds of values of variables (when plotting)\nXmin = X1_min_graph\nXmax = X1_max_graph\nYmin = Y_min_graph\nYmax = Y_max_graph\n\n# the value of the independent variable (when plotting)\nnx = 100\nhx = (Xmax - Xmin)/(nx - 1)\nX_calc_graph = np.linspace(Xmin, Xmax, nx)\n\n# plotting\nfig, axes = plt.subplots(figsize=(420/INCH, 297/INCH))\ntitle_figure = Title_String + '\\n' + Task_Theme + '\\n'\nfig.suptitle(title_figure, fontsize = 16)\ntitle_axes = 'Fuel mileage ratio'\naxes.set_title(title_axes, fontsize = 18)\n\n# actual data\nsns.scatterplot(\n    x=X_fact, y=Y_fact,\n    label='data',\n    s=75,\n    color='red',\n    ax=axes)\n\n# models\nfor func_name in model_list:\n    R2 = round(result_df.loc[func_name,'R2'], DecPlace)\n    RMSE = round(result_df.loc[func_name,'RMSE'], DecPlace)\n    MAE = round(result_df.loc[func_name,'MAE'], DecPlace)\n    MSPE = \"{:.3%}\".format(result_df.loc[func_name,'MSPE'])\n    MAPE = \"{:.3%}\".format(result_df.loc[func_name,'MAPE'])\n    label = func_name + ' '+ r'$(R^2 = $' + f'{R2}' + ', ' + f'RMSE = {RMSE}' + ', ' + f'MAE = {MAE}' + ', ' + f'MSPE = {MSPE}' + ', ' + f'MAPE = {MAPE})'\n    func = calculation_results_df.loc[func_name, 'func']\n    popt = calculation_results_df.loc[func_name, 'popt']\n    sns.lineplot(\n        x=X_calc_graph, y=func(X_calc_graph, *popt),\n        color=color_dict[func_name],\n        linewidth=linewidth_dict[func_name],\n        legend=True,\n        label=label,\n        ax=axes)\n\naxes.set_xlim(Xmin, Xmax)\naxes.set_ylim(Ymin, Ymax)\naxes.set_xlabel(Variable_Name_X1, fontsize = 14)\naxes.set_ylabel(Variable_Name_Y, fontsize = 14)\naxes.legend(prop={'size': 12})\n\nplt.show()\nАппроксимация с использованием scipy.optimize.least_squares\nОсобенности использования функции:\nРеализация \nнелинейного\n метода наименьших квадратов, \nс возможностью\n введения ограничений на значения параметров.\nИмеется возможность выбора алгоритма оптимизации: \ntrf (Trust Region Reflective)\n - установлен по умолчанию, надежный метод, особенно подходящий для больших задач с ограничениями;\ndogbox\n - для небольших задач с ограничениями;\nlm (Levenberg-Marquardt algorithm)\n - для небольших задач без ограничений.\nСсылка на документацию\n.\nfrom scipy.optimize import least_squares\n\n# algorithm to perform minimization\nmethods_dict = {\n        'linear':             'lm',\n        'quadratic':          'lm',\n        'qubic':              'lm',\n        'power':              'lm',\n        'exponential type 1': 'lm',\n        'exponential type 2': 'lm',\n        'logarithmic':        'lm',\n        'hyperbolic':         'lm'}\n\n# actual data\nX_fact = X1\nY_fact = Y\n\n# function for calculating residuals\nresidual_func = lambda p, x, y: y - func(x, *p)\n\n# return all optional outputs\nfull_output = True\n\n# variables to save the calculation results\ncalculation_results_df = pd.DataFrame(columns=['func', 'p0', 'popt', 'jac', 'hess', 'SSE', 'MSE', 'pcov', 'perr', 'Y_calc', 'error_metrics'])\nerror_metrics_results_df = pd.DataFrame(columns=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE', 'R2'])\n\n# calculations\nfor func_name in model_list:\n    print(f'{func_name.upper()} MODEL: {formulas_dict[func_name]}')\n    func = models_dict[func_name]\n    calculation_results_df.loc[func_name, 'func'] = func\n    p0 = p0_dict[func_name]\n    print(f'p0 = {p0}')\n    calculation_results_df.loc[func_name, 'p0'] = p0\n    res = sci.optimize.least_squares(residual_func, p0, args=(X_fact, Y_fact), method=methods_dict[func_name])\n    (popt, jac) = (res.x, res.jac)\n    calculation_results_df.loc[func_name, 'popt'] = popt\n    calculation_results_df.loc[func_name, 'jac'] = jac\n    print(f'parameters = {popt}')\n    #print(f'jac =\\n {jac}')\n    hess = np.linalg.inv(np.dot(jac.T, jac))\n    calculation_results_df.loc[func_name, 'hess'] = hess\n    print(f'hess =\\n {hess}')\n    SSE = (residual_func(popt, X_fact, Y_fact)**2).sum()\n    calculation_results_df.loc[func_name, 'SSE'] = SSE\n    print(f'SSE = {SSE}')\n    MSE = SSE / (len(Y_fact)-len(p0))\n    calculation_results_df.loc[func_name, 'MSE'] = MSE\n    print(f'MSE = {MSE}')\n    pcov = hess * MSE\n    calculation_results_df.loc[func_name, 'pcov'] = pcov\n    print(f'pcov =\\n {pcov}')\n    perr = np.sqrt(np.diag(pcov))\n    calculation_results_df.loc[func_name, 'perr'] = perr\n    print(f'perr = {perr}\\n')\n    Y_calc = func(X_fact, *popt)\n    calculation_results_df.loc[func_name, 'Y_calc'] = Y_calc\n    (error_metrics_dict, error_metrics_df) = regression_error_metrics(Yfact=Y_fact, Ycalc=Y_calc, model_name=func_name)\n    calculation_results_df.loc[func_name, 'error_metrics'] = error_metrics_dict.values()\n    error_metrics_results_df = pd.concat([error_metrics_results_df, error_metrics_df])\nLINEAR MODEL: y = b0 + b1*x\np0 = [0, 0]\nparameters = [ 1.1850e+01 -2.1760e-03]\nhess =\n [[ 8.7797e-02 -7.3041e-05]\n [-7.3041e-05  7.6636e-08]]\nSSE = 84.42039151684142\nMSE = 1.5928375757894608\npcov =\n [[ 1.3985e-01 -1.1634e-04]\n [-1.1634e-04  1.2207e-07]]\nperr = [3.7396e-01 3.4938e-04]\n\nQUADRATIC MODEL: y = b0 + b1*x + b2*x^2\np0 = [0, 0, 0]\nparameters = [ 1.4343e+01 -7.0471e-03  1.8760e-06]\nhess =\n [[ 2.8731e-01 -4.6282e-04  1.5012e-07]\n [-4.6282e-04  8.3812e-07 -2.9328e-10]\n [ 1.5012e-07 -2.9328e-10  1.1295e-13]]\nSSE = 53.26102533463595\nMSE = 1.0242504872045375\npcov =\n [[ 2.9428e-01 -4.7404e-04  1.5376e-07]\n [-4.7404e-04  8.5844e-07 -3.0039e-10]\n [ 1.5376e-07 -3.0039e-10  1.1569e-13]]\nperr = [5.4247e-01 9.2652e-04 3.4013e-07]\n\nQUBIC MODEL: y = b0 + b1*x + b2*x^2 + b3*x^3\np0 = [0, 0, 0, 0]\nparameters = [ 1.4838e+01 -8.8040e-03  3.5207e-06 -4.1264e-10]\nhess =\n [[ 5.7657e-01 -1.4894e-03  1.1111e-06 -2.4112e-10]\n [-1.4894e-03  4.4815e-06 -3.7039e-09  8.5573e-13]\n [ 1.1111e-06 -3.7039e-09  3.3058e-12 -8.0107e-16]\n [-2.4112e-10  8.5573e-13 -8.0107e-16  2.0099e-19]]\nSSE = 52.413833494705216\nMSE = 1.0277222253863767\npcov =\n [[ 5.9256e-01 -1.5307e-03  1.1419e-06 -2.4780e-10]\n [-1.5307e-03  4.6057e-06 -3.8066e-09  8.7945e-13]\n [ 1.1419e-06 -3.8066e-09  3.3974e-12 -8.2328e-16]\n [-2.4780e-10  8.7945e-13 -8.2328e-16  2.0656e-19]]\nperr = [7.6978e-01 2.1461e-03 1.8432e-06 4.5449e-10]\n\nPOWER MODEL: y = b0*x^b1\np0 = [0, 0]\nparameters = [33.4033 -0.1841]\nhess =\n [[ 1.2667e+01 -5.7226e-02]\n [-5.7226e-02  2.6284e-04]]\nSSE = 49.87968652202211\nMSE = 0.94112616079287\npcov =\n [[ 1.1922e+01 -5.3857e-02]\n [-5.3857e-02  2.4737e-04]]\nperr = [3.4528 0.0157]\n\nEXPONENTIAL TYPE 1 MODEL: y = b0*exp(b1^x)\np0 = [0, 0]\nparameters = [ 1.2624e+01 -2.7834e-04]\nhess =\n [[ 1.5566e-01 -1.1829e-05]\n [-1.1829e-05  1.1127e-09]]\nSSE = 75.19491109027253\nMSE = 1.4187719073636327\npcov =\n [[ 2.2085e-01 -1.6782e-05]\n [-1.6782e-05  1.5787e-09]]\nperr = [4.6994e-01 3.9732e-05]\n\nEXPONENTIAL TYPE 2 MODEL: y = b0*b1^x\np0 = [1, 1]\nparameters = [12.6242  0.9997]\nhess =\n [[ 1.5566e-01 -1.1825e-05]\n [-1.1825e-05  1.1121e-09]]\nSSE = 75.19491108936178\nMSE = 1.4187719073464486\npcov =\n [[ 2.2085e-01 -1.6778e-05]\n [-1.6778e-05  1.5778e-09]]\nperr = [4.6994e-01 3.9721e-05]\n\nLOGARITHMIC MODEL: y = b0 + b1*ln(x)\np0 = [0, 0]\nparameters = [23.9866 -2.1170]\nhess =\n [[ 2.1029 -0.3106]\n [-0.3106  0.0463]]\nSSE = 49.342567293214316\nMSE = 0.9309918357210248\npcov =\n [[ 1.9578 -0.2891]\n [-0.2891  0.0431]]\nperr = [1.3992 0.2075]\n\nHYPERBOLIC MODEL: y = b0+b1/x\np0 = [0, 0]\nparameters = [  9.2219 315.7336]\nhess =\n [[ 2.2732e-02 -2.5920e+00]\n [-2.5920e+00  1.4765e+03]]\nSSE = 78.69038671392154\nMSE = 1.484724277621161\npcov =\n [[ 3.3751e-02 -3.8485e+00]\n [-3.8485e+00  2.1922e+03]]\nperr = [ 0.1837 46.8207]\nПо аналогии с предыдущим примером, сформируем отдельный DataFrame, в котором соберем все результаты аппроксимации, и сравним с результатами из предыдущего примера:\nresult_df = pd.DataFrame(\n                list(calculation_results_df.loc[:,'p0'].values),\n                columns=['p0', 'p1', 'p2', 'p3'],\n                index=model_list)\n\nresult_df = result_df.join(pd.DataFrame(\n                                list(calculation_results_df.loc[:,'popt'].values),\n                                columns=['b0', 'b1', 'b2', 'b3'],\n                                index=model_list))\n\nresult_df = result_df.join(pd.DataFrame(\n                                list(calculation_results_df.loc[:,'perr'].values),\n                                columns=['std(b0)', 'std(b1)', 'std(b2)', 'std(b3)'],\n                                index=model_list))\n\nresult_df = result_df.join(pd.DataFrame(\n                                list(calculation_results_df.loc[:,'error_metrics'].values),\n                                columns=['MSE', 'RMSE',\t'MAE', 'MSPE', 'MAPE', 'RMSLE', 'R2'],\n                                index=model_list))\n\n#display(result_df)\n\n# settings for displaying the DataFrame: result_df\nresult_df_least_squares = result_df.copy()\n'''for elem in ['MSPE', 'MAPE']:\n    result_df_leastsq[elem] = result_df_leastsq[elem].apply(lambda s: float(s[:-1]))'''\nprint('scipy.optimize.least_squares:'.upper())\ndisplay(result_df_least_squares\n        .style\n            .format(\n                precision=DecPlace, na_rep='-',\n                formatter={\n                    'b1': '{:.' + str(DecPlace + 3) + 'f}',\n                    'b2': '{:.' + str(DecPlace) + 'e}',\n                    'b3': '{:.' + str(DecPlace) + 'e}',\n                    'std(b1)': '{:.' + str(DecPlace + 3) + 'f}',\n                    'std(b2)': '{:.' + str(DecPlace) + 'e}',\n                    'std(b3)': '{:.' + str(DecPlace) + 'e}'})\n            .highlight_min(color='green', subset=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE'])\n            .highlight_max(color='red', subset=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE'])\n            .highlight_max(color='green', subset='R2')\n            .highlight_min(color='red', subset='R2')\n            .applymap(lambda x: 'color: orange;' if abs(x) <= 10**(-(DecPlace-1)) else None, subset=['b0', 'b1', 'b2', 'b3', 'std(b0)', 'std(b1)', 'std(b2)', 'std(b3)']))    \n\nprint('scipy.optimize.leastsq:'.upper())\ndisplay(result_df_leastsq\n        .style\n            .format(\n                precision=DecPlace, na_rep='-',\n                formatter={\n                    'b1': '{:.' + str(DecPlace + 3) + 'f}',\n                    'b2': '{:.' + str(DecPlace) + 'e}',\n                    'b3': '{:.' + str(DecPlace) + 'e}',\n                    'std(b1)': '{:.' + str(DecPlace + 3) + 'f}',\n                    'std(b2)': '{:.' + str(DecPlace) + 'e}',\n                    'std(b3)': '{:.' + str(DecPlace) + 'e}'})\n            .highlight_min(color='green', subset=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE'])\n            .highlight_max(color='red', subset=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE'])\n            .highlight_max(color='green', subset='R2')\n            .highlight_min(color='red', subset='R2')\n            .applymap(lambda x: 'color: orange;' if abs(x) <= 10**(-(DecPlace-1)) else None, subset=['b0', 'b1', 'b2', 'b3', 'std(b0)', 'std(b1)', 'std(b2)', 'std(b3)']))    \nСформируем DataFrame, содержащий ошибки и доверительные интервалы параметров моделей аппроксимации, и сравним с результатами из предыдущего примера:\nresult_df_with_perr = pd.DataFrame()\n\nparameter_name_list = ['b0', 'b1', 'b2', 'b3']\nresult_df_with_perr[parameter_name_list] = result_df[parameter_name_list]\n\nparameter_format_dict = {\n    'b0': '{:.' + str(DecPlace) + 'f}',\n    'b1': '{:.' + str(DecPlace + 3) + 'f}',\n    'b2': '{:.' + str(DecPlace) + 'e}',\n    'b3': '{:.' + str(DecPlace) + 'e}',\n    'std(b1)': '{:.' + str(DecPlace + 3) + 'f}',\n    'std(b2)': '{:.' + str(DecPlace) + 'e}',\n    'std(b3)': '{:.' + str(DecPlace) + 'e}'}\n\nfor parameter in parameter_name_list:\n    for model in result_df.index:\n        if not result_df.isna().loc[model, parameter]:\n            result_df_with_perr.loc[model, parameter] = \\\n                str(parameter_format_dict[parameter].format(result_df.loc[model, parameter])) + \\\n                    ' ' + u\"\\u00B1\" + ' ' + \\\n                    str(parameter_format_dict[parameter].format(result_df.loc[model, 'std('+parameter+')']))\n            relative_errors = abs(result_df.loc[model, 'std('+parameter+')'] / result_df.loc[model, parameter])\n            result_df_with_perr.loc[model, 'error('+parameter+')'] = '{:.3%}'.format(relative_errors)\n\nresult_df_with_perr_least_squares = result_df_with_perr.copy()\nprint('scipy.optimize.least_squares:'.upper())\ndisplay(result_df_with_perr_least_squares)\n\nprint('scipy.optimize.leastsq:'.upper())\ndisplay(result_df_with_perr_leastsq)\nАппроксимация с использованием scipy.optimize.curve_fit\nОсобенности использования функции:\nРеализация \nнелинейного\n метода наименьших квадратов, \nс возможностью\n введения ограничений на значения параметров.\nИмеется возможность выбора алгоритма оптимизации (\ntrf\n, \ndogbox\n, \nlm\n), по аналогии с \nscipy.optimize.least_squares\n).\nСсылка на документацию\n.\nfrom scipy.optimize import curve_fit\n\n# algorithm to perform minimization\nmethods_dict = {\n        'linear':             'lm',\n        'quadratic':          'lm',\n        'qubic':              'lm',\n        'power':              'lm',\n        'exponential type 1': 'lm',\n        'exponential type 2': 'lm',\n        'logarithmic':        'lm',\n        'hyperbolic':         'lm'}\n\n# actual data\nX_fact = X1\nY_fact = Y\n\n# return all optional outputs\nfull_output = True\n\n# variables to save the calculation results\ncalculation_results_df = pd.DataFrame(columns=['func', 'p0', 'popt', 'pcov', 'perr', 'Y_calc', 'error_metrics'])\nerror_metrics_results_df = pd.DataFrame(columns=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE', 'R2'])\n\n# calculations\nfor func_name in model_list:\n    print(f'{func_name.upper()} MODEL: {formulas_dict[func_name]}')\n    func = models_dict[func_name]\n    calculation_results_df.loc[func_name, 'func'] = func\n    p0 = p0_dict[func_name]\n    print(f'p0 = {p0}')\n    calculation_results_df.loc[func_name, 'p0'] = p0\n    if full_output:\n        (popt, pcov, infodict, mesg, ier) = curve_fit(func, X_fact, Y_fact, p0=p0, method=methods_dict[func_name], full_output=full_output)\n        integer_flag = f'ier = {ier}, the solution was found' if ier<=4 else f'ier = {ier}, the solution was not found'\n        print(integer_flag, '\\n', mesg)\n        calculation_results_df.loc[func_name, 'popt'] = popt\n        print(f'parameters = {popt}')\n        calculation_results_df.loc[func_name, 'pcov'] = pcov\n        print(f'pcov =\\n {pcov}')\n        perr = np.sqrt(np.diag(pcov))\n        calculation_results_df.loc[func_name, 'perr'] = perr\n        print(f'perr = {perr}\\n') \n    else:\n        (popt, pcov) = curve_fit(func, X_fact, Y_fact, p0=p0, method=methods_dict[func_name], full_output=full_output)\n        calculation_results_df.loc[func_name, 'popt'] = popt\n        print(f'parameters = {popt}')\n        calculation_results_df.loc[func_name, 'pcov'] = pcov\n        print(f'pcov =\\n {pcov}')\n        perr = np.sqrt(np.diag(pcov))\n        calculation_results_df.loc[func_name, 'perr'] = perr\n        print(f'perr = {perr}\\n') \n    Y_calc = func(X_fact, *popt)\n    calculation_results_df.loc[func_name, 'Y_calc'] = Y_calc\n    (error_metrics_dict, error_metrics_df) = regression_error_metrics(Yfact=Y_fact, Ycalc=Y_calc, model_name=func_name)\n    calculation_results_df.loc[func_name, 'error_metrics'] = error_metrics_dict.values()\n    error_metrics_results_df = pd.concat([error_metrics_results_df, error_metrics_df])        \nLINEAR MODEL: y = b0 + b1*x\np0 = [0, 0]\nier = 3, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000 and the relative error between two consecutive iterates is at \n  most 0.000000\nparameters = [ 1.1850e+01 -2.1760e-03]\npcov =\n [[ 1.3985e-01 -1.1634e-04]\n [-1.1634e-04  1.2207e-07]]\nperr = [3.7396e-01 3.4938e-04]\n\nQUADRATIC MODEL: y = b0 + b1*x + b2*x^2\np0 = [0, 0, 0]\nier = 1, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000\nparameters = [ 1.4343e+01 -7.0471e-03  1.8760e-06]\npcov =\n [[ 2.9428e-01 -4.7404e-04  1.5376e-07]\n [-4.7404e-04  8.5844e-07 -3.0039e-10]\n [ 1.5376e-07 -3.0039e-10  1.1569e-13]]\nperr = [5.4247e-01 9.2652e-04 3.4013e-07]\n\nQUBIC MODEL: y = b0 + b1*x + b2*x^2 + b3*x^3\np0 = [0, 0, 0, 0]\nier = 1, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000\nparameters = [ 1.4838e+01 -8.8040e-03  3.5207e-06 -4.1264e-10]\npcov =\n [[ 5.9256e-01 -1.5307e-03  1.1419e-06 -2.4780e-10]\n [-1.5307e-03  4.6057e-06 -3.8066e-09  8.7945e-13]\n [ 1.1419e-06 -3.8066e-09  3.3974e-12 -8.2328e-16]\n [-2.4780e-10  8.7945e-13 -8.2328e-16  2.0656e-19]]\nperr = [7.6978e-01 2.1461e-03 1.8432e-06 4.5449e-10]\n\nPOWER MODEL: y = b0*x^b1\np0 = [0, 0]\nier = 1, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000\nparameters = [33.4033 -0.1841]\npcov =\n [[ 1.1922e+01 -5.3856e-02]\n [-5.3856e-02  2.4737e-04]]\nperr = [3.4528 0.0157]\n\nEXPONENTIAL TYPE 1 MODEL: y = b0*exp(b1^x)\np0 = [0, 0]\nier = 1, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000\nparameters = [ 1.2624e+01 -2.7834e-04]\npcov =\n [[ 2.2085e-01 -1.6783e-05]\n [-1.6783e-05  1.5787e-09]]\nperr = [4.6994e-01 3.9733e-05]\n\nEXPONENTIAL TYPE 2 MODEL: y = b0*b1^x\np0 = [1, 1]\nier = 1, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000\nparameters = [12.6242  0.9997]\npcov =\n [[ 2.2085e-01 -1.6778e-05]\n [-1.6778e-05  1.5778e-09]]\nperr = [4.6994e-01 3.9722e-05]\n\nLOGARITHMIC MODEL: y = b0 + b1*ln(x)\np0 = [0, 0]\nier = 3, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000 and the relative error between two consecutive iterates is at \n  most 0.000000\nparameters = [23.9866 -2.1170]\npcov =\n [[ 1.9578 -0.2891]\n [-0.2891  0.0431]]\nperr = [1.3992 0.2075]\n\nHYPERBOLIC MODEL: y = b0+b1/x\np0 = [0, 0]\nier = 1, the solution was found \n Both actual and predicted relative reductions in the sum of squares\n  are at most 0.000000\nparameters = [  9.2219 315.7336]\npcov =\n [[ 3.3751e-02 -3.8485e+00]\n [-3.8485e+00  2.1922e+03]]\nperr = [ 0.1837 46.8207]\nПо аналогии с предыдущим примером, сформируем отдельный DataFrame, в котором соберем все результаты аппроксимации, и сравним с результатами из предыдущих примеров:\nresult_df = pd.DataFrame(\n                list(calculation_results_df.loc[:,'p0'].values),\n                columns=['p0', 'p1', 'p2', 'p3'],\n                index=model_list)\n\nresult_df = result_df.join(pd.DataFrame(\n                                list(calculation_results_df.loc[:,'popt'].values),\n                                columns=['b0', 'b1', 'b2', 'b3'],\n                                index=model_list))\n\nresult_df = result_df.join(pd.DataFrame(\n                                list(calculation_results_df.loc[:,'perr'].values),\n                                columns=['std(b0)', 'std(b1)', 'std(b2)', 'std(b3)'],\n                                index=model_list))\n\nresult_df = result_df.join(pd.DataFrame(\n                                list(calculation_results_df.loc[:,'error_metrics'].values),\n                                columns=['MSE', 'RMSE',\t'MAE', 'MSPE', 'MAPE', 'RMSLE', 'R2'],\n                                index=model_list))\n\n#display(result_df)\n\n# settings for displaying the DataFrame: result_df\nresult_df_curve_fit = result_df.copy()\n'''for elem in ['MSPE', 'MAPE']:\n    result_df_leastsq[elem] = result_df_leastsq[elem].apply(lambda s: float(s[:-1]))'''\nprint('scipy.optimize.curve_fit:'.upper())\ndisplay(result_df_curve_fit\n        .style\n            .format(\n                precision=DecPlace, na_rep='-',\n                formatter={\n                    'b1': '{:.' + str(DecPlace + 3) + 'f}',\n                    'b2': '{:.' + str(DecPlace) + 'e}',\n                    'b3': '{:.' + str(DecPlace) + 'e}',\n                    'std(b1)': '{:.' + str(DecPlace + 3) + 'f}',\n                    'std(b2)': '{:.' + str(DecPlace) + 'e}',\n                    'std(b3)': '{:.' + str(DecPlace) + 'e}'})\n            .highlight_min(color='green', subset=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE'])\n            .highlight_max(color='red', subset=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE'])\n            .highlight_max(color='green', subset='R2')\n            .highlight_min(color='red', subset='R2')\n            .applymap(lambda x: 'color: orange;' if abs(x) <= 10**(-(DecPlace-1)) else None, subset=['b0', 'b1', 'b2', 'b3', 'std(b0)', 'std(b1)', 'std(b2)', 'std(b3)']))    \n\nprint('scipy.optimize.least_squares:'.upper())\ndisplay(result_df_least_squares\n        .style\n            .format(\n                precision=DecPlace, na_rep='-',\n                formatter={\n                    'b1': '{:.' + str(DecPlace + 3) + 'f}',\n                    'b2': '{:.' + str(DecPlace) + 'e}',\n                    'b3': '{:.' + str(DecPlace) + 'e}',\n                    'std(b1)': '{:.' + str(DecPlace + 3) + 'f}',\n                    'std(b2)': '{:.' + str(DecPlace) + 'e}',\n                    'std(b3)': '{:.' + str(DecPlace) + 'e}'})\n            .highlight_min(color='green', subset=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE'])\n            .highlight_max(color='red', subset=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE'])\n            .highlight_max(color='green', subset='R2')\n            .highlight_min(color='red', subset='R2')\n            .applymap(lambda x: 'color: orange;' if abs(x) <= 10**(-(DecPlace-1)) else None, subset=['b0', 'b1', 'b2', 'b3', 'std(b0)', 'std(b1)', 'std(b2)', 'std(b3)']))    \n\nprint('scipy.optimize.least_squares:'.upper())\ndisplay(result_df_leastsq\n        .style\n            .format(\n                precision=DecPlace, na_rep='-',\n                formatter={\n                    'b1': '{:.' + str(DecPlace + 3) + 'f}',\n                    'b2': '{:.' + str(DecPlace) + 'e}',\n                    'b3': '{:.' + str(DecPlace) + 'e}',\n                    'std(b1)': '{:.' + str(DecPlace + 3) + 'f}',\n                    'std(b2)': '{:.' + str(DecPlace) + 'e}',\n                    'std(b3)': '{:.' + str(DecPlace) + 'e}'})\n            .highlight_min(color='green', subset=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE'])\n            .highlight_max(color='red', subset=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE'])\n            .highlight_max(color='green', subset='R2')\n            .highlight_min(color='red', subset='R2')\n            .applymap(lambda x: 'color: orange;' if abs(x) <= 10**(-(DecPlace-1)) else None, subset=['b0', 'b1', 'b2', 'b3', 'std(b0)', 'std(b1)', 'std(b2)', 'std(b3)']))    \nПо аналогии с предыдущим примером, сформируем отдельный DataFrame, содержащий ошибки и доверительные интервалы параметров моделей аппроксимации, и сравним с результатами из предыдущих примеров:\nresult_df_with_perr = pd.DataFrame()\n\nparameter_name_list = ['b0', 'b1', 'b2', 'b3']\nresult_df_with_perr[parameter_name_list] = result_df[parameter_name_list]\n\nparameter_format_dict = {\n    'b0': '{:.' + str(DecPlace) + 'f}',\n    'b1': '{:.' + str(DecPlace + 3) + 'f}',\n    'b2': '{:.' + str(DecPlace) + 'e}',\n    'b3': '{:.' + str(DecPlace) + 'e}',\n    'std(b1)': '{:.' + str(DecPlace + 3) + 'f}',\n    'std(b2)': '{:.' + str(DecPlace) + 'e}',\n    'std(b3)': '{:.' + str(DecPlace) + 'e}'}\n\nfor parameter in parameter_name_list:\n    for model in result_df.index:\n        if not result_df.isna().loc[model, parameter]:\n            result_df_with_perr.loc[model, parameter] = \\\n                str(parameter_format_dict[parameter].format(result_df.loc[model, parameter])) + \\\n                    ' ' + u\"\\u00B1\" + ' ' + \\\n                    str(parameter_format_dict[parameter].format(result_df.loc[model, 'std('+parameter+')']))\n            relative_errors = abs(result_df.loc[model, 'std('+parameter+')'] / result_df.loc[model, parameter])\n            result_df_with_perr.loc[model, 'error('+parameter+')'] = '{:.3%}'.format(relative_errors)\n\nresult_df_with_perr_curve_fit = result_df_with_perr.copy()\nprint('scipy.optimize.curve_fit:'.upper())\ndisplay(result_df_with_perr_curve_fit)\n\nprint('scipy.optimize.least_squares:'.upper())\ndisplay(result_df_with_perr_least_squares)\n\nprint('scipy.optimize.leastsq:'.upper())\ndisplay(result_df_with_perr_leastsq)\nАппроксимация с использованием scipy.optimize.minimize\nОсобенности использования функции:\nМинимизация значений скалярной функции одной или нескольких переменных; при использовании в качестве функции суммы квадратов получаем реализацию \nнелинейного\n метода наименьших квадратов.\nИмеется возможность выбора широкого набора алгоритмов оптимизации (\nNelder-Mead\n, \nPowell\n, \nCG\n, \nBFGS\n, \nNewton-CG\n, \nL-BFGS-B\n, \nTNC\n, \nCOBYLA\n, \nSLSQP\n, \ntrust-constr\n, \ndogleg\n, \ntrust-ncg\n, \ntrust-exact\n, \ntrust-krylov\n, custom), получить информацию можно с помощью функции \nscipy.optimize.show_options\n (\nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.show_options.html#scipy.optimize.show_options\n), а также есть возможность добавить пользовательский алгоритм оптимизации.\nСсылка на документацию\n.\nРабота с данной функцией принципиально не отличается от разобранных выше, поэтому в целях экономии места в данном обзоре подробности приводить не буду; все рабочие файлы с примерами расчетов доступны в моем репозитории на GitHub (\nhttps://github.com/AANazarov/Statistical-methods\n).\nАппроксимация с использованием библиотеки lmfit\nБиблиотека \nlmfit\n - это универсальный инструмент для решения задач нелинейной оптимизации, в том числе и для аппроксимации зависимостей, представляет собой дальнейшее развитие \nscipy.optimize\n.\nОсобенности использования библиотеки \nlmfit\n для аппроксимации:\nВ качестве переменных вместо обычных чисел типа  \nfloat\n используются объекты класса \nParameter\n, которые обладают гораздо более обширным функционалом (\nhttps://lmfit.github.io/lmfit-py/parameters.html#lmfit.parameter.Parameter\n): \nдля них возможно задавать границы допустимых значений и ограничения в виде алгебраических выражений;\nвозможно фиксировать значения в процессе подгонки;\nпосле подгонки возможно получить такие атрибуты, как стандартная ошибка и коэффициенты корреляции с другими параметрами модели;\nвозможно указывать размер шага для точек сетки в методе полного перебора и т.д.\nИмеется возможность выбора широкого набора алгоритмов оптимизации (\nleastsq\n - default, \nleast_squares\n, \ndifferential_evolution\n, \nbrute\n, \nbasinhopping\n, \nampgo\n, \nnelder\n, \nlbfgsb\n, \npowell\n, \ncg\n, \nnewton\n, \ncobyla\n, \nbfgs\n, \ntnc\n, \ntrust-ncg\n, \ntrust-exact\n, \ntrust-krylov\n, \ntrust-constr\n, \ndogleg\n, \nslsqp\n, \nemcee\n, \nshgo\n, \ndual_annealing\n) (\nhttps://lmfit.github.io/lmfit-py/fitting.html#choosing-different-fitting-methods\n).\nИмеется возможность выбора различных целевых функций минимизации: по умолчанию - \nсумма квадратов остатков\n \n, но в случае, например, наличия выбросов может применяться \nnegentropy\n -  отрицательная энтропия, использующая нормальное распределение \n (где\n \n), \nneglogcauchy\n - отрицательная логарифмическая вероятность,использующая распределения Коши\n \n, \nлибо \ncallable\n - функция, определенная пользователем; подробнее см. \nhttps://lmfit.github.io/lmfit-py/fitting.html#using-the-minimizer-class\n.\nУлучшена оценка доверительных интервалов. Хотя \nscipy.optimize.leastsq\n автоматически вычисляет неопределенности и корреляции из ковариационной матрицы, точность этих оценок иногда вызывает сомнения. Чтобы помочь решить эту проблему, lmfit имеет функции явного исследования пространства параметров и определения уровней доверительной вероятности даже в самых сложных случаях.\nВозможность работы с классом \nModel\n (\nhttps://lmfit.github.io/lmfit-py/model.html#lmfit.model.Model\n).\nИмеется набор встроенных моделей (\nhttps://lmfit.github.io/lmfit-py/builtin_models.html#builtin-models-chapter\n).\nДля полного контроля над процессом оптимизации имеется возможность работы с объектом класса \nMinimizer\n (\nhttps://lmfit.github.io/lmfit-py/fitting.html#using-the-minimizer-class\n).\nИмеются встроенные инструменты вывода отчетов и построения графиков.\nСсылка на документацию к библиотеке: \nhttps://lmfit.github.io/lmfit-py/\nВ документации к библиотеке \nlmfit\n приведено большое количество примеров, приведем здесь их описание с целью облегчения поиска полезной информации:\nFit with Data in a pandas DataFrame - аппроксимация по данным содержащимся в pandas DataFrame (\nhttps://lmfit.github.io/lmfit-py/examples/example_use_pandas.html#sphx-glr-examples-example-use-pandas-py\n).\nUsing an ExpressionModel - использование модуля \nExpressionModel\n для построение моделей с выражениями, определенными пользователем (\nhttps://lmfit.github.io/lmfit-py/examples/example_expression_model.html#sphx-glr-examples-example-expression-model-py\n).\nFit Using Inequality Constraint - использование ограничений типа неравенств (\nhttps://lmfit.github.io/lmfit-py/examples/example_fit_with_inequality.html#sphx-glr-examples-example-fit-with-inequality-py\n).\nFit Using differential_evolution Algorithm - пример использования алгоритма \nDifferential_evolution\n (\nhttps://lmfit.github.io/lmfit-py/examples/example_diffev.html#sphx-glr-examples-example-diffev-py\n).\nFit Using Bounds - аппроксимация с использованием ограничений параметров (\nhttps://lmfit.github.io/lmfit-py/examples/example_fit_with_bounds.html#sphx-glr-examples-example-fit-with-bounds-py\n).\nFit Specifying Different Reduce Function - использование различных целевых функций минимизации (\nhttps://lmfit.github.io/lmfit-py/examples/example_reduce_fcn.html#sphx-glr-examples-example-reduce-fcn-py\n).\nBuilding a lmfit model with SymPy - построение модели с использованием библиотеки символьных вычислений \nSymPy\n (\nhttps://lmfit.github.io/lmfit-py/examples/example_sympy.html#sphx-glr-examples-example-sympy-py\n).\nFit with Algebraic Constraint - аппроксимация с использованием алгебраических ограничений (\nhttps://lmfit.github.io/lmfit-py/examples/example_fit_with_algebraic_constraint.html#sphx-glr-examples-example-fit-with-algebraic-constraint-py\n).\nFit Multiple Data Sets - одновременная подгонка нескольких наборов данных (\nhttps://lmfit.github.io/lmfit-py/examples/example_fit_multi_datasets.html#sphx-glr-examples-example-fit-multi-datasets-py\n).\nFit using the Model interface - использование класса \nModel\n (\nhttps://lmfit.github.io/lmfit-py/examples/example_Model_interface.html#sphx-glr-examples-example-model-interface-py\n).\nFit Specifying a Function to Compute the Jacobian - аппроксимация с указанием аналитической функции для вычисления якобиана (для ускорения вычислений) (\nhttps://lmfit.github.io/lmfit-py/examples/example_fit_with_derivfunc.html#sphx-glr-examples-example-fit-with-derivfunc-py\n).\nOutlier detection via leave-one-out - обнаружение выбросов методом исключения одного наблюдения (\nhttps://lmfit.github.io/lmfit-py/examples/example_detect_outliers.html#sphx-glr-examples-example-detect-outliers-py\n).\nEmcee and the Model Interface - использование алгоритма \nemcee\n (\nhttps://lmfit.github.io/lmfit-py/examples/example_emcee_Model_interface.html#sphx-glr-examples-example-emcee-model-interface-py\n). Алгоритм \nemcee\n может быть использован для получения апостериорного распределения вероятностей параметров, заданного набором экспериментальных данных, он исследует пространство параметров, чтобы определить распределение вероятностей для параметров, но без явной цели попытаться уточнить решение. Его не следует использовать для аппроксимации, но это полезный метод для более тщательного изучения пространства параметров вокруг решения, он позволяет получить лучшее понимание распределения вероятности для параметров; см. также \nhttps://emcee.readthedocs.io/en/stable/\n.\nComplex Resonator Model - в примере показано, как показано, как подобрать параметры сложной модели (резонатора) с помощью \nlmfit.Model\n и определения пользовательского класса \nModel\n (\nhttps://lmfit.github.io/lmfit-py/examples/example_complex_resonator_model.html#sphx-glr-examples-example-complex-resonator-model-py\n).\nModel Selection using lmfit and emcee - в примере показано, как можно использовать алгоритм \nemcee\n для выбора байесовской модели (\nhttps://lmfit.github.io/lmfit-py/examples/lmfit_emcee_model_selection.html#sphx-glr-examples-lmfit-emcee-model-selection-py\n).\nCalculate Confidence Intervals - расчет доверительных интервалов (\nhttps://lmfit.github.io/lmfit-py/examples/example_confidence_interval.html#sphx-glr-examples-example-confidence-interval-py\n).\nFit Two Dimensional Peaks - в примере показано, как обрабатывать двумерные данные с двухмерными распределениями вероятностей (\nhttps://lmfit.github.io/lmfit-py/examples/example_two_dimensional_peak.html#sphx-glr-examples-example-two-dimensional-peak-py\n).\nGlobal minimization using the brute method (a.k.a. grid search) - глобальная минимизация методом \nbrute\n (он же поиск по сетке) (\nhttps://lmfit.github.io/lmfit-py/examples/example_brute.html#sphx-glr-examples-example-brute-py\n). \nПримеры из документации (Examples from the documentation):\nCоздание и сохранение модели в файл *.sav: \ndoc_model_savemodel.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/model_savemodel.html#sphx-glr-examples-documentation-model-savemodel-py\n)\ndoc_model_savemodelresult.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/model_savemodelresult.html#sphx-glr-examples-documentation-model-savemodelresult-py\n)\ndoc_model_savemodelresult2.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/model_savemodelresult2.html#sphx-glr-examples-documentation-model-savemodelresult2-py\n)\nЗагрузка модели из файла *.sav: \ndoc_model_loadmodelresult.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/model_loadmodelresult.html#sphx-glr-examples-documentation-model-loadmodelresult-py\n)\ndoc_model_loadmodelresult2.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/model_loadmodelresult2.html#sphx-glr-examples-documentation-model-loadmodelresult2-py\n)\ndoc_model_loadmodel.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/model_loadmodel.html#sphx-glr-examples-documentation-model-loadmodel-py\n)\nСоздание отчетов: \ndoc_fitting_withreport.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/fitting_withreport.html#sphx-glr-examples-documentation-fitting-withreport-py\n)\nРабота с доверительными интервалами: \ndoc_confidence_basic.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/confidence_basic.html#sphx-glr-examples-documentation-confidence-basic-py\n)\nпостроение доверительного интервала для кривой Гаусса \ndoc_model_uncertainty.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/model_uncertainty.html#sphx-glr-examples-documentation-model-uncertainty-py\n)\ndoc_confidence_advanced.py (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/confidence_advanced.html#sphx-glr-examples-documentation-confidence-advanced-py\n)\nАппроксимация вероятностных распределений: \nраспределение Гаусса - \ndoc_model_gaussian.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/model_gaussian.html#sphx-glr-examples-documentation-model-gaussian-py\n)\nсмесь распределения Гаусса и линейной зависимости - \ndoc_model_two_components.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/model_two_components.html#sphx-glr-examples-documentation-model-two-components-py\n)\nРазличные опции \nlmfit\n: \ndoc_model_with_nan_policy.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/model_with_nan_policy.html#sphx-glr-examples-documentation-model-with-nan-policy-py\n)\ndoc_model_with_iter_callback.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/model_with_iter_callback.html#sphx-glr-examples-documentation-model-with-iter-callback-py\n)\ndoc_parameters_basic.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/parameters_basic.html#sphx-glr-examples-documentation-parameters-basic-py\n)\ndoc_parameters_valuesdict.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/parameters_valuesdict.html#sphx-glr-examples-documentation-parameters-valuesdict-py\n)\nИспользование различных встроенных моделей: \nступенчатая аппроксимация \ndoc_builtinmodels_stepmodel.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/builtinmodels_stepmodel.html#sphx-glr-examples-documentation-builtinmodels-stepmodel-py\n), \ndoc_model_composite.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/model_composite.html#sphx-glr-examples-documentation-model-composite-py\n)\nсмесь двух гауссовых и экспоненциального распределения: \ndoc_builtinmodels_nistgauss.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/builtinmodels_nistgauss.html#sphx-glr-examples-documentation-builtinmodels-nistgauss-py\n), \ndoc_builtinmodels_nistgauss2.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/builtinmodels_nistgauss2.html#sphx-glr-examples-documentation-builtinmodels-nistgauss2-py\n)\nмодели с пиками (распределения Гаусса и Коши-Лоренца ) \ndoc_builtinmodels_peakmodels.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/builtinmodels_peakmodels.html#sphx-glr-examples-documentation-builtinmodels-peakmodels-py\n)\nсплайны \ndoc_builtinmodels_splinemodel.py\n (doc_builtinmodels_splinemodel.py)\nобзор различных возможностей (смесь распределений, дверительные интервалы, графики и т.д.) \ndoc_model_uncertainty2.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/model_uncertainty2.html#sphx-glr-examples-documentation-model-uncertainty2-py\n)\nалгортим \nemcee\n \ndoc_fitting_emcee.py\n (\nhttps://lmfit.github.io/lmfit-py/examples/documentation/fitting_emcee.html#sphx-glr-examples-documentation-fitting-emcee-py\n)\nВначале рассмотрим различные приемы аппроксимации в \nlmfit\n:\nпример 1 - аппроксимация с использованием интерфейса \nModel\n (самый простой случай);\nпример 2 - аппроксимация с использованием интерфейса \nModel\n и объекта класса \nParameter\n, расчетом доверительных интервалов;\nпример 3 - аппроксимация с использованием функции \nminimize\n;\nпример 4 - аппроксимация c использованием объекта класса \nMinimazer\n.\nЗатем рассмотрим различные частные случаи аппроксимации в \nlmfit\n:\nпример 5 - аппроксимация с заданием начальных значений (initial values) (на примере экспоненциальной зависимости I типа);\nпример 6 - аппроксимация c использованием встроенных моделей библиотеки \nlmfit\n;\nпример 7 - аппроксимация c использованием методов глобального поиска;\nпример 8 - обнаружение выбросов (outlier detection) при аппроксимации\nПример 1 - аппроксимация с использованием интерфейса Model (самый простой случай)\nДля начала рассмотрим самую обычную аппроксимацию на примере линейной зависимости, для подгонки параметров модели используем метод \nfit\n класса \nModel\n.\nПояснения к расчету:\nВначале исходные данные - двумерную таблицу (\nX1\n, \nY\n) - приводим к виду, отсортированному по возрастанию переменной \nX1\n, для того, чтобы встроенные графические возможности библиотеки \nlmfit\n корректно отображали информацию.\nС помощью \nlmfit.Model\n создаем модель.\nС помощью \nmodel.fit\n выполняем аппроксимацию (нахождение параметров). По умолчанию используется алгоритм Левенберга-Марквардта (Levenberg–Marquardt algorithm) (\nleastsq\n, \nleast_squares\n).\nС помощью \nresult.fit_report\n выводим отчет с результатами, который содержит: \nметрики качества аппроксимации: \nchi-square\n  - в данном случае сумма квадратов остатков \n (т.е. по сути \nSSE\n), \nreduced chi-square\n \n и \n;\nинформационные критерии Акаике и Байеса;\nпараметры модели \n и \n с их стандратными ошибками, относительными ошибками в % и начальными условиями (\ninit\n);\nкоэффициенты корреляции между параметрами \nC(b0, b1\n).\nВыводим различные виды графиков: \nresult.plot_fit()\n - обычный точечный график;\nresult.plot_residuals()\n - график остатков;\nresult.plot()\n - точечный график, совмещенный с графиком остатков.\nimport lmfit\nprint('{:<35}{:^0}'.format(\"Текущая версия модуля lmfit: \", lmfit.__version__), '\\n')\n\n# Preparation of input data\ndataset_sort_df = dataset_df.loc[:, ['X1', 'Y']].sort_values(by=['X1'])\n#display(dataset_sort_df)\nY_sort = np.array(dataset_sort_df['Y'])\n#print(Y_sort, type(Y_sort), len(Y_sort))\nX_sort = np.array(dataset_sort_df['X1'])\n#print(X_sort, type(X_sort), len(X_sort))\n\n# LINEAR MODEL\nfunc_name = 'linear'\nprint(f'{func_name.upper()} MODEL: {formulas_dict[func_name]}')\nfunc = models_dict[func_name]\np0 = p0_dict[func_name]\nmodel = lmfit.Model(func, independent_vars=['x'], name=func_name)\nresult = model.fit(Y_sort, x=X_sort, b0=p0[0], b1=p0[1])\nprint(result.fit_report())\nresult.plot_fit()\nplt.show()\nresult.plot_residuals()\nplt.show()\nresult.plot()\nplt.show()\n\nТекущая версия модуля lmfit:       1.1.0 \n\nLINEAR MODEL: y = b0 + b1*x\n[[Model]]\n    Model(linear)\n[[Fit Statistics]]\n    # fitting method   = leastsq\n    # function evals   = 7\n    # data points      = 55\n    # variables        = 2\n    chi-square         = 84.4203915\n    reduced chi-square = 1.59283758\n    Akaike info crit   = 27.5661686\n    Bayesian info crit = 31.5808350\n    R-squared          = 0.42259718\n[[Variables]]\n    b0:  11.8501350 +/- 0.37396000 (3.16%) (init = 0)\n    b1: -0.00217603 +/- 3.4938e-04 (16.06%) (init = 0)\n[[Correlations]] (unreported correlations are < 0.100)\n    C(b0, b1) = -0.890\nПример 2 - аппроксимация с использованием интерфейса Model и объекта класса Parameter, расчетом доверительных интервалов\nРассмотрим аппроксимацию с более широким использованием возможностей библиотеки \nlmfit\n на примере степенной зависимости:\nиспользуем объект класса \nParameter\n для задания свойств параметров модели;\nвыведем дополнительные отчеты;\nвыполним расчет доверительного интервала (подробнее см. \nhttps://lmfit.github.io/lmfit-py/confidence.html#lmfit.conf_interval\n, \nhttps://lmfit.github.io/lmfit-py/examples/example_confidence_interval.html#sphx-glr-examples-example-confidence-interval-py\n, \nhttps://lmfit.github.io/lmfit-py/examples/documentation/confidence_advanced.html#sphx-glr-examples-documentation-confidence-advanced-py\n)).\nПояснения к расчету:\nИсходные данные также должны быть отсортированы по возрастанию переменной \nX1\n.\nТакже с помощью \nlmfit.Model\n создаем модель.\nС помощью \nmodel.make_params\n задаем свойства параметров модели (в этом примере мы просто задаем начальные значения параметров, не затрагивая прочие атрибуты - минимальные и максимальные значения, алгебраические ограничения и т.д.).\nТакже с помощью \nmodel.fit\n выполняем аппроксимацию (нахождение параметров), при этом расширим набор используемых настроек - добавим \nmethod\n и \nnan_policy\n (правда, значения установим по умолчанию \nleastsq\n и \nraise\n).\nТакже с помощью \nresult.fit_report\n выводим отчет с результатами, при этом установим ограничение на коэффициент корреляции между параметрами (\nmin_correl=0.2\n).\nС помощью \nresult.params.pretty_print()\n выведем отчет о параметрах модели.\nС помощью \nresult.ci_report()\n выведем отчет о границах доверительных интервалов для параметров модели.\nС помощью \nresult.eval_components\n и \nresult.eval\n выведем расчетные значения модели: в первом случае - в виде словаря (dict), во-втором случае - в виде массива (numpy.ndarray).\nС помощью \nresult.eval_uncertainty\n выведем ширину доверительного интервала для расчетных значений.\nПо аналогии с предыдущим примером выведем графики \nresult.plot_fit()\n и \nresult.plot_residuals()\n с настройкой их атрибутов (размер окна, подписи осей и пр.).\nДополнительно выведем графики двух видов: \nна первом графике будут отображаться фактические данные (\ndata\n), расчетные данные по модели аппроксимации (\nbest fit\n) и также доверительный интервал шириной \n;\nна втором графике будет отображаться остатки (\nresiduals\n).\nЗамечание касательно расчета доверительных интервалов\n: для отдельных моделей \nlmfit\n может выдавать ошибку \nCannot determine Confidence Intervals without sensible uncertainty estimates\n, в этом случае необходимо установить пакет \nnumdifftools\n (\nhttps://pypi.org/project/numdifftools/\n).\n# POWER MODEL\nfunc_name = 'power'\nprint(f'{func_name.upper()} MODEL: {formulas_dict[func_name]}')\n\n# define objective function\nfunc = models_dict[func_name]\n\n# initial values\np0 = p0_dict[func_name]\n\n# create a Model\nmodel = lmfit.Model(func,   \n                    independent_vars=['x'],\n                    name=func_name)\n\n# create a set of Parameters\nparams = model.make_params(b0=p0[0],\n                           b1=p0[1])\n\n# calculations\nresult = model.fit(Y_sort, params, x=X_sort,\n                   method='leastsq',\n                   nan_policy='raise')\n\n# report of the fitting results (https://lmfit.github.io/lmfit-py/model.html#lmfit.model.ModelResult.fit_report)\nprint(f'result.fit_report:\\n{result.fit_report(min_correl=0.2)}\\n')\n\n# report of parameters data (https://lmfit.github.io/lmfit-py/parameters.html#lmfit.parameter.Parameters.pretty_print)\nprint('result.params.pretty_print():')\nresult.params.pretty_print()\nprint('\\n')\n\n# report of the confidence intervals (https://lmfit.github.io/lmfit-py/confidence.html#lmfit.ci_report)\nprint(f'result.ci_report:\\n{result.ci_report()}\\n')\n\n# evaluate each component of a composite model function (https://lmfit.github.io/lmfit-py/model.html#lmfit.model.ModelResult.eval_components)\ncomps = result.eval_components(x=X_sort)\nprint(f'result.eval_components = \\n{comps}\\n')\n\n# evaluate the uncertainty of the model function (https://lmfit.github.io/lmfit-py/model.html#lmfit.model.ModelResult.eval_uncertainty)\ndely = result.eval_uncertainty(sigma=3)\nprint(f'result.eval_uncertainty(sigma=3) = \\n{dely}\\n')\n\n# graphic (data, best-fit, uncertainty band and residuals)\nfig, axes = plt.subplots(2, 1, figsize=(297/INCH, 210/INCH))\nfig.suptitle(f'Model({func_name})', fontsize=16)\n\n#label = func_name + ' '+ r'$(R^2 = $' + f'{R2}' + ', ' + f'RMSE = {RMSE}' + ', ' + f'MAE = {MAE}' + ', ' + f'MSPE = {MSPE}' + ', ' + f'MAPE = {MAPE})'\naxes[0].set_title('data, best-fit, and uncertainty band')\nsns.scatterplot(x=X_sort, y=Y_sort, label='data', s=50, color='red', ax=axes[0])\nR2 = round(result.rsquared, DecPlace)\nsns.lineplot(x=X_sort, y=result.best_fit, color='blue', linewidth=2, legend=True, label='best fit (' + r'$R^2 = $' + f'{R2})', ax=axes[0])\naxes[0].fill_between(X_sort, result.best_fit-dely, result.best_fit+dely, color=\"grey\", alpha=0.3, label=r'3-$\\sigma$ band')\naxes[0].legend()\naxes[0].set_ylabel('Y')\n#axes[0].set_xlim(X1_min_graph, X1_max_graph)\n#axes[0].set_ylim(Y_min_graph, Y_max_graph)\n\nsns.scatterplot(x=X_sort, y=-result.residual, ax=axes[1], s=50)\naxes[1].axhline(y = 0, color = 'k', linewidth = 1)\naxes[1].set_xlabel('X')\naxes[1].set_ylabel('residuals')\n#axes[1].set_xlim(X1_min_graph, X1_max_graph)\n#axes[1].set_ylim(Y_min_graph, Y_max_graph)\n\nplt.show()\nPOWER MODEL: y = b0*x^b1\nresult.fit_report:\n[[Model]]\n    Model(power)\n[[Fit Statistics]]\n    # fitting method   = leastsq\n    # function evals   = 29\n    # data points      = 55\n    # variables        = 2\n    chi-square         = 49.8796865\n    reduced chi-square = 0.94112616\n    Akaike info crit   = -1.37456420\n    Bayesian info crit = 2.64010217\n    R-squared          = 0.65884224\n[[Variables]]\n    b0:  33.4032965 +/- 3.45275807 (10.34%) (init = 0)\n    b1: -0.18409814 +/- 0.01572801 (8.54%) (init = 0)\n[[Correlations]] (unreported correlations are < 0.200)\n    C(b0, b1) = -0.992\n\nresult.params.pretty_print():\nName     Value      Min      Max   Stderr     Vary     Expr Brute_Step\nb0      33.4     -inf      inf    3.453     True     None     None\nb1   -0.1841     -inf      inf  0.01573     True     None     None\n\n\nresult.ci_report:\n       99.73%    95.45%    68.27%    _BEST_    68.27%    95.45%    99.73%\n b0:  -9.61176  -6.50967  -3.33032  33.40330  +3.57568  +7.51972 +11.99711\n b1:  -0.04701  -0.03103  -0.01551  -0.18410  +0.01594  +0.03283  +0.05128\n\nresult.eval_components = \n{'power': array([17.1825, 12.2259, 11.9584, 11.2070, 11.1216, 10.7403, 10.4265,\n       10.3691, 10.3104, 10.2881, 10.2818, 10.1608, 10.1608, 10.0915,\n       10.0161, 10.0087,  9.9898,  9.9158,  9.8985,  9.7892,  9.7778,\n        9.7755,  9.7687,  9.7574,  9.7507,  9.6757,  9.6744,  9.6691,\n        9.6450,  9.5917,  9.5481,  9.4926,  9.4354,  9.4156,  9.4067,\n        9.3493,  9.3306,  9.3222,  9.3222,  9.3106,  9.2345,  9.1804,\n        9.1564,  9.0159,  9.0065,  8.9122,  8.8808,  8.8796,  8.8268,\n        8.6752,  8.6554,  8.1915,  8.1472,  8.1083,  7.7727])}\n\nresult.eval_uncertainty(sigma=3) = \n[2.5746 0.8186 0.7464 0.5692 0.5522 0.4861 0.4464 0.4407 0.4355 0.4337\n 0.4332 0.4249 0.4249 0.4213 0.4183 0.4180 0.4174 0.4158 0.4155 0.4151\n 0.4151 0.4151 0.4152 0.4153 0.4154 0.4167 0.4167 0.4168 0.4174 0.4191\n 0.4208 0.4234 0.4265 0.4277 0.4282 0.4320 0.4333 0.4339 0.4339 0.4348\n 0.4408 0.4454 0.4476 0.4613 0.4623 0.4726 0.4762 0.4763 0.4825 0.5013\n 0.5039 0.5682 0.5747 0.5804 0.6303]\nПример 3 - аппроксимация c использованием функции minimize\nПро функцию \nminimize\n подробнее см. \nhttps://lmfit.github.io/lmfit-py/fitting.html#the-minimize-function\n.\n# POWER MODEL\nfunc_name = 'power'\nprint(f'{func_name.upper()} MODEL: {formulas_dict[func_name]}')\n\n# define objective function\nfunc = models_dict[func_name]\n\n# initial values\np0 = p0_dict[func_name]\n\n# create a set of Parameters\nparams = lmfit.Parameters()\nparams.add('b0', value=p0[0], min=-inf, max=inf)\nparams.add('b1', value=p0[1], min=-inf, max=inf)\n\n# function for calculating residuals\nresidual_func = lambda params, x, ydata: func(x, params['b0'].value, params['b1'].value) - ydata\n\n# calculations\nresult = lmfit.minimize(residual_func, params, args=(X_sort, Y_sort))\nlmfit.report_fit(result)\n#print(lmfit.fit_report(result))    # other type of output\nbest_fit = Y_sort + result.residual\nPOWER MODEL: y = b0*x^b1\n[[Fit Statistics]]\n    # fitting method   = leastsq\n    # function evals   = 29\n    # data points      = 55\n    # variables        = 2\n    chi-square         = 49.8796865\n    reduced chi-square = 0.94112616\n    Akaike info crit   = -1.37456420\n    Bayesian info crit = 2.64010217\n[[Variables]]\n    b0:  33.4032965 +/- 3.45275807 (10.34%) (init = 0)\n    b1: -0.18409814 +/- 0.01572801 (8.54%) (init = 0)\n[[Correlations]] (unreported correlations are < 0.100)\n    C(b0, b1) = -0.992\nПример 4 - аппроксимация c использованием объекта класса Minimazer\nПро класс \nMinimazer\n подробнее см. \nhttps://lmfit.github.io/lmfit-py/fitting.html#using-the-minimizer-class\n.\n# POWER MODEL\nfunc_name = 'power'\nprint(f'{func_name.upper()} MODEL: {formulas_dict[func_name]}')\n\n# define objective function\nfunc = models_dict[func_name]\n\n# initial values\np0 = p0_dict[func_name]\n\n# create a set of Parameters\nparams = lmfit.Parameters()\nparams.add('b0', value=p0[0], min=-inf, max=inf)\nparams.add('b1', value=p0[1], min=-inf, max=inf)\n\n# function for calculating residuals\nresidual_func = lambda params, x, ydata: func(x, params['b0'].value, params['b1'].value) - ydata\n\n# create Minimizer\nminner = lmfit.Minimizer(residual_func, params, fcn_args=(X_sort, Y_sort), nan_policy='omit')\n\n# calculations\nresult = minner.minimize()\n#print(f'\\nfirst solve with {method_1.upper()} algorithm:')\nlmfit.report_fit(result)\n#print(lmfit.fit_report(result))    # other type of output\nbest_fit = Y_sort + result.residual\n#print(best_fit)\n\n# calculate the confidence intervals for parameters\nci, tr = lmfit.conf_interval(minner, result, sigmas=[3], trace=True)\nprint('\\nreport_ci(ci):')\nlmfit.report_ci(ci)\n\n# graphic (data, best-fit, and residuals)\nfig, axes = plt.subplots(2, 1, figsize=(297/INCH, 210/INCH))\nfig.suptitle(f'Model({func_name})', fontsize=16)\n\nsns.scatterplot(x=X_sort, y=Y_sort, label='data', s=50, color='red', ax=axes[0])\nsns.lineplot(x=X_sort, y=best_fit, color='blue', linewidth=2, legend=True, label='best fit', ax=axes[0])\naxes[0].set_ylabel('Y')\n#axes[0].set_xlim(X1_min_graph, X1_max_graph)\n#axes[0].set_ylim(Y_min_graph, Y_max_graph)\n\nsns.scatterplot(x=X_sort, y=-result.residual, ax=axes[1], s=50)\naxes[1].axhline(y = 0, color = 'k', linewidth = 1)\naxes[1].set_xlabel('X')\naxes[1].set_ylabel('residuals')\n#axes[1].set_xlim(X1_min_graph, X1_max_graph)\n#axes[1].set_ylim(Y_min_graph, Y_max_graph)\n\nplt.show()\nPOWER MODEL: y = b0*x^b1\n[[Fit Statistics]]\n    # fitting method   = leastsq\n    # function evals   = 29\n    # data points      = 55\n    # variables        = 2\n    chi-square         = 49.8796865\n    reduced chi-square = 0.94112616\n    Akaike info crit   = -1.37456420\n    Bayesian info crit = 2.64010217\n[[Variables]]\n    b0:  33.4032965 +/- 3.45275807 (10.34%) (init = 0)\n    b1: -0.18409814 +/- 0.01572801 (8.54%) (init = 0)\n[[Correlations]] (unreported correlations are < 0.100)\n    C(b0, b1) = -0.992\n\nreport_ci(ci):\n       99.73%    _BEST_    99.73%\n b0:  -9.61177  33.40330 +11.99711\n b1:  -0.04701  -0.18410  +0.05128\nПример 5 - аппроксимация с заданием начальных значений (initial values)\nНекоторые модели при аппроксимации требуют задания начальных значений - для успешной реализации алгоритма заданием нулей или единиц в качестве \np0\n не обойтись. Рассмотрим аппроксимацию экспоненциальной модели I типа с заданием начальных условий, при этом воспользуемся возможностью библиотеки \nlmfit\n вывести на график зависимость с оптимальными параметрами (\nbest fit\n) и зависимость с начальными параметрами (\ninit fit\n) это наглядно демонстрирует, как улучшается модель в процессе поиска оптимальных параметров.\n# EXPONENTIAL MODEL TYPE I: y = b0*exp(b1^x)\nfunc_name = 'exponential type 1'\nprint(f'{func_name.upper()} MODEL: {formulas_dict[func_name]}')\n\n# define objective function\nfunc = models_dict[func_name]\n\n# initial values\nn = len(Y_sort)\n(x1, x2, y1, y2) = (X_sort[0], X_sort[n-1], Y_sort[0], Y_sort[n-1])\np0_b0 = y2 * np.exp(np.log(y2/y1) * x2/(x1-x2))\np0_b1 = -np.log(y2/y1) / (x1-x2)\np0 = [p0_b0, p0_b1]\nprint(f'p0 = {p0}')\n\n# create a Model\nmodel = lmfit.Model(func,   \n                    independent_vars=['x'],\n                    name=func_name)\n\n# create a set of Parameters\nparams = lmfit.Parameters()\nparams.add('b0', value=p0[0], min=-inf, max=inf)\nparams.add('b1', value=p0[1], min=-inf, max=inf)\n\n# calculations\nresult = model.fit(Y_sort, params, x=X_sort,\n                   method='leastsq',\n                   nan_policy='raise')\n\n# report of the fitting results (https://lmfit.github.io/lmfit-py/model.html#lmfit.model.ModelResult.fit_report)\nprint(f'result.fit_report:\\n{result.fit_report(min_correl=0.2)}\\n')\n\n# report of the confidence intervals (https://lmfit.github.io/lmfit-py/confidence.html#lmfit.ci_report)\nprint(f'result.ci_report:\\n{result.ci_report()}\\n')\n\n# evaluate the uncertainty of the model function (https://lmfit.github.io/lmfit-py/model.html#lmfit.model.ModelResult.eval_uncertainty)\ndely = result.eval_uncertainty(sigma=3)\n\n# graphic (data, best-fit, uncertainty band and residuals)\nfig, axes = plt.subplots(2, 1, figsize=(297/INCH, 210/INCH))\nfig.suptitle(f'Model({func_name})', fontsize=16)\n\naxes[0].set_title('data, init-fit, best-fit, and uncertainty band')\nsns.scatterplot(x=X_sort, y=Y_sort, label='data', s=50, color='red', ax=axes[0])\nsns.lineplot(x=X_sort, y=result.init_fit, color='cyan', linewidth=2, linestyle='dotted', legend=True, label='init fit', ax=axes[0])\nR2 = round(result.rsquared, DecPlace)\nsns.lineplot(x=X_sort, y=result.best_fit, color='blue', linewidth=2, legend=True, label='best fit (' + r'$R^2 = $' + f'{R2})', ax=axes[0])\naxes[0].fill_between(X_sort, result.best_fit-dely, result.best_fit+dely, color=\"grey\", alpha=0.3, label=r'3-$\\sigma$ band')\naxes[0].legend()\naxes[0].set_ylabel('Y')\n#axes[0].set_xlim(X1_min_graph, X1_max_graph)\n#axes[0].set_ylim(Y_min_graph, Y_max_graph)\n\nsns.scatterplot(x=X_sort, y=-result.residual, ax=axes[1], s=50)\naxes[1].axhline(y = 0, color = 'k', linewidth = 1)\naxes[1].set_xlabel('X')\naxes[1].set_ylabel('residuals')\n#axes[1].set_xlim(X1_min_graph, X1_max_graph)\n#axes[1].set_ylim(Y_min_graph, Y_max_graph)\n\nplt.show()\nEXPONENTIAL TYPE 1 MODEL: y = b0*exp(b1^x)\np0 = [17.037879730518267, -0.00023560415890108336]\nresult.fit_report:\n[[Model]]\n    Model(exponential type 1)\n[[Fit Statistics]]\n    # fitting method   = leastsq\n    # function evals   = 19\n    # data points      = 55\n    # variables        = 2\n    chi-square         = 75.1949111\n    reduced chi-square = 1.41877191\n    Akaike info crit   = 21.2012704\n    Bayesian info crit = 25.2159368\n    R-squared          = 0.48569589\n[[Variables]]\n    b0:  12.6241795 +/- 0.46994783 (3.72%) (init = 17.03788)\n    b1: -2.7834e-04 +/- 3.9734e-05 (14.27%) (init = -0.0002356042)\n[[Correlations]] (unreported correlations are < 0.200)\n    C(b0, b1) = -0.899\n\nresult.ci_report:\n       99.73%    95.45%    68.27%    _BEST_    68.27%    95.45%    99.73%\n b0:  -1.48083  -0.98324  -0.49401  12.62418  +0.51299  +1.06128  +1.66495\n b1:  -0.00014  -0.00009  -0.00004  -0.00028  +0.00004  +0.00009  +0.00013\nПример 6 - аппроксимация с использованием встроенных моделей библиотеки lmfit\nРассмотрим аппроксимацию с использованием встроенных моделей библиотеки \nlmfit\n. Набор этих моделей достаточно обширен (подробнее см. \nhttps://lmfit.github.io/lmfit-py/builtin_models.html\n,\n \nhttps://lmfit.github.io/lmfit-py/examples/documentation/model_uncertainty2.html#sphx-glr-examples-documentation-model-uncertainty2-py\n), при этом для представляют интерес следующие модели:\nлинейная \nLinearModel\n;\nквадратическая \nQuadraticModel\n;\nполиномиальная \nPolynomialModel\n (мы рассмотрим случай m=3 - кубическую модель);\nэкспоненциальная \nExponentialModel\n;\nстепенная \nPowerLawModel\n.\nПри этом необходимо учитывать, что встроенные модели в библиотеке \nlmfit\n имеют свой формализованный вид и параметры, заданные в виде служебных слов, которые нужно учитывать в программном коде (так, например, параметры линейной модели имеют названия \nslope\n и \nintercept\n):\nНаименование\nУравнение\nПараметры и их названия в программном коде\nСсылка\nLinearModel\nslope\n (\n), \nintercept\n (\n)\nhttps://lmfit.github.io/lmfit-py/builtin_models.html#lmfit.models.LinearModel\nQuadraticModel\na\n, \nb\n, \nc\nhttps://lmfit.github.io/lmfit-py/builtin_models.html#lmfit.models.QuadraticModel\nPolynomialModel\nc0\n...\nc1\nhttps://lmfit.github.io/lmfit-py/builtin_models.html#lmfit.models.PolynomialModel\nExponentialModel\namplitude\n (\n), \ndecay\n (\n)\nhttps://lmfit.github.io/lmfit-py/builtin_models.html#lmfit.models.ExponentialModel\nPowerLawModel\namplitude\n (\n), \nexponent\n (\n)\nhttps://lmfit.github.io/lmfit-py/builtin_models.html#lmfit.models.PowerLawModel\nДля экспоненциальной модели \nExponentialModel\n мы оценим начальные значения параметров с помощью метода \nModel.guess\n (\nhttps://lmfit.github.io/lmfit-py/model.html#model-class-methods\n).\n# list of model\nmodel_list = ['linear', 'quadratic', 'qubic', 'power', 'exponential type 1']\n\n# model reference\nmodel_exponential_type_1 = lmfit.models.ExponentialModel(prefix='exponential_type_1_')\nmodel_dict = {\n        'linear':             lmfit.models.LinearModel(prefix='linear_'),\n        'quadratic':          lmfit.models.QuadraticModel(prefix='quadratic_'),\n        'qubic':              lmfit.models.PolynomialModel(degree=3, prefix='qubic_'),\n        'power':              lmfit.models.PowerLawModel(prefix='power_'),\n        'exponential type 1': model_exponential_type_1}\n\n# create a set of Parameters\nparams_dict = dict()\n\nparams_linear = lmfit.Parameters()\nparams_linear.add('linear_intercept',    value = 0, min = -inf, max = inf)\nparams_linear.add('linear_slope',        value = 0, min = -inf, max = inf)\nparams_dict['linear'] = params_linear\n\nparams_quadratic = lmfit.Parameters()\nparams_quadratic.add('quadratic_a',    value = 0, min = -inf, max = inf)\nparams_quadratic.add('quadratic_b',    value = 0, min = -inf, max = inf)\nparams_quadratic.add('quadratic_c',    value = 0, min = -inf, max = inf)\nparams_dict['quadratic'] = params_quadratic\n\nparams_qubic = lmfit.Parameters()\nparams_qubic.add('qubic_c0',    value = 0, min = -inf, max = inf)\nparams_qubic.add('qubic_c1',    value = 0, min = -inf, max = inf)\nparams_qubic.add('qubic_c2',    value = 0, min = -inf, max = inf)\nparams_qubic.add('qubic_c3',    value = 0, min = -inf, max = inf)\nparams_dict['qubic'] = params_qubic\n\nparams_power = lmfit.Parameters()\nparams_power.add('power_amplitude',    value = 0, min = -inf, max = inf)\nparams_power.add('power_exponent',     value = 0, min = -inf, max = inf)\nparams_dict['power'] = params_power\n\nparams_exponential_type_1 = lmfit.Parameters()\n#params_exponential_type_1.add('exponential_type_1_amplitude',    value = 0, min = -inf, max = inf)\n#params_exponential_type_1.add('exponential_type_1_decay',        value = 0, min = -inf, max = inf)\npars_exponential_type_1 = model_exponential_type_1.guess(Y_sort, x=X_sort)\nparams_dict['exponential type 1'] = pars_exponential_type_1\n\n# calculations\nresult_dict = dict()\nfor func_name in model_list:\n    model = model_dict[func_name]\n    result = model.fit(Y_sort, params_dict[func_name], x=X_sort)\n    print(f'{func_name.upper()} MODEL:\\n{result.fit_report(min_correl=0.2)}\\n')\n    result_dict[func_name] = result\n\n# graphic 1 (various models)\nfig, axes = plt.subplots(figsize=(420/INCH, 297/INCH))\n#fig.suptitle(f'Model(various models)', fontsize=16)\naxes.set_title('Model(various models)', fontsize=16)\nsns.scatterplot(x=X_sort, y=Y_sort, label='data', s=50, color='red', ax=axes)\nfor func_name in model_list:\n        result = result_dict[func_name]\n        R2 = round(result.rsquared, DecPlace)\n        label = func_name + ' (' + r'$R^2 = $' + f'{R2})'\n        sns.lineplot(x=X_sort, y=result.best_fit, linewidth=2, color=color_dict[func_name], legend=True, label=label, ax=axes)    # the color_dict was defined earlier\nplt.show()\n\n# graphic 2 (various models)\nnumber_models = len(model_list)\nfig, axes = plt.subplots(2, number_models, figsize=(210/INCH*number_models, 297/INCH))\nfig.suptitle('Model(various models)', fontsize=20)\nfor i, func_name in enumerate(model_list):\n        ax1 = plt.subplot(2, number_models, i+1)\n        ax2 = plt.subplot(2, number_models, i+1+number_models)\n        ax1.set_title(f'Model({func_name})', fontsize=16)\n        result = result_dict[func_name]\n        dely = result.eval_uncertainty(sigma=3)\n        sns.scatterplot(x=X_sort, y=Y_sort, label='data', s=50, color='red', ax=ax1)\n        R2 = round(result.rsquared, DecPlace)\n        label = func_name + ' (' + r'$R^2 = $' + f'{R2})'\n        sns.lineplot(x=X_sort, y=result.best_fit, linewidth=2, color=color_dict[func_name], legend=True, label=label, ax=ax1)    # the color_dict was defined earlier\n        ax1.fill_between(X_sort, result.best_fit-dely, result.best_fit+dely, color=\"grey\", alpha=0.3, label=r'3-$\\sigma$ band')\n        ax1.legend()\n        ax1.set_ylabel('Y')\n        sns.scatterplot(x=X_sort, y=-result.residual, ax=ax2, s=50)\n        ax2.axhline(y = 0, color = 'k', linewidth = 1)\n        ax2.set_xlabel('X')\n        ax2.set_ylabel('residuals')\nplt.show()\nLINEAR MODEL:\n[[Model]]\n    Model(linear, prefix='linear_')\n[[Fit Statistics]]\n    # fitting method   = leastsq\n    # function evals   = 7\n    # data points      = 55\n    # variables        = 2\n    chi-square         = 84.4203915\n    reduced chi-square = 1.59283758\n    Akaike info crit   = 27.5661686\n    Bayesian info crit = 31.5808350\n    R-squared          = 0.42259718\n[[Variables]]\n    linear_intercept:  11.8501350 +/- 0.37396000 (3.16%) (init = 0)\n    linear_slope:     -0.00217603 +/- 3.4938e-04 (16.06%) (init = 0)\n[[Correlations]] (unreported correlations are < 0.200)\n    C(linear_intercept, linear_slope) = -0.890\n\nQUADRATIC MODEL:\n[[Model]]\n    Model(parabolic, prefix='quadratic_')\n[[Fit Statistics]]\n    # fitting method   = leastsq\n    # function evals   = 13\n    # data points      = 55\n    # variables        = 3\n    chi-square         = 53.2610253\n    reduced chi-square = 1.02425049\n    Akaike info crit   = 4.23294056\n    Bayesian info crit = 10.2549401\n    R-squared          = 0.63571519\n[[Variables]]\n    quadratic_a:  1.8760e-06 +/- 3.4013e-07 (18.13%) (init = 0)\n    quadratic_b: -0.00704710 +/- 9.2652e-04 (13.15%) (init = 0)\n    quadratic_c:  14.3434596 +/- 0.54247230 (3.78%) (init = 0)\n[[Correlations]] (unreported correlations are < 0.200)\n    C(quadratic_a, quadratic_b) = -0.953\n    C(quadratic_b, quadratic_c) = -0.943\n    C(quadratic_a, quadratic_c) = 0.833\n\nQUBIC MODEL:\n[[Model]]\n    Model(polynomial, prefix='qubic_')\n[[Fit Statistics]]\n    # fitting method   = leastsq\n    # function evals   = 16\n    # data points      = 55\n    # variables        = 4\n    chi-square         = 52.4138335\n    reduced chi-square = 1.02772223\n    Akaike info crit   = 5.35105531\n    Bayesian info crit = 13.3803880\n    R-squared          = 0.64150965\n[[Variables]]\n    qubic_c0:  14.8384962 +/- 0.76977661 (5.19%) (init = 0)\n    qubic_c1: -0.00880398 +/- 0.00214609 (24.38%) (init = 0)\n    qubic_c2:  3.5207e-06 +/- 1.8432e-06 (52.35%) (init = 0)\n    qubic_c3: -4.1264e-10 +/- 4.5449e-10 (110.14%) (init = 0)\n[[Correlations]] (unreported correlations are < 0.200)\n    C(qubic_c2, qubic_c3) = -0.983\n    C(qubic_c1, qubic_c2) = -0.962\n    C(qubic_c0, qubic_c1) = -0.927\n    C(qubic_c1, qubic_c3) = 0.902\n    C(qubic_c0, qubic_c2) = 0.805\n    C(qubic_c0, qubic_c3) = -0.708\n\nPOWER MODEL:\n[[Model]]\n    Model(powerlaw, prefix='power_')\n[[Fit Statistics]]\n    # fitting method   = leastsq\n    # function evals   = 29\n    # data points      = 55\n    # variables        = 2\n    chi-square         = 49.8796865\n    reduced chi-square = 0.94112616\n    Akaike info crit   = -1.37456420\n    Bayesian info crit = 2.64010217\n    R-squared          = 0.65884224\n[[Variables]]\n    power_amplitude:  33.4032965 +/- 3.45275807 (10.34%) (init = 0)\n    power_exponent:  -0.18409814 +/- 0.01572801 (8.54%) (init = 0)\n[[Correlations]] (unreported correlations are < 0.200)\n    C(power_amplitude, power_exponent) = -0.992\n\nEXPONENTIAL TYPE 1 MODEL:\n[[Model]]\n    Model(exponential, prefix='exponential_type_1_')\n[[Fit Statistics]]\n    # fitting method   = leastsq\n    # function evals   = 16\n    # data points      = 55\n    # variables        = 2\n    chi-square         = 75.1949111\n    reduced chi-square = 1.41877191\n    Akaike info crit   = 21.2012705\n    Bayesian info crit = 25.2159368\n    R-squared          = 0.48569589\n[[Variables]]\n    exponential_type_1_amplitude:  12.6242247 +/- 0.46996177 (3.72%) (init = 11.84731)\n    exponential_type_1_decay:      3592.61300 +/- 512.762035 (14.27%) (init = 4650.427)\n[[Correlations]] (unreported correlations are < 0.200)\n    C(exponential_type_1_amplitude, exponential_type_1_decay) = -0.899\nПример 7 - аппроксимация с использованием методов глобального поиска\nРассмотрим более сложный случай на примере \nлогистической зависимости I типа\n \n. \nОбычно аппроксимация подобных зависимостей требует более тщательного подхода по сравнению с разобранными ранее примерами - необходимо варьировать алгоритмы, предварительно оценивать начальные точки для приближения (методом трех точек и т.д.). В этом случае эффективными могут оказаться \nметоды глобального поиска\n.\nВ рамках данного обзора у нас нет возможности сильно углубляться в данную тему, поэтому просто проиллюстрируем возможности \nlmfit\n - проведем аппроксимацию в два этапа:\nВначале оценим параметры зависимости алгоритмом \nbasinhopping\n (см. \nhttps://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.basinhopping.html\n, \nhttps://machinelearningmastery.com/basin-hopping-optimization-in-python/\n).\nПроверим оценку параметров алгоритмом глобальной оптимизации по сетке \nbrute\n (см. \nhttps://lmfit.github.io/lmfit-py/examples/example_brute.html\n).\nlogistic_type_1_func = lambda x, b0, b1, b2: b0 / (1 + b1*np.exp(-b2*x))\n\n# LOGISTIC MODEL TYPE I: y = b0 / (1 + b1*np.exp(-b2*x))\nprint('LOGISTIC MODEL TYPE I: y = b0 / (1 + b1*np.exp(-b2*x))')\n\n# define objective function\nfunc = logistic_type_1_func\nfunc_name = 'logistic type 1'\n\n# initial values\np0 = [0, 0, 0]\n\n# create a set of Parameters\nparams = lmfit.Parameters()\nparams.add('b0', value=p0[0], min=-inf, max=inf, brute_step=0.01)\nparams.add('b1', value=p0[1], min=-inf, max=inf, brute_step=0.01)\nparams.add('b2', value=p0[2], min=-inf, max=inf, brute_step=0.001)\n\n# function for calculating residuals\nresidual_func = lambda params, x, ydata: func(x, params['b0'].value,\n                                                 params['b1'].value,\n                                                 params['b2'].value\n                                                 ) - ydata\n\n# create Minimizer\nminner = lmfit.Minimizer(residual_func, params, fcn_args=(X_sort, Y_sort), nan_policy='omit')\n\n# first solve with basinhopping algorithm\nmethod_1 = 'basinhopping'\nresult_1 = minner.minimize(method=method_1)\nprint(f'\\nfirst solve with {method_1.upper()} algorithm:')\nlmfit.report_fit(result_1)\n#print(lmfit.fit_report(result_1))\nbest_fit_1 = Y_sort + result_1.residual\n#print(best_fit_1)\n\n# then solve with brute algorithm using the basinhopping solution as a starting point\nmethod_2 = 'brute'\nresult_2 = minner.minimize(method=method_2,\n                           params=result_1.params,\n                           Ns=30,\n                           keep=30)\nprint(f'\\nsecond solve with {method_2.upper()} algorithm using the {method_1.upper()} solution as a starting point:')\n#lmfit.report_fit(result_2)\nprint(lmfit.fit_report(result_2))\nbest_fit_2 = Y_sort + result_2.residual\n#print(best_fit_2)\n\n# calculate the confidence intervals for parameters\nci, tr = lmfit.conf_interval(minner, result_2, sigmas=[3], trace=True)\nlmfit.report_ci(ci)\n#print(tr)\n\n# graphics\nfig, axes = plt.subplots(2, 1, figsize=(297/INCH, 210/INCH))\nfig.suptitle(f'Model({func_name})', fontsize=14)\n\naxes[0].set_title('data, first fit and second fit')\nsns.scatterplot(x=X_sort, y=Y_sort, label='data', s=50, color='red', ax=axes[0])\nsns.lineplot(x=X_sort, y=best_fit_1, color='cyan', linewidth=6, legend=True, label=f'first fit ({method_1})', ax=axes[0])\nsns.lineplot(x=X_sort, y=best_fit_2, color='blue', linewidth=2, legend=True, label=f'second fit ({method_2})', ax=axes[0])\naxes[0].set_ylabel('Y')\n\nsns.scatterplot(x=X_sort, y=-result_2.residual, ax=axes[1], s=50)\naxes[1].axhline(y = 0, color = 'k', linewidth = 1)\naxes[1].set_ylabel('residuals')\n\nplt.show()\nirst solve with BASINHOPPING algorithm:\n[[Fit Statistics]]\n    # fitting method   = basinhopping\n    # function evals   = 17238\n    # data points      = 55\n    # variables        = 3\n    chi-square         = 51.0205702\n    reduced chi-square = 0.98116481\n    Akaike info crit   = 1.86926364\n    Bayesian info crit = 7.89126320\n[[Variables]]\n    b0:  7.71901123 +/- 0.45562749 (5.90%) (init = 0)\n    b1: -0.53569038 +/- 0.03472051 (6.48%) (init = 0)\n    b2:  0.00118052 +/- 2.6156e-04 (22.16%) (init = 0)\n[[Correlations]] (unreported correlations are < 0.100)\n    C(b0, b2) = 0.912\n    C(b0, b1) = 0.550\n    C(b1, b2) = 0.242\n\nsecond solve with BRUTE algorithm using the BASINHOPPING solution as a starting point:\n[[Fit Statistics]]\n    # fitting method   = brute\n    # function evals   = 28830\n    # data points      = 55\n    # variables        = 3\n    chi-square         = 51.0205702\n    reduced chi-square = 0.98116481\n    Akaike info crit   = 1.86926364\n    Bayesian info crit = 7.89126320\n##  Warning: uncertainties could not be estimated:\n[[Variables]]\n    b0:  7.71901123 +/- 0.45562749 (5.90%) (init = 0)\n    b1: -0.53569038 +/- 0.03472051 (6.48%) (init = 0)\n    b2:  0.00118052 +/- 2.6156e-04 (22.16%) (init = 0)\n[[Correlations]] (unreported correlations are < 0.100)\n    C(b0, b2) = 0.912\n    C(b0, b1) = 0.550\n    C(b1, b2) = 0.242\n       99.73%    _BEST_    99.73%\n b0:  -3.89422   7.71901  +1.37162\n b1:  -0.20046  -0.53569  +0.12587\n b2:  -0.00096   0.00118  +0.00149\nПример 8 - обнаружение выбросов (outlier detection) при аппроксимации\nВлияние выбросов на результаты аппроксимации может быть весьма существенным и игнорировать это влияние нельзя. Не будем сейчас сильно углубляться в этот вопрос, это предмет для отдельного рассмотрения, а просто проиллюстрируем прием выявления выбросов \nметодом исключения\n (см. \nhttps://lmfit.github.io/lmfit-py/examples/example_detect_outliers.html#sphx-glr-examples-example-detect-outliers-py\n). Этот метод очень прост и заключается в следующем: будем последовательно исключать точки \n из набора исходных данных, строить модель и анализировать, как влияет исключение точки на результат аппроксимации.\nВ качестве исходных данных рассмотрим зависимость расхода топлива (FuelFlow) от среднемесячной температуры (Temperature) из нашего датасета:\ngraph_scatterplot_sns(\n    X2, Y,\n    Xmin=X2_min_graph, Xmax=X2_max_graph,\n    Ymin=Y_min_graph, Ymax=Y_max_graph,\n    color='orange',\n    title_figure=Title_String, title_figure_fontsize=14,\n    title_axes='Dependence of FuelFlow (Y) on temperature (X2)', title_axes_fontsize=16,\n    x_label=Variable_Name_X2,\n    y_label=Variable_Name_Y,\n    label_fontsize=14, tick_fontsize=12,\n    label_legend='', label_legend_fontsize=12,\n    s=80)\nПодготовим исходные данные:\n# Preparation of input data\ndataset_sort_df = dataset_df.loc[:, ['X2', 'Y']].sort_values(by=['X2'])\ndisplay(dataset_sort_df.describe())\n\nY_sort = np.array(dataset_sort_df['Y'])\nX_sort = np.array(dataset_sort_df['X2'])\nПостроим линейную модель:\n# LINEAR MODEL\nfunc_name = 'linear'\nprint(f'{func_name.upper()} MODEL: {formulas_dict[func_name]}')\n\nfunc = models_dict[func_name]\np0 = p0_dict[func_name]\n\nmodel = lmfit.Model(func,   \n                    independent_vars=['x'],\n                    name=func_name)\n\nparams = lmfit.Parameters()\nparams.add('b0', value=p0[0], min=-inf, max=inf)\nparams.add('b1', value=p0[1], min=-inf, max=inf)\n\nresult = model.fit(Y_sort, params, x=X_sort,\n                   method='leastsq',\n                   nan_policy='raise')\n\nprint(f'result.fit_report:\\n{result.fit_report(min_correl=0.2)}\\n')\ndely = result.eval_uncertainty(sigma=3)\n\n# graphic (data, best-fit, uncertainty band and residuals)\nfig, axes = plt.subplots(2, 1, figsize=(297/INCH, 210/INCH))\nfig.suptitle(f'Model({func_name})', fontsize=16)\n\naxes[0].set_title('data, init-fit, best-fit, and uncertainty band')\nsns.scatterplot(x=X_sort, y=Y_sort, label='data', s=50, color='red', ax=axes[0])\nR2 = round(result.rsquared, DecPlace)\nsns.lineplot(x=X_sort, y=result.best_fit, color='blue', linewidth=2, legend=True, label='best fit (' + r'$R^2 = $' + f'{R2})', ax=axes[0])\naxes[0].fill_between(X_sort, result.best_fit-dely, result.best_fit+dely, color=\"grey\", alpha=0.3, label=r'3-$\\sigma$ band')\naxes[0].legend()\naxes[0].set_ylabel('Y')\n#axes[0].set_xlim(X1_min_graph, X1_max_graph)\naxes[0].set_ylim(Y_min_graph, Y_max_graph)\n\nsns.scatterplot(x=X_sort, y=-result.residual, ax=axes[1], s=50)\naxes[1].axhline(y = 0, color = 'k', linewidth = 1)\naxes[1].set_xlabel('X')\naxes[1].set_ylabel('residuals')\n#axes[1].set_xlim(X1_min_graph, X1_max_graph)\n#axes[1].set_ylim(Y_min_graph, Y_max_graph)\n\nplt.show()\nLINEAR MODEL: y = b0 + b1*x\nresult.fit_report:\n[[Model]]\n    Model(linear)\n[[Fit Statistics]]\n    # fitting method   = leastsq\n    # function evals   = 7\n    # data points      = 55\n    # variables        = 2\n    chi-square         = 126.256130\n    reduced chi-square = 2.38219113\n    Akaike info crit   = 49.7038688\n    Bayesian info crit = 53.7185352\n    R-squared          = 0.13645691\n[[Variables]]\n    b0:  10.1218956 +/- 0.23996519 (2.37%) (init = 0)\n    b1: -0.06161458 +/- 0.02129069 (34.55%) (init = 0)\n[[Correlations]] (unreported correlations are < 0.200)\n    C(b0, b1) = -0.498\n\nГрафик влияния на метрику качества аппроксимации - \nreduced chi-square\n \n:\nfrom collections import defaultdict\n\nbest_vals = defaultdict(lambda: np.zeros(X_sort.size))\nstderrs = defaultdict(lambda: np.zeros(X_sort.size))\nchi_sq = np.zeros_like(X_sort)\n\nfor i in range(X_sort.size):\n    idx2 = np.arange(0, X_sort.size)\n    idx2 = np.delete(idx2, i)\n    tmp_x = X_sort[idx2]\n    tmp = model.fit(Y_sort[idx2], x=tmp_x, b0=result.params['b0'], b1=result.params['b1'])\n    \n    chi_sq[i] = tmp.chisqr\n    for p in tmp.params:\n        tpar = tmp.params[p]\n        best_vals[p][i] = tpar.value\n        stderrs[p][i] = (tpar.stderr / result.params[p].stderr)\n\nfig, ax = plt.subplots(figsize=(297/INCH, 210/INCH))\nax.plot(X_sort, (result.chisqr - chi_sq) / chi_sq)\nax.set_ylabel(r'Relative red. $\\chi^2$ change')\nax.set_xlabel('X')\nax.legend()\nplt.show()\nГрафик влияния на значения параметров \n, \n и их ошибки:\nfig, axs = plt.subplots(4, figsize=(297/INCH, 210/INCH*3), sharex='col')\naxs[0].plot(X_sort, best_vals['b0'])\naxs[0].set_ylabel('best b0')\n\naxs[1].plot(X_sort, best_vals['b1'])\naxs[1].set_ylabel('best b1')\n\naxs[2].plot(X_sort, stderrs['b0'])\naxs[2].set_ylabel('err b0 change')\n\naxs[3].plot(X_sort, stderrs['b1'])\naxs[3].set_ylabel('err b1 change')\n\naxs[3].set_xlabel('X')\nplt.show()\nВидим, что выброс \n в интервале \n очень хорошо заметен на всех графиках.\n Наконец, исключим выброс и построим линейную модель по очищенным данным:\nmask = dataset_sort_df['Y'] < 15\ndataset_sort_df_clear = dataset_sort_df[mask]\ndisplay(dataset_sort_df_clear.describe())\n\nX_sort_clear = np.array(dataset_sort_df_clear['X2'])\nY_sort_clear = np.array(dataset_sort_df_clear['Y'])\n# LINEAR MODEL\nfunc_name = 'linear'\nprint(f'{func_name.upper()} MODEL: {formulas_dict[func_name]}')\n\nfunc = models_dict[func_name]\np0 = p0_dict[func_name]\n\nmodel = lmfit.Model(func,   \n                    independent_vars=['x'],\n                    name=func_name)\n\nparams = lmfit.Parameters()\nparams.add('b0', value=p0[0], min=-inf, max=inf)\nparams.add('b1', value=p0[1], min=-inf, max=inf)\n\nresult = model.fit(Y_sort_clear, params, x=X_sort_clear,\n                   method='leastsq',\n                   nan_policy='raise')\n\nprint(f'result.fit_report:\\n{result.fit_report(min_correl=0.2)}\\n')\ndely = result.eval_uncertainty(sigma=3)\n\n# graphic (data, best-fit, uncertainty band and residuals)\nfig, axes = plt.subplots(2, 1, figsize=(297/INCH, 210/INCH))\nfig.suptitle(f'Model({func_name})', fontsize=16)\n\naxes[0].set_title('data, init-fit, best-fit, and uncertainty band')\nsns.scatterplot(x=X_sort_clear, y=Y_sort_clear, label='data', s=50, color='red', ax=axes[0])\nR2 = round(result.rsquared, DecPlace)\nsns.lineplot(x=X_sort_clear, y=result.best_fit, color='blue', linewidth=2, legend=True, label='best fit (' + r'$R^2 = $' + f'{R2})', ax=axes[0])\naxes[0].fill_between(X_sort_clear, result.best_fit-dely, result.best_fit+dely, color=\"grey\", alpha=0.3, label=r'3-$\\sigma$ band')\naxes[0].legend()\naxes[0].set_ylabel('Y')\n#axes[0].set_xlim(X1_min_graph, X1_max_graph)\naxes[0].set_ylim(Y_min_graph, Y_max_graph)\n\nsns.scatterplot(x=X_sort_clear, y=-result.residual, ax=axes[1], s=50)\naxes[1].axhline(y = 0, color = 'k', linewidth = 1)\naxes[1].set_xlabel('X')\naxes[1].set_ylabel('residuals')\n#axes[1].set_xlim(X1_min_graph, X1_max_graph)\n#axes[1].set_ylim(Y_min_graph, Y_max_graph)\n\nplt.show()\nLINEAR MODEL: y = b0 + b1*x\nresult.fit_report:\n[[Model]]\n    Model(linear)\n[[Fit Statistics]]\n    # fitting method   = leastsq\n    # function evals   = 7\n    # data points      = 54\n    # variables        = 2\n    chi-square         = 67.8407274\n    reduced chi-square = 1.30462937\n    Akaike info crit   = 16.3216480\n    Bayesian info crit = 20.2996161\n    R-squared          = 0.28334888\n[[Variables]]\n    b0:  10.0379989 +/- 0.17802593 (1.77%) (init = 0)\n    b1: -0.07177285 +/- 0.01582893 (22.05%) (init = 0)\n[[Correlations]] (unreported correlations are < 0.200)\n    C(b0, b1) = -0.488\nВидим, что исключение выброса позволило улучшить качество аппроксимации с \n до \n.\n Сравнительный анализ инструментов аппроксимации\nОчевидно, что библиотека \nlmfit\n является наиболее совершенным инструментом аппроксимации в \npython\n, но и в то же время - наиболее сложным в освоении. Для более простых задач вполне может использоваться \nscipy.optimize.curve_fit\n.\nПро быстродействие различных инструментов аппроксимации - см., например, \nhttps://mmas.github.io/least-squares-fitting-numpy-scipy\n.\nВ общем, каждый исследователь вправе сам выбирать инструменты для решения своих задач.\nСОЗДАНИЕ ПОЛЬЗОВАТЕЛЬСКОЙ ФУНКЦИИ ДЛЯ АППРОКСИМАЦИИ ЗАВИСИМОСТЕЙ\nСоздадим пользовательскую функцию \nsimple_approximation\n, которая позволит выполнять простую аппроксимацию зависимостей (по аналогии с тем, как это реализовано в MS Excel), а именно:\nработать с наиболее распространенными моделями аппроксимации (линейной, квадратической, кубической, степенной, экспоненциальной, логарифмической, гиперболической);\nвыполнять визуализацию, в том числе нескольких моделей на одном графике;\nвыводить на графике уравнение моделей;\nвыводить на графике основные метрики качества моделей (\n, \n, \n);\nвыводить числовые значения параметров моделей и метрик качества в виде DataFrame;\nвыводить значения \n, рассчитанные по моделям, в виде DataFrame.\nРазумеется, данная функция не претендует на абсолютную полноту, она предназначена только лишь для облегчения предварительных исследований. В случае необходимости расширить набор моделей и/или выводимых данных, любой исследователь сможет получить необходимый результат по аналогии с тем, как это было продемонстрировано в примерах расчетов выше.\nФункция \nsimple_approximation\n имеет следующие параметры:\nX_in\n, \nY_in\n - исходные значения переменных \n и \n;\nmodels_list_in\n - список моделей для аппроксимации (например, полный перечень \nmodels_list = ['linear', 'quadratic', 'qubic', 'power', 'exponential', 'logarithmic', 'hyperbolic']\n);\np0_dict_in=None\n - словарь (dict) с начальными значениями параметров (по умолчанию равны 1);\nXmin\n, \nXmax\n, \nYmin\n, \nYmax\n - границы значений для построения графиков;\nnx_in=100\n - число точек, на которое разбивается область значений переменной \n для построения графиков;\nDecPlace=4\n - точность (число знаков после запятой) выводимых значений параметров и метрик качества моделей;\nresult_table=False\n, \nvalue_table_calc=False\n, \nvalue_table_graph=False\n - выводить или нет (True/False) DataFrame с параметрами моделей и метриками качества, с расчетными значениями переменной \n, с расчетными значениями переменной \n для графиков;\ntitle_figure=None\n, \ntitle_figure_fontsize=16\n, \ntitle_axes=None\n, \ntitle_axes_fontsize=18\n - заголовки и шрифт заголовков графиков;\nx_label=None\n, \ny_label=None\n - наименования осей графиков;\nlinewidth=2\n - толщина линий графиков;\nlabel_fontsize=14\n, \ntick_fontsize=12\n, \nlabel_legend_fontsize=12\n - заголовки и шрифт легенды;\ncolor_list_in=None\n - словарь (dict), задающий цвета для графиков различных моделей (по умолчанию используется заданный в функции);\nb0_formatter=None\n, \nb1_formatter=None\n, \nb2_formatter=None\n, \nb3_formatter=None\n - числовой формат для отображения параметров моделей (по умолчанию используется заданный в функции);\ngraph_size=(420/INCH, 297/INCH)\n - размер окна графиков;\nfile_name=None\n - имя файла для сохранения графика на диске.\nФункция \nsimple_approximation\n возвращает:\nresult_df\n - DataFrame с параметрами моделей и метриками качества;\nvalue_table_calc\n и \nvalue_table_graph\n - DataFrame с расчетными значениями переменной \n, с расчетными значениями переменной \n для графиков;\nвыводит график аппроксимации.\ndef simple_approximation(\n    X_in, Y_in,\n    models_list_in,\n    p0_dict_in=None,\n    Xmin=None, Xmax=None,\n    Ymin=None, Ymax=None,\n    nx_in=100,\n    DecPlace=4,\n    result_table=False, value_table_calc=False, value_table_graph=False,\n    title_figure=None, title_figure_fontsize=16,\n    title_axes=None, title_axes_fontsize=18,\n    x_label=None, y_label=None,\n    linewidth=2,\n    label_fontsize=14, tick_fontsize=12, label_legend_fontsize=12,\n    color_list_in=None,\n    b0_formatter=None, b1_formatter=None, b2_formatter=None, b3_formatter=None,\n    graph_size=(420/INCH, 297/INCH),\n    file_name=None):\n    \n    # Equations\n    linear_func = lambda x, b0, b1: b0 + b1*x\n    quadratic_func = lambda x, b0, b1, b2: b0 + b1*x + b2*x**2\n    qubic_func = lambda x, b0, b1, b2, b3: b0 + b1*x + b2*x**2 + b3*x**3\n    power_func = lambda x, b0, b1: b0 * x**b1\n    exponential_func = lambda x, b0, b1: b0 * np.exp(b1*x)\n    logarithmic_func = lambda x, b0, b1: b0 + b1*np.log(x)\n    hyperbolic_func = lambda x, b0, b1: b0 + b1/x\n    \n    # Model reference\n    p0_dict = {\n        'linear':      [1, 1],\n        'quadratic':   [1, 1, 1],\n        'qubic':       [1, 1, 1, 1],\n        'power':       [1, 1],\n        'exponential': [1, 1],\n        'logarithmic': [1, 1],\n        'hyperbolic':  [1, 1]}\n    \n    models_dict = {\n        'linear':      [linear_func,      p0_dict['linear']],\n        'quadratic':   [quadratic_func,   p0_dict['quadratic']],\n        'qubic':       [qubic_func,       p0_dict['qubic']],\n        'power':       [power_func,       p0_dict['power']],\n        'exponential': [exponential_func, p0_dict['exponential']],\n        'logarithmic': [logarithmic_func, p0_dict['logarithmic']],\n        'hyperbolic':  [hyperbolic_func,  p0_dict['hyperbolic']]}\n    \n    models_df = pd.DataFrame({\n        'func': (\n            linear_func,\n            quadratic_func,\n            qubic_func,\n            power_func,\n            exponential_func,\n            logarithmic_func,\n            hyperbolic_func),\n        'p0': (\n            p0_dict['linear'],\n            p0_dict['quadratic'],\n            p0_dict['qubic'],\n            p0_dict['power'],\n            p0_dict['exponential'],\n            p0_dict['logarithmic'],\n            p0_dict['hyperbolic'])},\n        index=['linear', 'quadratic', 'qubic', 'power', 'exponential', 'logarithmic', 'hyperbolic'])\n        \n    models_dict_keys_list = list(models_dict.keys())\n    models_dict_values_list = list(models_dict.values())\n        \n    # Initial guess for the parameters\n    if p0_dict_in:\n        p0_dict_in_keys_list = list(p0_dict_in.keys())\n        for elem in models_dict_keys_list:\n            if elem in p0_dict_in_keys_list:\n                models_dict[elem][1] = p0_dict_in[elem]\n            \n    # Calculations\n    X_fact = np.array(X_in)\n    Y_fact = np.array(Y_in)\n    \n    nx = 100 if not(nx_in) else nx_in\n    hx = (Xmax - Xmin)/(nx - 1)\n    X_calc_graph = np.linspace(Xmin, Xmax, nx)\n    \n    parameters_list = list()\n    models_list = list()\n    \n    error_metrics_df = pd.DataFrame(columns=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE', 'RMSLE', 'R2'])\n    Y_calc_graph_df = pd.DataFrame({'X': X_calc_graph})\n    Y_calc_df = pd.DataFrame({\n        'X_fact': X_fact,\n        'Y_fact': Y_fact})\n    \n    for elem in models_list_in:\n        if elem in models_dict_keys_list:\n            func = models_dict[elem][0]\n            p0 = models_dict[elem][1]\n            popt_, _ = curve_fit(func, X_fact, Y_fact, p0=p0)\n            models_dict[elem].append(popt_)\n            Y_calc_graph = func(X_calc_graph, *popt_)\n            Y_calc = func(X_fact, *popt_)\n            Y_calc_graph_df[elem] = Y_calc_graph\n            Y_calc_df[elem] = Y_calc\n            parameters_list.append(popt_)\n            models_list.append(elem)\n            (model_error_metrics, result_error_metrics) = regression_error_metrics(Yfact=Y_fact, Ycalc=Y_calc_df[elem], model_name=elem)\n            error_metrics_df = pd.concat([error_metrics_df, result_error_metrics])\n                \n    parameters_df = pd.DataFrame(parameters_list,\n                                 index=models_list)\n    parameters_df = parameters_df.add_prefix('b')\n    result_df = parameters_df.join(error_metrics_df)\n                        \n    # Legend for a linear model\n    if \"linear\" in models_list_in:\n        b0_linear = round(result_df.loc[\"linear\", \"b0\"], DecPlace)\n        b0_linear_str = str(b0_linear)\n        b1_linear = round(result_df.loc[\"linear\", \"b1\"], DecPlace)\n        b1_linear_str = f' + {b1_linear}' if b1_linear > 0 else f' - {abs(b1_linear)}'\n        R2_linear = round(result_df.loc[\"linear\", \"R2\"], DecPlace)\n        MSPE_linear = result_df.loc[\"linear\", \"MSPE\"]\n        MAPE_linear = result_df.loc[\"linear\", \"MAPE\"]\n        label_linear = 'linear: ' + r'$Y_{calc} = $' + b0_linear_str + b1_linear_str + f'{chr(183)}X' + ' ' + \\\n            r'$(R^2 = $' + f'{R2_linear}' + ', ' + f'MSPE = {MSPE_linear}' + ', ' + f'MAPE = {MAPE_linear})'\n    \n    # Legend for a quadratic model\n    if \"quadratic\" in models_list_in:\n        b0_quadratic = round(result_df.loc[\"quadratic\", \"b0\"], DecPlace)\n        b0_quadratic_str = str(b0_quadratic)\n        b1_quadratic = result_df.loc[\"quadratic\", \"b1\"]\n        b1_quadratic_str = f' + {b1_quadratic:.{DecPlace}e}' if b1_quadratic > 0 else f' - {abs(b1_quadratic):.{DecPlace}e}'\n        b2_quadratic = result_df.loc[\"quadratic\", \"b2\"]\n        b2_quadratic_str = f' + {b2_quadratic:.{DecPlace}e}' if b2_quadratic > 0 else f' - {abs(b2_quadratic):.{DecPlace}e}'\n        R2_quadratic = round(result_df.loc[\"quadratic\", \"R2\"], DecPlace)\n        MSPE_quadratic = result_df.loc[\"quadratic\", \"MSPE\"]\n        MAPE_quadratic = result_df.loc[\"quadratic\", \"MAPE\"]\n        label_quadratic = 'quadratic: ' + r'$Y_{calc} = $' + b0_quadratic_str + b1_quadratic_str + f'{chr(183)}X' + b2_quadratic_str + f'{chr(183)}' + r'$X^2$' + ' ' + \\\n            r'$(R^2 = $' + f'{R2_quadratic}' + ', ' + f'MSPE = {MSPE_quadratic}' + ', ' + f'MAPE = {MAPE_quadratic})'\n    \n    # Legend for a qubic model\n    if \"qubic\" in models_list_in:\n        b0_qubic = round(result_df.loc[\"qubic\", \"b0\"], DecPlace)\n        b0_qubic_str = str(b0_qubic)\n        b1_qubic = result_df.loc[\"qubic\", \"b1\"]\n        b1_qubic_str = f' + {b1_qubic:.{DecPlace}e}' if b1_qubic > 0 else f' - {abs(b1_qubic):.{DecPlace}e}'\n        b2_qubic = result_df.loc[\"qubic\", \"b2\"]\n        b2_qubic_str = f' + {b2_qubic:.{DecPlace}e}' if b2_qubic > 0 else f' - {abs(b2_qubic):.{DecPlace}e}'\n        b3_qubic = result_df.loc[\"qubic\", \"b3\"]\n        b3_qubic_str = f' + {b3_qubic:.{DecPlace}e}' if b3_qubic > 0 else f' - {abs(b3_qubic):.{DecPlace}e}'\n        R2_qubic = round(result_df.loc[\"qubic\", \"R2\"], DecPlace)\n        MSPE_qubic = result_df.loc[\"qubic\", \"MSPE\"]\n        MAPE_qubic = result_df.loc[\"qubic\", \"MAPE\"]\n        label_qubic = 'qubic: ' + r'$Y_{calc} = $' + b0_qubic_str + b1_qubic_str + f'{chr(183)}X' + \\\n            b2_qubic_str + f'{chr(183)}' + r'$X^2$' + b3_qubic_str + f'{chr(183)}' + r'$X^3$' + ' ' + \\\n            r'$(R^2 = $' + f'{R2_qubic}' + ', ' + f'MSPE = {MSPE_qubic}' + ', ' + f'MAPE = {MAPE_qubic})'\n    \n    # Legend for a power model\n    if \"power\" in models_list_in:\n        b0_power = round(result_df.loc[\"power\", \"b0\"], DecPlace)\n        b0_power_str = str(b0_power)\n        b1_power = round(result_df.loc[\"power\", \"b1\"], DecPlace)\n        b1_power_str = str(b1_power)\n        R2_power = round(result_df.loc[\"power\", \"R2\"], DecPlace)\n        MSPE_power = result_df.loc[\"power\", \"MSPE\"]\n        MAPE_power = result_df.loc[\"power\", \"MAPE\"]\n        label_power = 'power: ' + r'$Y_{calc} = $' + b0_power_str + f'{chr(183)}' + r'$X$'\n        for elem in b1_power_str:\n            label_power = label_power + r'$^{}$'.format(elem)\n        label_power = label_power  + ' ' + r'$(R^2 = $' + f'{R2_power}' + ', ' + f'MSPE = {MSPE_power}' + ', ' + f'MAPE = {MAPE_power})'\n    \n    # Legend for a exponential model\n    if \"exponential\" in models_list_in:\n        b0_exponential = round(result_df.loc[\"exponential\", \"b0\"], DecPlace)\n        b0_exponential_str = str(b0_exponential)\n        b1_exponential = result_df.loc[\"exponential\", \"b1\"]\n        b1_exponential_str = f'{b1_exponential:.{DecPlace}e}'\n        R2_exponential = round(result_df.loc[\"exponential\", \"R2\"], DecPlace)\n        MSPE_exponential = result_df.loc[\"exponential\", \"MSPE\"]\n        MAPE_exponential = result_df.loc[\"exponential\", \"MAPE\"]\n        label_exponential = 'exponential: ' + r'$Y_{calc} = $' + b0_exponential_str + f'{chr(183)}' + r'$e$'\n        for elem in b1_exponential_str:\n            label_exponential = label_exponential + r'$^{}$'.format(elem)\n        label_exponential = label_exponential + r'$^{}$'.format(chr(183)) + r'$^X$' + ' ' + \\\n            r'$(R^2 = $' + f'{R2_exponential}' + ', ' + f'MSPE = {MSPE_exponential}' + ', ' + f'MAPE = {MAPE_exponential})'\n    \n    # Legend for a logarithmic model\n    if \"logarithmic\" in models_list_in:\n        b0_logarithmic = round(result_df.loc[\"logarithmic\", \"b0\"], DecPlace)\n        b0_logarithmic_str = str(b0_logarithmic)\n        b1_logarithmic = round(result_df.loc[\"logarithmic\", \"b1\"], DecPlace)\n        b1_logarithmic_str = f' + {b1_logarithmic}' if b1_logarithmic > 0 else f' - {abs(b1_logarithmic)}'\n        R2_logarithmic = round(result_df.loc[\"logarithmic\", \"R2\"], DecPlace)\n        MSPE_logarithmic = result_df.loc[\"logarithmic\", \"MSPE\"]\n        MAPE_logarithmic = result_df.loc[\"logarithmic\", \"MAPE\"]\n        label_logarithmic = 'logarithmic: ' + r'$Y_{calc} = $' + b0_logarithmic_str + b1_logarithmic_str + f'{chr(183)}ln(X)' + ' ' + \\\n            r'$(R^2 = $' + f'{R2_logarithmic}' + ', ' + f'MSPE = {MSPE_logarithmic}' + ', ' + f'MAPE = {MAPE_logarithmic})'\n    \n    # Legend for a hyperbolic model\n    if \"hyperbolic\" in models_list_in:\n        b0_hyperbolic = round(result_df.loc[\"hyperbolic\", \"b0\"], DecPlace)\n        b0_hyperbolic_str = str(b0_hyperbolic)\n        b1_hyperbolic = round(result_df.loc[\"hyperbolic\", \"b1\"], DecPlace)\n        b1_hyperbolic_str = f' + {b1_hyperbolic}' if b1_hyperbolic > 0 else f' - {abs(b1_hyperbolic)}'\n        R2_hyperbolic = round(result_df.loc[\"hyperbolic\", \"R2\"], DecPlace)\n        MSPE_hyperbolic = result_df.loc[\"hyperbolic\", \"MSPE\"]\n        MAPE_hyperbolic = result_df.loc[\"hyperbolic\", \"MAPE\"]\n        label_hyperbolic = 'hyperbolic: ' + r'$Y_{calc} = $' + b0_hyperbolic_str + b1_hyperbolic_str + f' / X' + ' ' + \\\n            r'$(R^2 = $' + f'{R2_hyperbolic}' + ', ' + f'MSPE = {MSPE_hyperbolic}' + ', ' + f'MAPE = {MAPE_hyperbolic})'\n        \n    # Legends\n    label_legend_dict = {\n        'linear':      label_linear if \"linear\" in models_list_in else '',\n        'quadratic':   label_quadratic if \"quadratic\" in models_list_in else '',\n        'qubic':       label_qubic if \"qubic\" in models_list_in else '',\n        'power':       label_power if \"power\" in models_list_in else '',\n        'exponential': label_exponential if \"exponential\" in models_list_in else '',\n        'logarithmic': label_logarithmic if \"logarithmic\" in models_list_in else '',\n        'hyperbolic':  label_hyperbolic if \"hyperbolic\" in models_list_in else ''}\n    \n    # Graphics\n    color_dict = {\n        'linear':      'blue',\n        'quadratic':   'green',\n        'qubic':       'brown',\n        'power':       'magenta',\n        'exponential': 'cyan',\n        'logarithmic': 'orange',\n        'hyperbolic':  'grey'}\n    \n    if not(Xmin) and not(Xmax):\n        Xmin = min(X_fact)*0.99\n        Xmax = max(X_fact)*1.01\n    if not(Ymin) and not(Ymax):\n        Ymin = min(Y_fact)*0.99\n        Ymax = max(Y_fact)*1.01     \n        \n    fig, axes = plt.subplots(figsize=(graph_size))\n    fig.suptitle(title_figure, fontsize = title_figure_fontsize)\n    axes.set_title(title_axes, fontsize = title_axes_fontsize)\n    \n    sns.scatterplot(\n        x=X_fact, y=Y_fact,\n        label='data',\n        s=50,\n        color='red',\n        ax=axes)\n       \n    for elem in models_list_in:\n        if elem in models_dict_keys_list:\n            sns.lineplot(\n                x=X_calc_graph, y=Y_calc_graph_df[elem],\n                color=color_dict[elem],\n                linewidth=linewidth,\n                legend=True,\n                label=label_legend_dict[elem],\n                ax=axes)\n\n    axes.set_xlim(Xmin, Xmax)\n    axes.set_ylim(Ymin, Ymax)\n    axes.set_xlabel(x_label, fontsize = label_fontsize)\n    axes.set_ylabel(y_label, fontsize = label_fontsize)\n    axes.tick_params(labelsize = tick_fontsize)\n    axes.legend(prop={'size': label_legend_fontsize})\n\n    plt.show()\n    \n    if file_name:\n        fig.savefig(file_name, orientation = \"portrait\", dpi = 300)\n    \n    # result output\n    output_list = [result_df, Y_calc_df, Y_calc_graph_df]\n    result_list = [result_table, value_table_calc, value_table_graph]\n    result = list()\n    for i, elem in enumerate(result_list):\n        if elem:\n            result.append(output_list[i])\n    \n    # result display\n    for elem in ['MSPE', 'MAPE']:\n        result_df[elem] = result_df[elem].apply(lambda s: float(s[:-1]))\n    b_formatter = [b0_formatter, b1_formatter, b2_formatter, b3_formatter]\n    if result_table:\n        display(result_df\n                .style\n                    .format(\n                        precision=DecPlace, na_rep='-',\n                        formatter={\n                            'b0': b0_formatter if b0_formatter else '{:.4f}',\n                            'b1': b1_formatter if b1_formatter else '{:.4f}',\n                            'b2': b2_formatter if b2_formatter else '{:.4e}',\n                            'b3': b3_formatter if b3_formatter else '{:.4e}'})\n                    .highlight_min(color='green', subset=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE'])\n                    .highlight_max(color='red', subset=['MSE', 'RMSE', 'MAE', 'MSPE', 'MAPE'])\n                    .highlight_max(color='green', subset='R2')\n                    .highlight_min(color='red', subset='R2')\n                    .applymap(lambda x: 'color: orange;' if abs(x) <= 10**(-(DecPlace-1)) else None, subset=parameters_df.columns))\n    \n    if value_table_calc:\n        display(Y_calc_df)\n    if value_table_graph:\n        display(Y_calc_graph_df)\n                    \n    return result\nВыполним аппроксимацию зависимости \nсреднемесячного расхода топлива автомобиля (л/100 км) (FuelFlow)\n от \nсреднемесячного пробега (км) (Mileage)\n:\ntitle_figure = Title_String + '\\n' + Task_Theme + '\\n'\ntitle_axes = 'Fuel mileage ratio'\n    \nmodels_list = ['linear', 'power', 'exponential', 'quadratic', 'qubic', 'logistic', 'logarithmic', 'hyperbolic']\n\np0_dict_in= {'exponential': [1, -0.1],\n             'power': [1, -0.1]}\n\n(result_df,\nvalue_table_calc,\nvalue_table_graph\n) = simple_approximation(\n    X1, Y,\n    models_list, p0_dict_in,\n    Xmin = X1_min_graph, Xmax = X1_max_graph,\n    Ymin = Y_min_graph, Ymax = Y_max_graph,\n    DecPlace=DecPlace,\n    result_table=True, value_table_calc=True, value_table_graph=True,\n    title_figure = title_figure,\n    title_axes = title_axes,\n    x_label=Variable_Name_X1,\n    y_label=Variable_Name_Y,\n    linewidth=2,\n    b1_formatter='{:.' + str(DecPlace + 3) + 'f}', b2_formatter='{:.' + str(DecPlace) + 'e}', b3_formatter='{:.' + str(DecPlace) + 'e}')\nВыполним аппроксимацию зависимости \nсреднемесячного расхода топлива автомобиля (л/100 км) (FuelFlow)\n от \nсрднемесячной температуры (Temperature)\n:\nmodels_list = ['linear', 'exponential', 'quadratic']\n\n(result_df,\nvalue_table_calc,\nvalue_table_graph\n) = simple_approximation(\n    X_sort_clear, Y_sort_clear,\n    models_list, p0_dict_in,\n    Xmin = X2_min_graph, Xmax = X2_max_graph,\n    Ymin = Y_min_graph, Ymax = Y_max_graph,\n    DecPlace=DecPlace,\n    result_table=True, value_table_calc=True, value_table_graph=True,\n    title_figure = title_figure,\n    title_axes = 'Dependence of FuelFlow on temperature',\n    x_label=Variable_Name_X2,\n    y_label=Variable_Name_Y,\n    linewidth=2,\n    b1_formatter='{:.' + str(DecPlace + 3) + 'f}', b2_formatter='{:.' + str(DecPlace) + 'e}', b3_formatter='{:.' + str(DecPlace) + 'e}')\nНебольшой offtop - сравнение результатов расчетов с использованием различного программного обеспечения\nНапоследок хочу привести данные сравнения результатов аппроксимации с помощью различных программных средств:\nPython\n, функция \nscipy.optimize.curve_fit\n;\nMS Excel\n;\nсистема компьютерной алгебры \nMaxima\n (которую я настоятельно рекомендую освоить всем специалистам, выполняющим математические расчеты).\nВсе файлы с расчетами доступны в моем репозитории на GitHub (\nhttps://github.com/AANazarov/MyModulePython\n).\nИтак:\nPython\n, функция \nscipy.optimize.curve_fit\n:\nМодель\nУравнение\nКоэффициент \nлинейная\n0.4226\nквадратическая\n0.6357\nкубическая\n0.6415\nстепенная\n0.6588\nэкспоненциальная I типа\n0.4857\nэкспоненциальная II типа\n0.4857\nлогарифмическая\n0.6625\nгиперболическая\n0.4618\nMS Excel\n:\nМодель\nУравнение\nКоэффициент \nлинейная\n0.4226\nквадратическая\n0.6359\nкубическая\n0.6417\nстепенная\n0.6582\nэкспоненциальная I типа\n0.4739\nэкспоненциальная II типа\n-\n-\nлогарифмическая\n0.6627\nгиперболическая\n-\n-\nMaxima:\nМодель\nУравнение\nКоэффициент \nлинейная\n0.4226\nквадратическая\n0.6357\nкубическая\n0.6415\nстепенная\n0.6588\nэкспоненциальная I типа\n0.4857\nэкспоненциальная II типа\n0.4857\nлогарифмическая\n0.6625\nгиперболическая\n0.4618\nКак видим, результаты имеют незначительные отличия для степенных и экспоненциальных зависимостей. Есть над чем задуматься...\nИТОГИ \nИтак, подведем итоги:\nмы рассмотрели основные инструменты \nPython\n для аппроксимации зависимостей;\nразобрали особенности определения ошибок и построения доверительных интервалов для параметров моделей аппроксимации;\nтакже предложен пользовательская функция для аппроксимации простых зависимостей (по аналогии с тем, как это реализовано в Excel), облегчающая работу исследователя и уменьшающая размер программного кода.\nСамо собой, мы рассмотрели в рамках данного обзора далеко не все вопросы, связанные с аппроксимацией; так отдельного рассмотрения, безусловно, заслуживают следующие вопросы:\nаппроксимация более сложных зависимостей (модифицированных кривых, S-образных кривых и пр.), требующих предварительной оценки начальных условий для алгоритма оптимизации;\nвлияние выбора алгоритма оптимизации на результат и быстройдействие;\nаппроксимация с ограничениями в виде интервалов (\nbounds\n) и в виде алгебраических выражений (\nconstraints\n);\nболее глубокое рассмотрение возможностей библиотеки \nlmfit\n для решения задач аппроксимации,\nи т.д.\nВ общем, есть куда двигаться дальше.\nИсходный код находится в моем репозитории на GitHub (\nhttps://github.com/AANazarov/Statistical-methods\n).\nНадеюсь, данный обзор поможет специалистам \nDataScience\n в работе.\n \n ",
    "tags": [
        "python",
        "регрессионный анализ",
        "аппроксимация",
        "leastsq",
        "least_squares",
        "curve_fit",
        "minimize",
        "lmfit"
    ]
}