{
    "article_id": "725282",
    "article_name": "Kandinsky 2.1, или Когда +0,1 значит очень много",
    "content": "В ноябре 2022 года мы выпустили свою первую диффузионную модель для синтеза изображений по текстовым описаниям \nKandinsky 2.0\n, которая собрала как позитивные, так и отрицательные отклики. Её ключевой особенностью была мультиязычность и использование двойного текстового энкодера на входе сети: XLMR-clip и mT5-small. Рефлексия после релиза подтолкнула нас к перестройке планов по развитию архитектуры и к сильному стремлению получить буст в качестве генераций, чтобы выйти на уровень аналогичных решений, названия которых слишком хорошо известны, чтобы их называть. В то же время мы могли наблюдать за появлением новых генеративных моделей и их файнтюнов, таких как \nControlNet\n, \nGigaGAN\n, \nGLIGEN\n, \nInstruct Pix2Pix\n и др. В этих работах представлены и новые взгляды на генерацию, и новые возможности использования латентного пространства для внесения контролируемых изменений через текстовые запросы, а также для смешивания изображений — возможности использования генеративных моделей расширяются постоянно. Бурное развитие прикладных кейсов привело к интенсивно нарастающему числу различных привлекательных для пользователей реализаций этих функций — визуализация городов, изображения известных личностей в нетипичных ситуациях и многие другие.\nПротестировать модель на своих запросах можно несколькими способами:  \nв \nTelegram‑боте\n (есть все доступные 4 режима генерации)\nна сайте \nfusionbrain.ai\n (доступна генерация по тексту и режимы inpainting/outpainting)\nна платформе \nMLSpace\n ​​в хабе предобученных моделей и датасетов DataHub\nв навыке Салют «Включи художника»\nна сайте \nrudalle.ru\nВ целях повышения качества генераций будущих пользователей мы также разрабатываем специальный промтбук, который позволит вам выжать из модели максимум :)\nАрхитектура и детали обучения\nРешение о внесении изменений в архитектуру пришло после продолжения обучения версии Kandinsky 2.0 и попыток получить устойчивые текстовые эмбеддинги мультиязычной языковой модели mT5. Закономерным выводом стало то, что использование только текстового эмбеддинга было недостаточно для качественного синтеза изображения. Проанализировав еще раз существующее решение DALL-E 2 от OpenAI было принято решение поэкспериментировать с image prior моделью (позволяет генерировать визуальный эмбеддинг \nCLIP\n по текстовому промту или текстовому эмбеддингу CLIP), одновременно оставаясь в парадигме латентного визуального пространства, чтобы не пришлось переобучать диффузионную часть UNet модели Kandinsky 2.0. Теперь чуть больше деталей про процесс обучения Kandinsky 2.1.\nНа первом этапе мы начали учить image prior модель для маппинга текстов и изображений, обучив отдельную диффузионную модель DiffusionMapping на текстовых и картиночных эмбеддингах CLIP. В качестве модели CLIP мы использовали предобученные веса \nmCLIP\n, а в основе DiffusionMapping лежит \nтрансформерная архитектура\n с параметрами:\nnum_layers=20\nnum_heads=32\nhidden_size=2048\nОбучив image prior модель (рисунок 1, слева), мы стали учить целевую модель синтеза изображения по текстовому описанию (рисунок 1, в центре). В этой реализации обученный image prior в виде модели DiffusionMapping использовался для синтеза по входному текстовому промту визуального эмбеддинга mCLIP, который далее применялся в процессе обучения диффузионной модели. Таким образом, мы обучали механизм обратной диффузии восстанавливать латентное представление изображения \nНЕ\n только из текстового эмбеддинга как в Kandinsky 2.0, а еще и из визуального эмбеддинга CLIP с condition на этот текстовый эмбеддинг. В паре с дополнительными чистыми данными такое архитектурное изменение привело к существенному росту качества генераций (генерации можно наблюдать в конце статьи). В качестве конечного блока модели выступает новый декодер, который из латентного представления позволяет получить финальное синтезированное изображение.\nСреди нескольких возможных способов использования генеративной модели мы особое внимание уделили возможности смешивания изображений (рисунок 1, справа). В отсутствии необходимости использовать текстовые эмбеддинги мы просто подаём в обученную диффузионную модель два визуальных эмбеддинга CLIP, и далее декодером восстанавливаем “смешанное” изображение.\nВ описании выше я умышленно не погрузился в детали нового декодера изображений, потому что этот этап заслуживает отдельного внимания, и  даже отдельной статьи, но кратко о том, почему мы ушли от использованного в прошлых версиях генеративных моделей VQGAN и стали использовать специально обученный для наших целей \nMoVQGAN\n модель, я расскажу в отдельном блоке ниже.\nРисунок 1. Обучение image prior модели (слева), архитектура text2image модели (в центре), механика смешивания изображений (справа).\nАрхитектура Kandinsky 2.1 содержит 3.3B параметров:\nText encoder (XLM-Roberta-Large-Vit-L-14) — 560M\nImage prior — 1B\nCLIP image encoder — 427M\nLatent Diffusion UNet — 1.22B\nMoVQ encoder/decoder — 67M\nДля того, чтобы каждый мог оценить качество новой модели Kandinsky 2.1 мы по традиции выкладываем веса в open source на следующих источниках:\nHuggingFace\nGitHub\nMLSpace\nДатасеты\nОбучение image prior модели выполнялось на датасете \nLAION Improved Aesthetics\n, а затем был выполнен файнтюн на данных \nLAION HighRes\n.\nОбучение основной Text2Image диффузионной модели выполнялось на основе 170M пар “текст-изображение” из датасета \nLAION HighRes\n (важным условием было наличие изображений с разрешением не меньше 768x768). Использование 170M пар для претрейна обусловлено тем, что мы сохранили диффузионный блок UNet от версии Kandinsky 2.0, что позволило не обучать его с нуля. Далее на этапе файнтюнинга применяли отдельно собранный из открытых источников датасет из 2М очень качественных изображений в высоком разрешении с описаниями (COYO, anime, landmarks_russia и ряд других).\nВизуальный автоэнкодер MoVQGAN\nВ ранних исследованиях (Kandinsky 1.0 и 2.0) мы использовали VQGAN автоэнкодер, который был специально дообучен под задачи синтеза изображений на таких сложных доменах, как тексты и лица людей. Это позволило добиться определённого успеха в части генераций изображений по сравнению с ванильной моделью VQGAN. Более подробно об экспериментах с VQGAN автоэнкодером мы писали в \nэтой\n статье.\nОчевидно, что весомо в эффектность генераций добавляет именно визуальный декодер, поэтому от его качества зависит очень много. Часто бывает так, что когда долго ищешь решение проблемы, оно приходит не сразу, но стоит немного отпустить мысль, как возникает какой-то новый путь. Получилось так, что на одном из регулярно проводимых в нашей команде ReadingClub мы делали обзор новой модели VQGAN - MoVQ: Modulating Quantized Vectors for High-Fidelity Image Generation [1]. Эта работа дала новую жизнь части модели, отвечающей за представление изображений в пространстве квантованных векторов.\nОсновное нововведение MoVQ заключается в добавлении слоя spatial conditional нормализации в блоки декодера, что позволило повысить реалистичность восстанавливаемых изображений. Сама идея spatial нормализации не новая и известна со времён StyleGAN и AdaIN слоёв, но авторы MoVQ применили её в пространстве квантованных представлений энкодера, что позволило избежать возникновения типичных повторяющихся артефактов, которые возникают из-за процесса квантования пространственно близких эмбеддингов в одинаковые индексы codebook. Использование spatial нормализации добавляет степеней свободы квантованным представлениям после энкодера и позволяет распространять по слоям декодера более вариативные представления эмбеддингов.\nПриведённые в [1] результаты сравнения с известными автоэнкодерами показывают качественное преимущество по всем известным метрикам (PSNR, SSIM, LPIPS и rFID) на двух датасетах: датасет лиц FFHQ и датасет ImageNet (рисунок 2). Отдельно хочется обратить внимание на тот факт, что размер codebook (\nNum Z\n) для модели MoVQ самый маленький и составляет всего 1024 вектора. Следует также отметить, что визуально качество MoVQ по изображениям оценить крайне трудно, но далее я приведу примеры декодированных изображений обученной нами модели на самых сложных доменах для автоэнкодеров: лица и текст.\nРисунок 2. Сравнение MoVQ с другими энкодерами [1]\nВпечатлившись результатами MoVQ, мы реализовали блок spatial нормализации в рамках нашей модели VQGAN, потому что технически эта операция была совершенно понятна и проста. Модель содержит всего 67M параметров, но результаты восстановления действительно не позволяют в большинстве случаев отличить декодированное изображение от исходного (groundtruth). Модель обучалась на 1 GPU в течение двух недель на данных \nLAION HighRes\n. Как и обещал, предлагаю оценить качество восстанавливаемых изображений на ярких примерах с декодированием лиц, текста и сложных сцен. В отдельной статье мы представим более подробное сравнение различных автоэнкодеров c нашей реализацией MoVQ.\nСравнение с другими Text2Image моделями\nДалеко не все современные решения делятся результатам качественной оценки своих моделей, ведь куда важнее пользовательский отклик - это лучшая оценка работы модели. Тем не менее, мы как исследовательская команда не можем себе позволить обойти стороной вычислительные эксперименты на уже всем известных датасетах для валидации генеративных моделей. С точки зрения метрики мы всё также используем уже ставшую золотым стандартом оценки \nFrechet Inception Distance (FID)\n, которая позволяет оценить близость двух вероятностных распределений (распределение оригинальных изображений и распределение синтезированных генеративной моделью изображений):\nгде \nr\n - оригинальные, а \ng\n - сгенерированные изображения.\nСравнение Kandinsky 2.1 и аналогичных решений мы проводили на датасете COCO_30k, который содержит 30 тыс. изображений с центральным кропом до 256x256 пикселей.. Измерять мы будем в режиме zero-shot, то есть датасет COCO не использовался при обучении модели. Полученные результаты (Таблица 1) позволяют утверждать, что Kandinsky 2.1 сделал большой качественный скачок относительно своих прошлых версий и вышел на уровень зарубежных аналогов, а некоторые из них даже превзошел. Среди известных решений модель Kandinsky 2.1 занимает уверенное третье место.\nТаблица 1. Значения метрики FID генеративных моделей на COCO_30k\n FID-30K\neDiff-I (2022)\n6,95\nImagen (2022)\n7,27\nKandinsky 2.1 (2023)\n💥\n8,21\nStable Diffusion 2.1 (2022)\n8,59\nGigaGAN, 512x512 (2023)\n9,09\nDALL-E 2 (2022)\n10,39\nGLIDE (2022)\n12,24\nKandinsky 1.0 (2022)\n15,40\nDALL-E (2021)\n17,89\nKandinsky 2.0 (2022)\n20,00\nGLIGEN (2022)\n21,04\nГенерации\nСинтез изображений по тексту\nСмешивание изображений\nВ столбце слева находится изображение “стиля”, которое мы хотим перенести на исходное изображение посредством диффузии.\nИсходное изображение\nСинтез изображений, похожих на референсное\nВ столбце слева находятся оригинальные изображения, а остальные изображения - являются его вариациями, синтезированными с помощью диффузионной модели.\nИзменение изображений по тексту\nСлева показаны исходные изображения, а справа текстовое описание и результат изменения исходной картинки по этому тексту.\nTelegram-бот\nОбновленная модель Kandinsky 2.1 доступна для использования в новом Telegram-боте, доступ к которому можно получить по \nссылке\n. В боте доступны 4 режима работы с моделью:\n Синтез изображений по тексту\n Смешивание изображений\n Синтез изображений, похожих на референсное\n Изменение изображений по тексту\nПри генерации изображений доступны 3 стиля:\n artstation — сформирован в ходе файнтюна на наборе изображений с сайта artstation.com \n 4k — сформирован в ходе файнтюна на наборе изображений высокого разрешения\n anime — сформирован в ходе файнтюна на наборе аниме‑изображений\nВыводы и планы\nВ итоге: что можно считать ключом к успеху? Вряд ли возможно выделить какое-то одно сделанное изменение из описанных мною выше, именно их совокупность как в рамках архитектуры, так и в части подготовки данных для претрейна/файнтюна позволила получить очень впечатляющий качественный прирост генераций, а также мы успели решить несколько прикладных задач на базе новой модели синтеза:\n смешивание изображений;\n изменение изображений на основе текстового описания;\n генерация изображений, похожих на заданное;\n inpainting/outpainting на изображениях.\nЧто же дальше? Мы продолжим исследовать возможности нового энкодера изображений, общей архитектуры и будем работать над усилением (увеличением) текстового энкодера, а также проведём эксперименты с генерацией изображений в разрешении 1024x1024 и выше. Всё это и многое другое мы имплементируем в следующих версиях модели Kandinsky :) \nСледите за новостями в каналах \nГрадиентное погружение\n, \nCompleteAI\n и \nAbstractDL\n. \nАвторы и контрибьют\nМодель  Kandinsky 2.1 была разработана и обучена исследователями команды Sber AI при партнёрской поддержке учёных из Института искусственного интеллекта AIRI на объединённом датасете Sber AI и компании SberDevices.\nКоллектив авторов: \nАрсений Шахматов\n (*), \nАнтон Разжигаев\n, \nВладимир Архипкин\n, \nАлександр Николич\n, \nИгорь Павлов, \nАнгелина Куц\n, \nАндрей Кузнецов\n, \nДенис Димитров\n.\n(*) - главный контрибьютор\nПолезные ссылки\nТелеграм-бот\nfusionbrain.ai\n \t\t\nrudalle.ru\nMLSpace\n[1] Zheng, Chuanxia, et al. \"Movq: Modulating quantized vectors for high-fidelity image generation.\" \nAdvances in Neural Information Processing Systems\n 35 (2022): 23412-23425.\n \n ",
    "tags": [
        "кандинский",
        "kandinsky",
        "kandinsky 2.1"
    ]
}