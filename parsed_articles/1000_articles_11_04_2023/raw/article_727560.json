{
    "article_id": "727560",
    "article_name": "Основные инструменты для работы в Data Engineering: введение для начинающих Data Engineer'ов",
    "content": "Всем привет!) \nМеня зовут Надя, я занимаю должность \nData Engineer\n в компании, которая специализируется на разработке мобильных игр. В этой статье я хочу поделиться информацией об основных инструментах, которые я использую в своей работе с данными, и рассказать о каждом из них подробнее.\nОтмечу, что в каждой компании инструменты могут отличаться, и мой опыт может не совпадать с опытом других \nData Engineer'ов\n. Однако, я постараюсь поделиться информацией, которая может быть полезна не только для профессионалов в этой области, но и для тех, кто только начинает свой путь в \nData Engineering\n. Буду рада, если этот материал поможет вам структурировать и закрепить свои знания.\nВ этой статье я расскажу о таких инструментах, как:\nPython\nPySpark\nAmazon S3\nApache Impala\nAmazon Redshift\nDatabricks\nApache Airflow\nGitHub, Git\nНиже я рассмотрю основные возможности каждого из инструментов и предоставлю ссылки на материалы, где вы сможете изучить их более подробно. Для более глубокого понимания и примеров использования каждого инструмента я также планирую написать отдельные статьи.\nPython\nНачну с самого базового. Насколько может быть известно, большинство специалистов при работе с данными используют в основном язык программирования такой как \nPython\n. Возникает вопрос - почему именно \nPython\n? Ответ очень прост, это связано с тем, что \nPython\n предоставляет множество инструментов для работы с данными и обработки информации. Вот несколько причин, почему \nPython\n так популярен в \nData Engineering\n:\nПростота:\n \nPython\n является простым и легко читаемым языком, что делает его идеальным выбором для работы с данными, особенно для начинающих. Многие высокоуровневые концепции, такие как функциональное программирование и объектно-ориентированное программирование, также легко изучаются в \nPython\n.\nБольшое сообщество:\n \nPython\n имеет огромное сообщество разработчиков, которые создают и поддерживают различные библиотеки и инструменты для обработки данных. Это значительно упрощает работу с данными и позволяет быстрее находить решения проблем.\nБиблиотеки:\n В \nPython\n существует множество библиотек для работы с данными, таких как \nPandas, NumPy, Scikit-Learn, Matplotlib\n и многие другие. Они предоставляют готовые решения для различных задач, таких как анализ данных, обработка и визуализация.\nИнструменты:\n \nPython\n также имеет различные инструменты, которые помогают при работе с данными. Например, \nJupyter Notebook\n позволяет создавать интерактивные записные книжки для анализа данных, а \nPySpark\n предоставляет возможность распределенной обработки больших объемов данных.\nКроме того, при написании кода важно учитывать паттерны проектирования и принципы ООП. Это помогает создавать более чистый и понятный код, а также облегчает поддержку и масштабирование приложения.\nШикарна книга, которую могу посоветовать - \n\"Python. К вершинам мастерства\"\n, автор - \nЛучано Рамальо.\nЕсли хотите пойти дальше и пощупать паттерны проектирования, то однозначно это книга \n\"Паттерны проектирования\"\n, авторы - \nЭрик Фримен, Элизабет Фримен и др.\nPySpark\nPython и Spark? Как такое возможно?\nА вот и да, в этом мире возможно все. \nPySpark\n - это фреймворк для обработки больших объемов данных на \nApache Spark\n, написанный на языке \nPython\n. Он широко используется в качестве инструмента для \nData Engineering\n, поскольку позволяет обрабатывать и анализировать данные в реальном времени, работать с данными из разных источников и проводить сложные аналитические вычисления.\nС помощью \nPySpark\n \nData Engineers\n могут работать с данными из разных источников, таких как \nHadoop Distributed File System (HDFS)\n, \nAmazon S3, Apache Cassandra, Apache HBase\n и других. \nPyspark\n позволяет объединять данные из разных источников в одном месте, проводить различные трансформации и обработки данных, а также агрегировать данные и проводить аналитические вычисления.\nОчень важно, что \nPySpark\n позволяет использовать распределенные вычисления для обработки больших объемов данных. Он позволяет работать с данными параллельно на нескольких узлах кластера, что значительно ускоряет обработку больших объемов данных и снижает время выполнения задач.\nAmazon S3\nНачнем с определения, что же такое Amazon S3 \nи с чем его едят\n?\nAmazon S3 (Simple Storage Service)\n – это хранилище объектов в облаке, предоставляемое компанией \nAmazon Web Services\n. \nS3\n предоставляет высокую доступность, масштабируемость, надежность и безопасность хранения данных. Он может быть использован для хранения любого типа данных, включая файлы, видео, аудио, изображения, документы и т.д.\nВ нашей команде для укладки данных в бакет \nS3\n используется следующий процесс - при игре пользователей в мобильные игры, все их действия, такие как нажатия на экран, покупки и другие события, собираются во внешней SDK и отправляются в наш бакет на \nS3\n в виде неструктурированных данных. Этот подход предоставляет нам удобство использования множества инструментов \nBig Data\n, таких как \nImpala\n, \nSpark\n, \nHive\n и других, для получения ценной информации из этих данных.\nНет ничего лучше \nофициальной документации\n для знакомства с \nS3\n и его настройки.\nApache Impala\nЧто же мы теперь делаем с дынными, хранящимся в S3?\n \nДля начала, мы можем использовать \nApache Hadoop\n, который позволяет распределенно обрабатывать и анализировать большие объемы данных. Однако, для работы с данными, хранящимися в S3, мы должны использовать дополнительный инструмент - \nAmazon S3A\n (\nS3A\n - это способ обращения к данным, хранящимся в \nS3\n, через \nHadoop\n).\nКогда мы устанавливаем связь между \nImpala\n и \nHadoop\n, \nImpala\n начинает работать как интерактивный \nSQL\n-запросовый движок, который может обращаться к данным, хранящимся в бакете \nS3\n. Для этого мы используем \nHadoop Distributed File System (HDFS)\n с помощью \nS3A\n, который предоставляет драйвер для чтения и записи данных из \nS3\n.\nImpala\n - это инструмент для выполнения быстрых SQL-запросов на больших объемах данных, которые хранятся в распределенной файловой системе \nHDFS\n. Он является интерактивным \nSQL\n-запросовым движком, который позволяет проводить анализ данных в режиме реального времени.\nТаким образом, используя \nImpala\n и \nHadoop\n вместе с \nS3A\n, мы можем получить доступ к большому объему неструктурированных данных, хранящихся в бакете \nS3\n, и производить анализ и обработку данных в режиме реального времени.\nИ всё-таки, чтобы уложить данные из \nS3\n бакета в \nImpala\n, сначала необходимо создать таблицу в \nImpala\n с указанием местонахождения данных в \nS3\n. Для этого можно использовать команду \nCREATE TABLE\n с указанием параметров, таких как формат файла, расположение данных в \nS3\n и т.д. Пример команды для создания таблицы в \nImpala\n и укладки в нее данных из \nS3\n:\nCREATE TABLE my_table\nSTORED AS PARQUET\nLOCATION 's3a://my_bucket/path/to/data';\nВ данном примере мы создаем таблицу с именем \nmy_table\n, указываем формат хранения данных в \nParquet\n, а также расположение данных в \nS3\n бакете \nmy_bucket\n по пути \npath/to/data\n. После создания таблицы, \nImpala\n сможет обращаться к данным в \nS3\n через \nHadoop\n с помощью \nS3A\n и проводить операции анализа и обработки данных в режиме реального времени.\nAmazon Redshift\nAmazon Redshift\n - это облачное решение для хранения и анализа данных, разработанное \nAmazon Web Services (AWS)\n. Оно представляет собой мощную колоночную базу данных, которая может обрабатывать большие объемы данных и поддерживает распределенную архитектуру.\nData Engineer-ы\n могут использовать \nAmazon Redshift\n для многих целей, таких как:\nХранение и обработка больших объемов данных:\n \nAmazon Redshift\n может обрабатывать терабайты данных и обеспечивает быстрый доступ к данным благодаря колоночной структуре.\nPySpark:\n \nAmazon Redshift\n может использоваться с \nPySpark\n, что дает возможность выполнять запросы на больших объемах данных, хранящихся в \nAmazon Redshift\n, используя распределенные вычисления.\nИспользование \nSQL\n для анализа данных:\n \nAmazon Redshift\n поддерживает \nSQL\n, что делает его более доступным для \nData Engineer\n, которые знакомы с \nSQL\n-запросами.\nИнтеграция с другими инструментами:\n \nAmazon Redshift\n интегрируется с другими сервисами \nAWS\n, такими как S3 и т.п., что позволяет \nData Engineer\n легко обрабатывать данные из разных источников.\nМасштабируемость:\n \nAmazon Redshift\n легко масштабируется в зависимости от объема данных, которые нужно хранить и обрабатывать.\nОптимизация:\n \nAmazon Redshift\n позволяет производить оптимизацию и анализ данных с помощью функций, таких как автоматическое распределение и сортировка данных, что ускоряет запросы и облегчает анализ больших объемов данных.\nШикарно все написано на \nэтом сайте\n, есть абсолютно все и даже больше\n, если хорошо искать.\nDatabricks\nDatabricks\n - это облачная платформа для обработки и анализа данных, которая позволяет работать с большими объемами данных и проводить исследования в режиме реального времени. Она основана на Apache Spark и предоставляет инструменты для работы с данными на \nPython\n, \nR\n, \nSQL\n и \nScala\n.\nОдним из \nпреимуществ\n \nDatabricks\n является его интеграция с \nPySpark\n. Это обеспечивает \nData Engineer\n возможность использовать \nPython\n для обработки и анализа данных в \nSpark\n.\nПеред началом работ в \nDatabricks \nнеобходимо понимать, что в нем для выполнения задач по обработке и анализу данных используется \nApache Spark\n, который запускается на кластере. \nКластер\n - это набор вычислительных ресурсов (виртуальных машин), на которых выполняются задачи. Кластер в \nDatabricks\n может быть настроен под различные требования, например, можно выбрать тип машин, количество вычислительных узлов, объем памяти и т.д.\nПри работе с кластером в \nDatabricks\n можно использовать \nSpark UI\n - веб-интерфейс, который позволяет отслеживать состояние выполнения задач, мониторить использование ресурсов и производительность кластера. \nSpark UI\n также позволяет просматривать подробную информацию о задачах и операциях, выполняемых в рамках кластера.\nДля написания и отладки \nPySpark\n-кода в \nDatabricks\n используется окружение \nnotebooks\n. \nNotebook\n - это интерактивный документ, который содержит код, результаты его выполнения, комментарии и графики (аналог уже знакомого \njupyer\n \nnotebook)\n. В \nDatabricks\n можно использовать как \nPython\n, так и \nSQL\n-код, а также интегрировать их в одном документе. \nИнтерфейс Databricks\nЕсли вы заинтересовались \nDatabricks\n и хотите глубже изучить его возможности, то рекомендуется обратиться к \nдокументации\n платформы.\nApache Airflow\nAirflow\n - это инструмент управления и планирования задач (\nworkflow management tool\n), который позволяет автоматизировать запуск и мониторинг задач и процессов в распределенных системах. \nData Engineer\n может использовать \nAirflow\n для автоматизации \nETL\n-процессов (извлечение, трансформация и загрузка данных). Например, для ежедневной загрузки данных из различных источников в хранилище данных. С помощью \nAirflow\n можно, например:\nУстановить расписание запуска задач\nНастроить оповещения о состоянии процесса\nАвтоматическое восстановление задач в случае ошибок\nОпределить зависимости между задачи\nAirflow\n имеет графический интерфейс пользователя, который позволяет легко настроить задачи и их зависимости, а также просматривать историю выполнения процесса и состояние каждой задачи.\nКонечно, и тут я порекомендую обратиться к \nофициальной документации\n :)\nGit и GitHub\nВ заключительной части статьи я хотела бы немного поделиться информацией о важности использования \nGit\n и \nGitHub\n для \nData Engineer\n и других разработчиков.\nВ нашей команде мы активно используем \nGitHub\n для управления и совместной работы над кодом. Это платформа для хранения, управления и совместной работы над исходным кодом. \nМы используем \nGit\n, систему контроля версий, для управления изменениями в нашем коде. С помощью \nGit\n мы можем сохранять изменения, вносить исправления и возвращаться к предыдущим версиям кода, если что-то идет не так. \nДавайте рассмотрим \nпример\n, чтобы понять необходимость рассматриваемых инструментов.\nДопустим, у нас есть команда \nData Engineer\n, которая работает над проектом по обработке больших данных. Каждый разработчик пишет свой код для обработки данных в отдельных файлах и сохраняет их в локальной папке. Чтобы совместно работать над проектом, они используют систему контроля версий \nGit\n и хранят все свои изменения и код в удаленном репозитории на \nGitHub\n. Таким образом, когда разработчики хотят работать над определенной частью проекта, они сначала получают последнюю версию кода из репозитория и создают новую ветку для своих изменений. Когда они закончили свою работу, они создают запрос на слияние \n(pull request)\n своей ветки с основной веткой проекта, чтобы другие разработчики могли просмотреть их изменения, оставить комментарии и решить, нужно ли объединять изменения в основной код проекта.\nНа YouTube или \nуже достала\n в \nофициальной документации \nможно найти всю нужную информацию для начала работы или для решения возникших вопросов. На вкус и цвет, как говорится :)\nБлагодарю за ознакомление с моей статьей, надеюсь, она оказалась для вас полезной. Если у вас возникли вопросы или вам нужна помощь в освоении какого-либо из инструментов, я буду рада помочь.\nМой ник в \ntelegram\n: \n@nadya8ch\n \n ",
    "tags": [
        "data en",
        "pyspark",
        "python",
        "data engineering",
        "airflow",
        "databricks",
        "redshift",
        "impala",
        "s3",
        "шаблоны проектирования"
    ]
}