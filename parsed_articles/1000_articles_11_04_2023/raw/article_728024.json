{
    "article_id": "728024",
    "article_name": "ЛЛаМы на Эльбрусе",
    "content": "Всем привет! Возможно, вы уже знаете о проекте \nLLaMA.cpp\n (на Хабре по этой теме был цикл новостей от \n@bugman\n). В самом репозитории никаких ссылок на модели не даётся, только указание на необходимые ресурсы. И ресурсы, надо сказать, немалые.\nДля самой крупной модели (65B) указано требование ~40Гб оперативной памяти. Конечно, на компьютерах разработчиков встретить такой объем вполне возможно. Но далеко не у всех. Столкнувшись с проблемой нехватки памяти «здесь и сейчас», я вспомнил про уникальное по своей щедрости предложение от кого‑то, связанного с Эльбрусами (я не знаю кто ты, но я найду тебя и... поблагодарю). \nНа ресурсе \nhttps://elbrus.kurisa.ch/\n можно запросить демо‑доступ к достаточно мощным серверам на Эльбрусе. Не знаю, какая на момент публикации очередь доступа, но доступ по SSH у меня уже был, поэтому я приступил к беспощадной эксплуатации.\nРадует, что на сервере уже установлены инструменты make, поэтому собрать исполнительный файл \nllama.cpp/main\n не составило труда. Сконвертированные модели к этому времени скачались в \n/srv/home/kpmy/dev/llama.cpp/models\n (можно использовать их в ваших экспериментах, не скачивая заново). \nПо рекомендациям автора llama.cpp, запускаем приложение в режиме чата: \nmake\n./main -m models/65B/ggml-model-q4_0.bin -n 256 --repeat_penalty 1.0 --color -i -r \"User:\" -p \"User: Tell me about Elbrus.\"\nВ целом, это и есть вся инструкция. Так как исполнитель написан на Си, ему особо ничего не нужно, кроме всех ядер шикарной серверной машины (предупреждений об ограничении на использование я не увидел, если что, ребята, извините).\nВ последней версии llama.cpp механизм работы с моделью был слегка изменён, теперь там mmap‑ится файл вместо полной загрузки в память, это ускорило старт программы.\nНа скриншоте видно нагрузку и пример работы в режиме «чата», правда, она нестабильная какая‑то, иногда начинает генерировать фразы за пользователя.\nПо производительности что‑то трудно сказать, так как «кажется», что на AMD Ryzen 5600G c 64Гб памяти под Windows эта модель крутится примерно с той же скоростью (это я узнал позже, когда раздобыл необходимый объем памяти). Но проверить это «аккуратнее» мне не так интересно, как выдумывать новые промпты к ИИ‑игрушке для «бедных».\nВозможно, ребятам из Эльбрус будет интересно подкинуть свои патчи к исходной программе и как‑то повысить производительность. В любом случае, огромное спасибо им за бесплатные ресурсы, всё уже очень неплохо работает. Может быть, процессорные нейросетки это перспективное направление для данной архитектуры, специалисты тут могут подсказать. \nКстати, если кто‑то знает, можно ли \nпохожим на llama.cpp\n простым средством генерировать картинки без GPU, пусть и за счёт потраченного процессорного времени, подсказывайте в комментариях.\n \n ",
    "tags": [
        "elbrus",
        "llama",
        "chat"
    ]
}