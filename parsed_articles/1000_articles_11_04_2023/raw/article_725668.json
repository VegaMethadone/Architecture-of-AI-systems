{
    "article_id": "725668",
    "article_name": "Как создать свою собственную нейронную сеть с нуля на Python",
    "content": "Как создать свою собственную нейронную сеть с нуля на Python\nМотивация\n: в рамках моего личного пути к лучшему пониманию глубокого обучения я решил создать нейронную сеть с нуля без библиотеки глубокого обучения, такой как TensorFlow. Я считаю, что понимание внутренней работы нейронной сети важно для любого начинающего специалиста по данным. Эта статья содержит то, что я узнал, и, надеюсь, она будет полезна и вам!\nЧто такое нейронная сеть?  \nВ большинстве вводных текстов по нейронным сетям при их описании используются аналогии с мозгом. Не углубляясь в аналогии с мозгом, я считаю, что проще описать нейронные сети как математическую функцию, которая отображает заданный вход в желаемый результат.\n Нейронные сети состоят из следующих компонентов:\nВходной слой, x\nПроизвольное количество скрытых слоев\nВыходной слой, y\nНабор весов и смещений между каждым слоем, W и b\nВыбор функции активации для каждого скрытого слоя, σ. В этом уроке мы будем использовать функцию активации\nНа приведенной ниже диаграмме показана архитектура двухуровневой нейронной сети (обратите внимание, что входной слой обычно исключается при подсчете количества слоев в нейронной сети).\nСоздать класс нейронной сети в Python очень просто.\nclass NeuralNetwork:\n    def __init__(self, x, y):\n        self.input = x\n        self.weights1 = np.random.rand(self.input.shape[1],4) \n        self.weights2 = np.random.rand(4,1)                 \n        self.y = y\n        self.output = np.zeros(y.shape)\nОбучение нейронной сети.\nВыход y простой двухслойной нейронной сети:\nВы могли заметить, что в приведенном выше уравнении веса W и смещения b являются единственными переменными, влияющими на выход y.\nЕстественно, правильные значения весов и смещений определяют силу прогнозов. Процесс точной настройки весов и смещений на основе входных данных известен как обучение нейронной сети.\nКаждая итерация процесса обучения состоит из следующих шагов:\nРасчет прогнозируемого выхода y, известный как \nпрямая связь\n.\nОбновление весов и смещений, известное как \nобратное распространение\n.\nПоследовательный график ниже иллюстрирует процесс.\nПрямая связь\n Как мы видели на последовательном графике выше, упреждающая связь — это просто простое исчисление, и для базовой двухслойной нейронной сети выходные данные нейронной сети таковы:\nДавайте добавим функцию прямой связи в наш код Python, чтобы сделать именно это. Обратите внимание, что для простоты мы приняли смещения равными 0.\nclass NeuralNetwork:\n    def __init__(self, x, y):\n        self.input = x\n        self.weights1 = np.random.rand(self.input.shape[1],4) \n        self.weights2 = np.random.rand(4,1)                 \n        self.y = y\n        self.output = np.zeros(self.y.shape)\n\n    def feedforward(self):\n        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n        self.output = sigmoid(np.dot(self.layer1, self.weights2))\nОднако нам по-прежнему нужен способ оценить «хорошесть» наших прогнозов (т. е. насколько далеки наши прогнозы)? Функция потерь позволяет нам сделать именно это.\nФункция потери\nЕсть много доступных функций потерь, и природа нашей проблемы должна диктовать наш выбор функции потерь. В этом уроке мы будем использовать простую ошибку суммы квадратов в качестве функции потерь.\nТо есть ошибка суммы квадратов представляет собой просто сумму разницы между каждым предсказанным значением и фактическим значением. Разница возводится в квадрат, так что мы измеряем абсолютное значение разницы.\nНаша цель в обучении — найти наилучший набор весов и смещений, который минимизирует функцию потерь.\nОбратное распространение\nТеперь, когда мы измерили ошибку нашего прогноза (потери), нам нужно найти способ распространить ошибку обратно и обновить наши веса и смещения.\nЧтобы узнать подходящую величину для корректировки весов и смещений, нам нужно знать производную функции потерь по отношению к весам и смещениям. \nВспомним из исчисления, что производная функции — это просто наклон функции.\nЕсли у нас есть производная, мы можем просто обновить веса и смещения, увеличивая/уменьшая ее (см. диаграмму выше). \nЭто известно как градиентный спуск. Однако мы не можем напрямую вычислить производную функции потерь по весам и смещениям, потому что уравнение функции потерь не содержит весов и смещений. Поэтому нам нужно цепное правило, чтобы помочь нам вычислить его.\nФу! Это было некрасиво, но позволяет нам получить то, что нам нужно — производную (наклон) функции потерь по весам, чтобы мы могли соответствующим образом скорректировать веса. Теперь, когда у нас это есть, давайте добавим функцию обратного распространения в наш код Python.\nclass NeuralNetwork:\n    def __init__(self, x, y):\n        self.input = x\n        self.weights1 = np.random.rand(self.input.shape[1],4) \n        self.weights2 = np.random.rand(4,1)                 \n        self.y = y\n        self.output = np.zeros(self.y.shape)\n\n    def feedforward(self):\n        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n\n    def backprop(self):\n        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * sigmoid_derivative(self.output)))\n        d_weights1 = np.dot(self.input.T,  (np.dot(2*(self.y - self.output) * sigmoid_derivative(self.output), self.weights2.T) * sigmoid_derivative(self.layer1)))\n\n        # update the weights with the derivative (slope) of the loss function\n        self.weights1 += d_weights1\n        self.weights2 += d_weights2\nСобираем все вместе\nТеперь, когда у нас есть полный код Python для прямого и обратного распространения, давайте применим нашу нейронную сеть на примере и посмотрим, насколько хорошо она работает.\nНаша нейронная сеть должна изучить идеальный набор весов для представления этой функции. Обратите внимание, что для нас не совсем тривиально определить веса только путем проверки.\nДавайте обучим нейронную сеть на 1500 итераций и посмотрим, что получится. Глядя на приведенный ниже график потерь на итерацию, мы ясно видим, что \nпотери монотонно уменьшаются к минимуму\n. Это согласуется с алгоритмом градиентного спуска, который мы обсуждали ранее.\nДавайте посмотрим на окончательный прогноз (выход) нейронной сети после 1500 итераций.\nМы сделали это! Наш алгоритм прямого и обратного распространения успешно обучил нейронную сеть, и прогнозы сошлись на истинных значениях. \nОбратите внимание, что есть небольшая разница между прогнозами и фактическими значениями. Это желательно, поскольку предотвращает переоснащение и позволяет нейронной сети лучше обобщать невидимые данные.\nЧто дальше?\nК счастью для нас, наше путешествие не закончено. Нам еще многое предстоит узнать о нейронных сетях и глубоком обучении. \nНапример:\nКакую еще функцию активации мы можем использовать, кроме сигмовидной? \nИспользование скорости обучения при обучении нейронной сети.\nИспользование сверток для задач классификации изображений.\nПоследние мысли\nЯ определенно многому научился, написав свою собственную нейронную сеть с нуля.\nХотя библиотеки глубокого обучения, такие как TensorFlow и Keras, упрощают создание глубоких сетей без полного понимания внутренней работы нейронной сети, я считаю, что начинающим специалистам по данным полезно получить более глубокое понимание нейронных сетей. Это упражнение было отличным вложением моего времени, и я надеюсь, что оно будет полезным и для вас!\n \n ",
    "tags": [
        "python",
        "нейросеть",
        "нейросети",
        "python3"
    ]
}