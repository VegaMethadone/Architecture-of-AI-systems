{
    "article_id": "727404",
    "article_name": "Книга «Машинное обучение. Портфолио реальных проектов»",
    "content": " Привет, Хаброжители!\n\r\n\n\r\nИзучите ключевые концепции машинного обучения‚ работая над реальными проектами! Машинное обучение — то, что поможет вам в анализе поведения клиентов, прогнозировании тенденций движения цен, оценке рисков и многом другом. Чтобы освоить машинное обучение, вам нужны отличные примеры, четкие объяснения и много практики. В книге все это есть!\n\r\n\n\r\nАвтор описывает реалистичные, практичные сценарии машинного обучения, а также предельно понятно раскрывает ключевые концепции. Вы разберете интересные проекты, такие как сервис прогнозирования цен на автомобили с использованием линейной регрессии и сервис прогнозирования оттока клиентов. Вы выйдете за рамки алгоритмов и изучите важные техники, например развертывание приложений в бессерверных системах и запуск моделей с помощью Kubernetes и Kubeflow. Пришло время закатать рукава и прокачать свои навыки в области машинного обучения!\n\r\n\n\n                        \nКому адресована книга\n\n                        \nЭта книга написана для людей, которые умеют программировать и могут быстро освоить основы Python. Вам не понадобится предварительный опыт работы с машинным обучением.\n\r\n\n\r\nИдеальный читатель — инженер-программист, который хотел бы начать работать с машинным обучением. Однако мотивированному студенту вуза, которому нужно писать код для учебы и сторонних проектов, книга, несомненно, пригодится.\n\r\n\n\r\nКроме того, книга будет полезна и людям, которые уже работают с машинным обучением, но хотят узнать больше. Многие из тех, кто уже работает в качестве специалистов по обработке данных и аналитиков данных, сказали, что она оказалась полезной для них, особенно главы о развертывании.\n\n                    \n\r\n\nМашинное обучение для регрессии\n\r\nВыполнив первоначальный анализ данных, мы готовы обучать модель. Проблема, которую мы решаем, — это задача регрессии: цель состоит в том, чтобы предсказать число‚ а именно цену автомобиля. Для этого проекта мы будем использовать простейшую регрессионную модель: линейную регрессию.\n\r\n\n\r\n\n2.3.1. Линейная регрессия\n\r\nЧтобы спрогнозировать цену автомобиля, нам нужно использовать какую-то модель машинного обучения. В нашем случае мы будем использовать линейную регрессию, которую реализуем самостоятельно. Как правило, вручную это не делается‚ и подобную работу выполняет какая-либо библиотека. Однако в этой главе мы хотим показать, что внутри таких фреймворков нет ничего волшебного: это просто код. Линейная регрессия — идеальная модель, поскольку она относительно проста и может быть реализована с помощью всего нескольких строк кода NumPy.\n\r\n\n\r\nДля начала разберемся, как работает линейная регрессия. Как мы знаем из главы 1, модель контролируемого машинного обучения имеет определенную форму:\n\r\n\n\r\ny ≈ g(X).\n\r\n\n\r\nЭто матричная форма. X — матрица, где признаки наблюдений являются строками матрицы, а y — вектор со значениями, которые требуется спрогнозировать.\n\r\n\n\r\nЭти матрицы и векторы могут запутывать, так что сделаем шаг назад и рассмотрим, что происходит с одним наблюдением x\ni\n и значением y\ni\n, которое мы хотим спрогнозировать. Индекс i здесь означает, что это номер наблюдения i, одно из m наблюдений, которые содержатся в нашем обучающем наборе данных.\n\r\n\n\r\nТогда для этого единственного наблюдения предыдущая формула выглядит следующим образом:\n\r\n\n\r\n\n\r\nЕсли у нас есть n признаков, то наш вектор x\ni\n является n-мерным, поэтому он содержит n компонентов:\n\r\n\n\r\n\n\r\nПоскольку он имеет n компонентов, мы можем записать функцию g как функцию с n параметрами, что совпадает с предыдущей формулой:\n\r\n\n\r\n\n\r\nВ нашем случае у нас в обучающем наборе данных 7150 автомобилей. Это означает, что m = 7150, а i может быть любым числом от 0 до 7149. Для i = 10, например, мы получим такой автомобиль:\n\r\n\n\r\n\nmake                                  rolls-royce\nmodel                      phantom_drophead_coupe\nyear                                         2015\nengine_fuel_type      premium_unleaded_(required)\nengine_hp                                     453\nengine_cylinders                               12\ntransmission_type                       automatic\ndriven_wheels                    rear_wheel_drive\nnumber_of_doors                                 2\nmarket_category         exotic,luxury,performance\nvehicle_size                                large\nvehicle_style                         convertible\nhighway_mpg                                    19\ncity_mpg                                       11\npopularity                                     86\nmsrp                                       479775\n\r\nВыберем несколько числовых признаков и пока проигнорируем остальные. Мы можем начать с лошадиных сил, миль на галлон по городу и популярности:\n\r\n\n\r\n\nengine_hp          453\ncity_mpg            11\npopularity          86\n\r\nЗатем присвоим эти признаки x\ni1\n, x\ni2\n и x\ni3\n соответственно. Таким образом, мы получаем вектор признаков xi с тремя компонентами:\n\r\n\n\r\n\n\r\nЧтобы лучше все понять описываемое, мы можем перевести эту математическую нотацию на Python. В нашем случае функция g имеет следующую сигнатуру:\n\r\n\n\r\n\ndef g(xi):\n    # xi — это список из n элементов\n    # делаем что-нибудь с xi\n    # вернуть результат\n    pass\n\r\nВ этом коде переменная xi — это наш вектор xi. В зависимости от реализации xi может быть списком с n элементами или массивом NumPy, имеющим размер n.\n\r\n\n\r\nДля автомобиля, описанного ранее, xi представляет собой список из трех элементов:\n\r\n\n\r\n\nxi = [453, 11, 86]\n\r\nКогда мы применяем функцию g к вектору xi, она выдает y_pred в качестве вывода, который является прогнозом g для xi:\n\r\n\n\r\n\ny_pred = g(xi)\n\r\nМы ожидаем, что прогноз окажется как можно ближе к yi, реальной цене автомобиля.\n\r\n\nПРИМЕЧАНИЕ\n\r\n\n\r\nЧтобы проиллюстрировать идеи, лежащие в основе математических формул, в этом разделе мы будем использовать Python. Нам не требуется использовать эти фрагменты кода для самого проекта. С другой стороны, запуск данного кода в Jupyter может помочь понять концепции.\nФункция g может выглядеть по-разному, и выбор алгоритма машинного обучения определяет способ ее работы.\n\r\n\n\r\nЕсли g — модель линейной регрессии, то она получит следующий вид:\n\r\n\n\r\n\n\r\nПеременные w\n0\n, w\n1\n, w\n2\n… wn служат параметрами модели:\n\r\n• w\n0\n — составляющая смещения;\n\r\n• w\n1\n, w\n2\n… w\nn\n — веса каждого признака x\ni1\n, x\ni2\n… x\nin\n.\n\r\n\n\r\nЭти параметры точно определяют, как модель должна комбинировать признаки, чтобы прогнозы в итоге получились максимально хорошими. Ничего страшного, если значение этих параметров пока не до конца ясно, поскольку мы рассмотрим их несколько позже.\n\r\n\n\r\nЧтобы формула была короче, используем обозначение суммы:\n\r\n\n\r\n\nУпражнение 2.2\n\r\n\n\r\nДля контролируемого обучения мы используем модель машинного обучения для единственного наблюдения y\ni\n ≈ g(x\ni\n). Что такое x\ni\n и y\ni\n в этом проекте?\n\r\n\n\r\nА. x\ni\n — вектор признаков, содержащий ряд чисел, описывающих объект (автомобиль), а y\ni\n — логарифм цены этого автомобиля.\n\r\n\n\r\nБ. y\ni\n — вектор признаков‚ содержащий ряд чисел, описывающих объект (автомобиль), а x\ni\n — логарифм цены этого автомобиля.\nУказанные веса — это то, что модель усваивает, когда мы ее обучаем. Чтобы лучше понять, как модель использует веса, рассмотрим следующие значения (табл. 2.1).\n\r\n\n\r\n\n\r\nИтак, если мы захотим перевести эту модель на Python, то она будет выглядеть следующим образом:\n\r\n\n\r\n\nw0 = 7.17\n#   [w1    w2    w3   ]\nw = [0.01, 0.04, 0.002]\nn = 3\n\ndef linear_regression(xi):\n    result = w0\n    for j in range(n):\n        result = result + xi[j] * w[j]\n    return result\n\r\nМы помещаем все веса объектов в один список w — точно так же, как мы ранее поступили с xi. Все, что нам теперь нужно сделать, — перебрать эти веса и умножить их на соответствующие значения признаков. Это и будет не чем иным, как прямым переводом предыдущей формулы на Python.\n\r\n\n\r\nРазобраться в этом достаточно легко. Взгляните еще раз на формулу:\n\r\n\n\r\n\n\r\nВ нашем примере имеются три признака, поэтому n = 3, и мы получаем\n\r\n\n\r\n\n\r\nЭто именно то, что мы видим в коде\n\r\n\n\r\n\nresult = w0 + xi[0] * w[0] + xi[1] * w[1] + xi[2] * w[2]\n\r\nза простым исключением, что индексация в Python начинается с 0, xi1 превращается в xi[0]‚ а w\n1\n — в w[0].\n\r\n\n\r\nТеперь посмотрим, что произойдет, когда мы применим модель к нашему наблюдению xi и заменим веса их значениями:\n\r\n\n\r\n\n\r\nПрогноз, который мы получим для этого наблюдения, будет равен 12,31. Вспомните, что во время предварительной обработки мы применили к нашей целевой переменной y логарифмическое преобразование. Вот почему модель, которую мы обучили на этих данных, также предсказывает логарифм цены. Чтобы отменить преобразование, нам нужно взять экспоненту логарифма. В нашем случае прогноз становится равным 603 000 долларов:\n\r\n\n\r\nexp(12,31 + 1) = 603 000.\n\r\n\n\r\nСмещение (7,17) — это значение, которое мы бы получили, если бы ничего не знали об автомобиле; оно служит базовой линией.\n\r\n\n\r\nОднако мы кое-что знаем об этом автомобиле: мощность, миль на галлон по городу (MPG) и популярность. Это признаки x\ni1\n, x\ni2\n и x\ni3\n, каждый из которых что-то говорит нам об автомобиле. Мы используем данную информацию для корректировки базовой линии.\n\r\n\n\r\nРассмотрим первый признак: лошадиные силы. Вес для этого признака равен 0,01, это значит, для каждой дополнительной единицы лошадиной силы мы корректируем базовую линию, добавляя 0,01. Поскольку у нас в двигателе 453 лошади, мы добавляем 4,53 к базовому показателю: 453 л/c · 0,01 = 4,53.\n\r\n\n\r\nТо же самое происходит и с MPG. Каждая дополнительная миля на галлон увеличивает цену на 0,04, поэтому мы добавляем 0,44: 11 м/г · 0,04 = 0,44.\n\r\n\n\r\nНаконец мы принимаем во внимание популярность. В нашем примере каждое упоминание в ленте Twitter приводит к увеличению на 0,002. В общей сложности популярность вносит 0,172 в окончательный прогноз.\n\r\n\n\r\nИменно поэтому мы получаем 12,31, когда сводим все воедино (рис. 2.11).\n\r\n\n\r\n\n\r\nТеперь вспомним, что на самом деле мы имеем дело с векторами, а не с отдельными числами. Мы знаем, что x\ni\n — это вектор с n компонентами:\n\r\n\n\r\n\n\r\nМы также можем объединить все веса в один вектор w:\n\r\n\n\r\n\n\r\nФактически мы уже делали это в примере Python, когда помещали все веса в список, который представлял собой вектор размерности 3 с весами для каждого отдельного признака. Вот как выглядят векторы в нашем примере:\n\r\n\n\r\n\n\r\nПоскольку теперь мы думаем о признаках и весах как о векторах x\ni\n и w соответственно, мы можем заменить сумму элементов этих векторов их скалярным произведением:\n\r\n\n\r\n\n\r\nСкалярное произведение — это способ умножения двух векторов: мы умножаем соответствующие элементы векторов, после чего суммируем результаты. В приложении В можно найти более подробную информацию об умножении вектора на вектор.\n\r\n\n\r\nПеревод формулы для скалярного произведения в код прост:\n\r\n\n\r\n\ndef dot(xi, w):\n    n = len(w)\n    result = 0.0\n    for j in range(n):\n    result = result + xi[j] * w[j]\nreturn result\n\r\nИспользуя новую нотацию, мы можем переписать все уравнение для линейной регрессии как\n\r\n\n\r\n\n\r\nгде\n\r\n\n\r\n• w\n0\n — компонент смещения;\n\r\n• w — n-мерный вектор весов.\n\r\n\n\r\nТеперь мы можем использовать новую функцию dot, поэтому функция линейной регрессии в Python становится очень короткой:\n\r\n\n\r\n\ndef linear_regression(xi):\n    return w0 + dot(xi, w)\n\r\nВ качестве альтернативы, если xi и w являются массивами NumPy, мы можем использовать для умножения встроенный метод dot:\n\r\n\n\r\n\ndef linear_regression(xi):\n    return w0 + xi.dot(w)\n\r\nЧтобы сделать его еще короче, мы можем объединить w0 и w в один (n + 1)-мерный вектор, добавив w\n0\n к w прямо перед w\n1\n:\n\r\n\n\r\n\n\r\nТаким образом, мы получаем вектор весов w, состоящий из компонента смещения w\n0\n, за которым следуют веса w\n1\n, w\n2\n,… из исходного вектора весов w.\n\r\n\n\r\nВ Python это проделать очень легко. Если у нас уже есть старые веса в списке w, то нам нужно лишь выполнить следующую операцию:\n\r\n\n\r\n\nw = [w0] + w\n\r\nПомните, что оператор + в Python объединяет списки, поэтому [1] + [2, 3, 4] создаст новый список из четырех элементов: [1, 2, 3, 4]. В нашем случае w уже является списком, поэтому мы создаем новый w с одним дополнительным элементом в начале: w0.\n\r\n\n\r\nПоскольку теперь w становится (n + 1)-мерным вектором, нам также нужно настроить вектор объектов x\ni\n так, чтобы скалярное произведение по-прежнему работало. Это легко сделать, добавив фиктивный признак x\ni0\n, который всегда принимает значение 1. Затем мы добавим этот новый фиктивный признак к x\ni\n прямо перед x\ni1\n:\n\r\n\n\r\n\n\r\nИли в коде:\n\r\n\n\r\n\nxi = [1] + xi\n\r\nМы создаем новый список xi с 1 в качестве первого элемента, за которым следуют все элементы из старого списка xi.\n\r\n\n\r\nС помощью этих модификаций мы можем выразить модель как скалярное произведение между новым x\ni\n и новым w:\n\r\n\n\r\n\n\r\nВ коде это выразить просто:\n\r\n\n\r\n\nw0 = 7,17\nw = [0.01, 0.04, 0.002]\nw = [w0] + w\n\ndef linear_regression(xi):\n    xi = [1] + xi\n    return dot(xi, w)\n\r\nЭти формулы для линейных регрессий эквивалентны, поскольку первый признак нового x\ni\n равен 1, поэтому, умножая первый компонент x\ni\n на первый компонент w, мы получаем компонент смещения, поскольку w\n0\n × 1 = w\n0\n.\n\r\n\n\r\nТеперь мы готовы вернуться к общей картине и поговорить о матричной форме. В данных содержится много наблюдений, и x\ni\n — одно из них. Таким образом, у нас есть m векторов признаков x\n1\n, x\n2\n, ..., x\ni\n, ..., x\nm\n, и каждый из этих векторов состоит из n + 1 признаков:\n\r\n\n\r\n\n\r\nМы можем сложить эти векторы вместе в виде строк матрицы. Назовем эту матрицу X (рис. 2.12).\n\r\n\n\r\n\n\r\nПосмотрим, как это выглядит в коде. Мы можем взять несколько строк из обучающего набора данных, например первую, вторую и десятую:\n\r\n\n\r\n\nx1  = [1, 148, 24, 1385]\nx2  = [1, 132, 25, 2031]\nx10 = [1, 453, 11, 86]\n\r\nТеперь объединим строки в другой список:\n\r\n\n\r\n\nX = [x1, x2, x10]\n\r\nСписок X теперь содержит три списка. Мы можем думать об этом как о матрице 3 × 4 — матрице с тремя строками и четырьмя столбцами:\n\r\n\n\r\n\nX = [[1, 148, 24, 1385],\n     [1, 132, 25, 2031],\n     [1, 453, 11, 86]]\n\r\nКаждый столбец этой матрицы представляет собой признак:\n\r\n1) первый столбец — фиктивный признак с «1»;\n\r\n2) второй столбец — мощность двигателя;\n\r\n3) третий — MPG в городе;\n\r\n4) и последний — популярность, или количество упоминаний в Twitter.\n\r\n\n\r\nВы уже знаете, что, для того чтобы сделать прогноз для одного вектора признаков, нам нужно вычислить скалярное произведение этого вектора признаков и вектора весов. Теперь у нас есть матрица X, которая в Python представляет собой список векторов признаков. Чтобы сделать прогнозы для всех строк матрицы, мы можем просто перебрать все строки X и вычислить скалярное произведение:\n\r\n\n\r\n\npredictions = []\n\nfor xi in X:\n    pred = dot(xi, w)\n    predictions.append(pred)\n\r\nВ линейной алгебре это умножение матрицы на вектор: мы умножаем матрицу X на вектор w. Формула для линейной регрессии превращается в \n\r\n\n\r\n\n\r\nРезультатом будет массив с прогнозами для каждой строки X. Более подробную информацию о матрично-векторном умножении можно найти в приложении В.\n\r\n\n\r\nПри такой формулировке матрицы код для применения линейной регрессии для составления прогнозов становится очень простым, как и перевод на NumPy:\n\r\n\n\r\n\npredictions = X.dot(w)\n\r\n\nУпражнение 2.3\n\r\n\n\r\nКогда мы умножаем матрицу X на вектор весов w, что мы получаем?\n\r\nА. Вектор y с фактической ценой.\n\r\nБ. Вектор y с прогнозами цен.\n\r\nВ. Одно число y с прогнозами цен.\n\r\n\n2.3.2. Обучающая модель линейной регрессии\n\r\nДо сих пор мы рассматривали только прогнозирование. Чтобы иметь возможность это сделать, нам нужно знать веса w. Как мы их получим?\n\r\n\n\r\nМы узнаем веса из данных: используем целевую переменную y, чтобы найти такую w, которая наилучшим образом сочетает в себе признаки X. «Наилучшим образом» в случае линейной регрессии означает, что ошибка между прогнозами g(X) и фактическим целевым значением y сведена к минимуму.\n\r\n\n\r\nУ нас для этого есть несколько способов. Мы используем нормальное уравнение, которое служит самым простым методом реализации. Весовой вектор w может быть вычислен по следующей формуле:\n\r\n\n\r\n\n\r\n\nПРИМЕЧАНИЕ\n\r\n\n\r\nВыведение нормального уравнения выходит за рамки этой книги. Мы даем некоторое представление о том, как оно работает, в приложении В, но более подробно познакомиться с темой вам поможет учебник по машинному обучению. Книга The Elements of Statistical Learning, 2nd edition Г. Фридмана, Р. Тибширани и Т. Хасти (Friedman, Tibshirani, Hastie) будет весьма полезной на начальном этапе.\nЭта математическая часть может напугать или запутать, но ее довольно легко перевести на NumPy:\n\r\n\n\r\n• X\nT\n — транспонирование X. В NumPy это X.T;\n\r\n\n\r\n• X\nT\nX — умножение матрицы на матрицу, которое мы можем выполнить с помощью метода dot из NumPy: X.T.dot(X);\n\r\n\n\r\n• X\n–1\n — величина, обратная X. Для обращения мы можем использовать функцию np.linalg.inv.\n\r\n\n\r\nТаким образом, приведенная выше формула превращается непосредственно в\n\r\n\n\r\n\ninv(X.T.dot(X)).dot(X.T).dot(y)\n\r\nБолее подробную информацию об этом уравнении можно найти в приложении В.\n\r\n\n\r\nЧтобы реализовать нормальное уравнение, нам нужно проделать следующее:\n\r\n1. Создать функцию, которая принимает матрицу X с признаками и вектор y с целью.\n\r\n2. Добавить фиктивный столбец (признак, который всегда содержит значение 1) в матрицу X.\n\r\n3. Обучить модель: вычислить веса w, используя нормальное уравнение.\n\r\n4. Разделить полученный w на смещение w\n0\n и остальные веса и вернуть их.\n\r\n\n\r\nПоследний шаг — разделение w на компонент смещения и остаток — необязателен и в основном нужен для удобства; в противном случае нам пришлось бы добавлять фиктивный столбец каждый раз, когда мы захотим получить прогноз, вместо того чтобы сделать это один раз во время обучения.\n\r\n\n\r\nРеализуем все это (листинг 2.2).\n\r\n\n\r\n\n\r\nС помощью шести строк кода мы внедрили наш первый алгоритм машинного обучения. В ❶ мы создаем вектор, содержащий только единицы, который мы добавляем к матрице X в качестве первого столбца; это фиктивный признак в ❷. Далее мы вычисляем X\nT\nX в ❸ и его обратное значение в ❹ и объединяем их, чтобы вычислить w в ❺. Наконец мы разделяем веса на смещение w\n0\n и остальные веса w в ❻.\n\r\n\n\r\nФункция column_stack в NumPy, которую мы использовали для добавления столбца‚ поначалу может смутить, поэтому рассмотрим ее более пристально:\n\r\n\n\r\n\nnp.column_stack([ones, X])\n\r\nОна принимает список массивов NumPy, который в нашем случае содержит ones и X, и складывает их (рис. 2.13).\n\r\n\n\r\n\n\r\nЕсли разделить веса на компонент смещения и остаток, то формула линейной регрессии для составления прогнозов немного изменится:\n\r\n\n\r\n\n\r\nЭто по-прежнему очень легко перевести на NumPy:\n\r\n\n\r\n\ny_pred = w0 + X.dot(w)\n\r\nДавайте используем ее для нашего проекта!\n\r\n\n\r\n\n\n                        \nОб авторе\n\n                        \nАлексей Григорьев\n проживает в Берлине со своей женой и сыном. Он опытный инженер-программист, специализирующийся на машинном обучении. Трудится в OLX Group главным специалистом по обработке данных, помогая своим коллегам внедрять машинное обучение в производство.\n\r\n\n\r\nВ свободное от работы время Алексей ведет DataTalks.Club, сообщество людей, которые любят науку о данных и машинное обучение. Кроме того, он является автором еще двух книг: Mastering Java for Data Science и TensorFlow Deep Learning Projects.\n\n                    \n\r\nБолее подробно с книгой можно ознакомиться на \nсайте издательства\n:\n\r\n\n\r\n» \nОглавление\n\r\n» \nОтрывок\n\r\n\n\r\nПо факту оплаты бумажной версии книги на e-mail высылается электронная книга.\n\r\nДля Хаброжителей скидка 25% по купону — \nМашинное обучение\n \n ",
    "tags": [
        "машинное обучение"
    ]
}