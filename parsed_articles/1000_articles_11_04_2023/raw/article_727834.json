{
    "article_id": "727834",
    "article_name": "Локальные нейросети (генерация картинок, локальный chatGPT). Запуск Stable Diffusion на AMD видеокартах",
    "content": "Многие слышали про Midjourney, но про то, что есть локальная Stable Diffusion, которая может даже больше, знает уже куда меньше людей, или они не знают, что она локальная. И если они пробовали её онлайн, то быстро приходили к выводу, что она сильно хуже чем Midjourney и не стоит обращать на неё более внимания. И да, SD появился раньше Midjourney. Для запуска хватит и cpu или 4гб видеопамяти. \nАналогично с chatGPT, про попытку сделать его локальную версию, не требующую супер компьютер, тоже мало кто слышал и знает, несмотря на то, что выходило несколько статей.\nслева stable diffusion, а справа... тоже stable diffusion\nЛокальная генерация картинок (Stable Diffusion, на видеокартах AMD, на CPU)\nЕсли вы просто поставите SD и попробуете что-то сгенерировать, то вы увидите в лучшем случае что-то такое:\nЭто совсем не похоже на уровень Midjourney. Все дело в том, что дефолтная SD модель не понимает, что именно вы от неё хотите без дополнительных правок. Поэтому требуется много ключевых слов (промтов) как позитивных, так и негативных, чтобы добиться приемлемого качества. \nДля того, чтобы этим не заниматься существуют уже дотюненные модели, где выставлено чтобы пальцев было 5, а тел 1, и другие ожидаемые от модели параметры. Пример, того же запроса, но на модели Deliberate:\nЕще один пример, вроде бы у SD 1.5 что-то получается, но это всё равно далеко от ожидаемого.\nЛокальный запуск\nИтак, чтобы локально запустить Stable Diffusion понадобится не так много. Установить один из вариантов веб-интерфейсов. Например, самые интересные это вот эти: \nstable-diffusion-webui - \nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui\nInvokeAI - \nhttps://github.com/invoke-ai/InvokeAI\nSHARK - \nhttps://github.com/nod-ai/SHARK\nВсе они отличаются какой-то особенностью, из-за чего могут быть полезны все. И все схожи в установке, которая не отличается какой-то сложностью, достаточно просто следовать инструкции.\nДля запуска будет достаточно видеокарты на 4гб. Если нет gpu, то доступен запуск на cpu - тогда генерация занимает минуты, и за счет оперативной памяти куда большие разрешения, но времени уйдет на это немыслимо. В зависимости от мощности процессора, генерация 512x512 может занять 3-4 минуты на 6 ядерном cpu. \nПосле этого нужно найти и скачать понравившуюся модель, которые расположены на сайте \nhttps://civitai.com/\nвыбрать есть из чего\nЕсли не хочется выбирать и сравнивать, то можете сразу взять \nDeliberate\n, она является одной из лучших моделей, у неё отлично и с анатомией, и с пальцами.\nВсе скачанные модели нужно размещать в папке моделей, либо вручную, для webui это будет путь:  stable-diffusion-webui\\models\\Stable-diffusion\\, либо через интерфейс добавления моделей, как у InvokeAI.\nСамым продвинутым веб-интерфейсом можно назвать stable-diffusion-webui, где помимо базовых широких возможностей без плагинов, также доступны и различные полезные плагины, например, ControlNet - который позволяет делать много разного, не доступного в базовом виде.\nВ сочетании с плагином Posex можно делать и позы.\nУ webui легко возвращать забытые промпты и настройки, для этого есть панель с кнопками под кнопкой Generate. Для сгенерированной картинки достаточно перетянуть её в поле ввода, или скопировать чужие промпты, или взять инфо из PNG Info и нажать на кнопку со стрелкой, после этого все поля будут автоматически заполнены.\nУ sd есть проблема с повторяемостью результатов. Из-за того, что разные видяхи используют разные ускорители вычислений, то например, если кто-то сгенерировал картинку с xformers на nvidia, на amd вы не сможете повторить её один в один, и наоборот. Но при этом если вы генерируете в разных веб-интерфейсах с одинаковыми настройками на одинаковой машине, результат будет повторяться. Возможно причина в чем-то другом, но в целом такая проблема существует.\nЗапуск с ускорением на видеокартах AMD\nДля своей работы sd использует pytorch, у которого доступны следующие варианты запуска: \nНа видеокартах nvidia доступно ускорение через CUDA, а на AMD через ROCm (hip). HIP - это прямой аналог cuda, но на данный момент он работает только под linux. Недавно blender для windows добавил поддержку hip, поэтому в целом работы по переносу hip на windows ведутся, но пока для пользователей windows доступны другие варианты, менее удобные, а именно враппер для Vulkan и DirectML.\nТаким образом, есть следующие варианты:\nзапускать на linux вместе с hip. Например, stable-diffusion-webui установит всё автоматически. Есть поддержка запуска в docker (под wsl2 не сработает).\nпод windows 2 варианта:\nиспользовать для ускорения vulkan - запуск на vulkan сейчас возможен только на sharp, работает почти на том же уровне что и hip, чуть медленнее. Но sharp имеет меньше возможностей и настроек.\nиспользовать форк \nhttps://github.com/lshqqytiger/stable-diffusion-webui-directml\n с ускорением через directml. По отзывам не всегда работает стабильно и отваливается с ошибкой о нехватки памяти, там где hip или vulkan работают без проблем. Лечится добавлением ключей запуска, которые замедляют генерацию, но снижают потребление памяти.\nНа данный момент для видеокарт amd самый быстрый вариант это linux + hip. И с этим есть некоторые особенности. Если всё не завелось автоматически и показывается сообщение об отсутствии hip-устройства, нужно явно указать версию HSA. В консоли перед запуском надо вбить один из 3х вариантов (в зависимости от вашей модели видеокарты):\nexport HSA_OVERRIDE_GFX_VERSION=10.3.0\nexport HSA_OVERRIDE_GFX_VERSION=9.0.12\nexport HSA_OVERRIDE_GFX_VERSION=8.3.0\nДругой вариант того же самого, например, это вводить команду запуска сразу с указанием нужного:\nHSA_OVERRIDE_GFX_VERSION=10.3.0 ~/invokeai/invoke.sh\nЕсли появляется проблема с нехваткой памяти, или видеокарта не поддерживает f16, помогут эти команды (только для webui, в других они выглядят по другому, если вообще есть):\n# Если ошибка про half:\n--precision full --no-half\n\n# Если хочется генерацию больших разрешений на 8гб памяти:\n--medvram \n\n# Если совсем мало памяти, например, 4гб:\n--lowvram \nКаждый из этих параметров сильно снижает скорость генерации (только medvram не существенно). Также medvram поможет при train.\nЛокальный chatGPT (только CPU)\nС локальным chatGPT ситуация не на столько качественная, как с картинками. Но что-то работает. Есть некоторое количество моделей, которые обучены следовать инструкциям. \nДля их локального запуска потребуется либо llama.cpp (\nhttps://github.com/ggerganov/llama.cpp\n), либо alpaca.cpp (\nhttps://github.com/antimatter15/alpaca.cpp\n) и найти нужную модель на huggingface. llama.cpp может запускать модели и от alpaca, и от gpt4all и от vicuna, поэтому можно сразу выбрать его для запуска.\nНапример, alpaca 30B: \nhttps://huggingface.co/Pi3141/alpaca-lora-30B-ggml/tree/main\nИли 13B vicuna: \nhttps://huggingface.co/eachadea/ggml-vicuna-13b-4bit/tree/main\n13B - это размер модели, 13 млрд параметров. Для запуска требуется меньше памяти, чем размер модели. Для alpaca 30B - 25гб памяти, а для vicuna 13B примерно 9.5гб памяти. Для 7B совсем мало требуется памяти, можно запускать на Raspberry Pi 4.\nСерия статей на хабре про эти модели: \nhttps://habr.com/ru/users/bugman/posts/\nИли даже больше, попытка энтузиастов по всему миру сделать открытый аналог chatGPT: \nhttps://habr.com/ru/articles/726584/\nvicuna понимает по-русски, но не рассчитывайте, что всегда и на хорошем уровне, это всё-таки не chatGPT\nДля запуска в интерактивном режиме (как chatGPT) нужна команда:\n# для llama.cpp\n./main -i --interactive-first -r \"### Human:\" --temp 0 -c 2048 -n -1 -t 12 --ignore-eos --repeat_penalty 1.2 --instruct -m ggml-vicuna-13b-4bit.bin\n\n# для alpaca.cpp (можно указать через --threads количество потоков процессора)\n./chat -m ggml-model-q4_0.bin --threads 12\nДля запуска 30B на alpaca.cpp, вам придется немного отредактировать файл chat.cpp перед компиляцией, как указано тут: \nhttps://huggingface.co/Pi3141/alpaca-lora-30B-ggml/discussions/3\n \n13B vicuna лучше справляется и с текстом и лучше генерирует код, чем 30B alpaca за счет более качественного подхода к обучению vicuna. Но 30B - это 30B, чем больше параметров, тем \"начитаннее\" бот.\nИтог\nНейросетей для запуска локально становится всё больше. Если вы знаете еще какие-то интересные, то делитесь ими в комментариях.\n \n ",
    "tags": [
        "stable diffusion",
        "alpaca",
        "нейросети",
        "vicuna"
    ]
}