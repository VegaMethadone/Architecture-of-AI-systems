{
    "article_id": "728158",
    "article_name": "Если вы отказались от регулярных выражений, то теперь у вас три проблемы",
    "content": "\r\nИзвестная шутка программистов гласит, что если решение вашей проблемы включает в себя парсинг текста при помощи регулярного выражения, то теперь у вас есть две проблемы. Некоторые программисты, прочитав шутку, решают попробовать иной подход. Возможно, регулярные выражения не так уж нужны. Возможно, задачу можно решить простым split строки или чем-то подобным. Однако другие могут задуматься немного глубже и задаться вопросом: «А если я сделаю нечто настолько дерзкое, что в результате получу три проблемы?» Мой пост написан в таком духе!\n\r\n\n\r\nВ нём используется код на Python, однако его легко можно адаптировать под любой язык с поддержкой функций высшего порядка.\n\r\n\n\r\n\n▍ Элементарные частицы\n\r\nЕсли вы задумаетесь о парсинге, то поймёте, что во многом он связан с потреблением входных данных. Давайте напишем функцию, выполняющую эту задачу:\n\r\n\n\r\n\ndef shift(inp):\n    return bool(inp) and (inp[0], inp[1:])\n\r\nПолучая входную последовательность \ninp\n, она возвращает первый элемент \ninp[0]\n и все оставшиеся элементы \ninp[1:]\n. Если входных данных не было, она возвращает \nFalse\n. Функция выглядит странно, но вот как она работает пошагово для символов строки:\n\r\n\n\r\n\n>>> shift('bar')\n('b', 'ar')\n>>> shift('ar')    # Применяется к оставшимся символам 'ar'\n('a', 'r')\n>>> shift('r')\n('r', '')\n>>> shift('')\nFalse\n>>>\n\r\nВсегда полезно бывает создать для каждой функции обратную ей функцию. По крайней мере, этому я научился, работая над ПО для физических расчётов. Что противоположно потреблению входных данных? Разумеется, отсутствие потребления входных данных. Давайте напишем эту функцию:\n\r\n\n\r\n\ndef nothing(inp):\n    return (None, inp)\n\r\n\nnothing()\n не выполняет обработку. Она возвращает \nNone\n для любых входных данных. Также она возвращает полученные входные данные (без изменений).\n\r\n\n\r\n\n>>> nothing('bar')\n(None, 'bar')\n>>>\n\r\n\nnothing()\n отличается от отсутствия доступных входных данных. Она просто означает, что вы решили \nНЕ\n делать ничего с имеющимися входными данными.\n\r\n\n\r\nОбе эти функции являются примерами того, что я буду называть «парсер». Парсер — это функция, определяемая своей сигнатурой вызова и соглашением о возвращаемых данных. В частности, парсер — это любая функция, принимающая какие-то входные данные (\ninp\n), а в случае успеха возвращающая кортеж \n(value, remaining)\n, где \nvalue\n — это некое нужное значение, а \nremaining\n — все оставшиеся входные данные, парсинг которых нужно выполнить. При неудаче парсер возвращает \nFalse\n.\n\r\n\n\r\nХотя эти функции и так короткие, можно ещё больше их сократить при помощи \nlambda\n:\n\r\n\n\r\n\nshift   = lambda inp: bool(inp) and (inp[0], inp[1:])\nnothing = lambda inp: (None, inp)\n\r\nПреимущество \nlambda\n в том, что она делает код компактным и наглядным. Кроме того, она не позволяет разработчикам пытаться добавлять значимые имена, документацию или type hint к тому, что будет развёрнуто.\n\r\n\n\r\nКстати, \nlambda\n — это третья проблема. Сдвиг, ничего и лямбда — это три проблемы. Давайте двигаться дальше.\n\r\n\n\r\n\n▍ Система парсинга\n\r\nТо, что мы получили, называется системой для парсинга. Как и у любой другой системы, у неё есть правила. В этой системе мы можем использовать только два имеющихся парсера (\nshift\n и \nnothing\n). Также можно использовать \nlambda\n для создания новых парсеров из имеющихся. Вот и всё.\n\r\n\n\r\n\n▍ Модификаторы парсеров\n\r\nДавайте напишем правило, применяющее предикат к результату парсера:\n\r\n\n\r\n\nfilt = lambda predicate: (\n         lambda parser:\n           lambda inp: (m:=parser(inp)) and predicate(m[0]) and m)\n\r\nТри лямбды?!? Моржовый оператор? Вычисление по краткой схеме? Что за ужас здесь творится? Да, я мог использовать вместо этого четыре лямбды:\n\r\n\n\r\n\nfilt = lambda predicate: (\n         lambda parser:\n           lambda inp:\n             (lambda m: m and predicate(m[0]) and m)(parser(inp)))\n\r\nОднако читаемость важна. К тому же — бог любит троицу. Три проблемы. Три лямбды. Три бивня. Не четыре лямбды и ни одного бивня.\n\r\n\n\r\nФункция \nfilt()\n получает на входе предикат и парсер и комбинирует их, чтобы создать новый парсер. Это выглядит немного странно, но работает схоже с декоратором. Вот пример того, как это использовать:\n\r\n\n\r\n\n>>> digit = filt(str.isdigit)(shift)\n>>> letter = filt(str.isalpha)(shift)\n>>> digit('456')\n('4', '56')\n>>> letter('456')\nFalse\n>>>\n\r\nВозвращаемое значение \nFalse\n в данном случае означает, что парсинг не сработал. Точно так же, как \nshift()\n возвращает \nFalse\n, когда заканчиваются входные данные, созданный при помощи \nfilt()\n парсер возвращает \nFalse\n, если созданное значение не соответствует переданному предикату.\n\r\n\n\r\nЗабавная формулировка \nfilt()\n позволяет вам легко создавать другие типы полезных фильтров, просто передавая произвольные предикаты. Вот фильтр для точного соответствия литералу:\n\r\n\n\r\n\nliteral = lambda value: filt(lambda v: v == value)\n\r\nВот фильтр, в котором значения должны браться из заданного множества допустимых значений:\n\r\n\n\r\n\nmemberof = lambda values: filt(lambda v: v in values)\n\r\nВот несколько примеров применения этих фильтров к имеющемуся парсеру:\n\r\n\n\r\n\n>>> dot = literal('.')(shift)\n>>> even = memberof('02468')(digit)   # Да, digit, не shift.\n>>> dot('.456')\n('.', '456')\n>>> dot('45.6')\nFalse\n>>> even('456')\n('4', '56')\n>>> even('345')\nFalse\n>>>\n\r\nЕстественно, можно продолжить всё упрощать. Чтобы выявлять соответствия отдельным символам, можно написать следующую вспомогательную функцию:\n\r\n\n\r\n\nchar = lambda v: literal(v)(shift)\n\r\n\n>>> dot = char('.')\n>>> dot('.456')\n('.', '456')\n>>>\n\r\nАналогично тому, что задача \nfilt()\n заключается в игнорировании элементов, противоположностью игнорирования является какое-то действие. Поэтому, чтобы сохранить баланс кода, нам нужна противоположная сила. Давайте назовём эту операцию \nfmap()\n:\n\r\n\n\r\n\nfmap = lambda func: (\n         lambda parser:\n           lambda inp: (m:=parser(inp)) and (func(m[0]), m[1]))\n\r\n\nfmap()\n получает на входе функцию и парсер и создаёт новый парсер, в котором переданная функция применяется для успешного парсинга. \nfmap()\n используется для преобразования значений. Например:\n\r\n\n\r\n\n>>> ndigit = fmap(int)(digit)\n>>> ndigit('456')\n(4, '56')\n>>> tenx = fmap(lambda x: 10*x)\n>>> tenx(ndigit)('456')\n(40, '56')\n>>> tenx(digit)('456')\n('4444444444', '56')\n>>>\n\r\n\nПримечание:\n \nmap\n и \nfilter\n — имена встроенных функций, которые Python использует для работы с итерируемыми элементами. Я бы использовал их, если бы это не запутывало присвоение имён. Поэтому я выбрал \nfmap\n и \nfilt\n. Концептуально наши функции служат семантически схожей цели.\n\r\n\n\r\n\n▍ Повторение\n\r\nПока наши парсеры работают только с одиночными входными данными. Чтобы делать более интересные вещи, нужно заставить их сопоставлять несколько входных данных. Например, несколько цифр или букв. Было бы здорово, если бы мы смогли задать нечто подобное:\n\r\n\n\r\n\ndigits = one_or_more(digit)\n\r\nЧтобы сделать это, мы можем использовать функциональные техники, задействующие рекурсию. Однако скажем напрямую — Python ужасно справляется с рекурсией по различным «причинам», меньшей из которых является его внутреннее ограничение на рекурсии. Этого мы делать не будем. Вместо этого вдохновимся театром и «сломаем четвёртую стену», повернувшись к зрителям и признав, что мы всё-таки пишем код на Python. Ладно, да будет так:\n\r\n\n\r\n\ndef one_or_more(parser):\n    def parse(inp):\n        result = [ ]\n        while (m:=parser(inp)):\n            value, inp = m\n            result.append(value)\n        return bool(result) and (result, inp)\n    return parse\n\r\nДа, цикл \nwhile\n не согласуется с нашей системой лямбд. Однако можно сказать, что это «pythonic» просто потому, что не ломает ограничения на рекурсии, поэтому давайте сделаем так. Как и другие наши функции, \none_or_more()\n принимает на входе парсер и создаёт на выходе новый парсер. Он многократно вызывает переданный парсер, пока больше не найдётся совпадений. Список создан.\n\r\n\n\r\n\n>>> digit = filt(str.isdigit)(shift)\n>>> digits = one_or_more(digit)\n>>> digits('456')\n(['4','5','6'], '')\n>>> digits('1abc')\n(['1'], 'abc')\n>>> digits('abc')\nFalse\n>>>\n\r\nЕсли вам не нравится, что цифры разделены на список, то используйте \nfmap\n, чтобы снова соединить их:\n\r\n\n\r\n\n>>> digits = fmap(''.join)(one_or_more(digit))\n>>> digits('456')\n('456', '')\n>>>\n\r\nЕсли вам нужно числовое значение, то добавьте ещё одну \nfmap\n:\n\r\n\n\r\n\n>>> value = fmap(int)(digits)\n>>> value('456')\n(456, '')\n>>>\n\r\n\nПримечание:\n если вы покажете своим коллегам функцию \none_or_more()\n, то они, вероятно, рассердятся, потому что вы не соблюдаете руководство по стилю. Они скажут: «Тебе нужно было использовать лямбду». Исправить это не так просто, но, возможно, вам удастся их обмануть при помощи \nfunctools.wraps()\n:\n\r\n\n\r\n\nfrom functools import wraps\n\n@wraps(lambda parser:_)\ndef one_or_more(parser):\n    @wraps(lambda inp:_)\n    def parse(inp):\n        ...\n    return parse\n\r\nПо крайней мере, в этом случае функция выглядит так, как будто берётся из лямбды, если кто-то не потратит время на изучение её определения.\n\r\n\n\r\n\nДомашнее задание:\n или же можно просто переписать \none_or_more()\n, не пользуясь ничем, кроме как лямбдой и рекурсией.\n\r\n\n\r\n\n▍ Последовательность\n\r\nИногда вам нужно спарсить одно после другого. Для этого можно написать оператор последовательности:\n\r\n\n\r\n\ndef seq(*parsers):\n    def parse(inp):\n        result = [ ]\n        for p in parsers:\n            if not (m:=p(inp)):\n                return False\n            value, inp = m\n            result.append(value)\n        return (result, inp)\n    return parse\n\r\n\nseq()\n получает на входе произвольное количество парсеров. Затем она создаёт новый парсер, в котором они выстроены по порядку, один за другим. Для успешного завершения парсинга все парсеры должны успешно выполнить свою задачу. Вот пример:\n\r\n\n\r\n\n>>> seq(letter, digit, letter)('a4x')\n(['a', '4', 'x'], '')\n>>> seq(letter, digit, letter)('abc')\nFalse\n>>> seq(letter, fmap(''.join)(one_or_more(digit)))('x12345')\n(['x', '12345'], '')\n>>> \n\r\nНаписав способ задания последовательностей, можно создать его полезные варианты. Например, иногда полезно выбрать только левую или правую часть пары.\n\r\n\n\r\n\nleft = lambda p1, p2: fmap(lambda p: p[0])(seq(p1, p2))\nright = lambda p1, p2: fmap(lambda p: p[1])(seq(p1, p2))\n\r\nВот пример:\n\r\n\n\r\n\n>>> left(letter, digit)('a4')\n('a', '')\n>>> right(letter, digit)('a4')\n('4', '')\n>>>\n\r\n\nДомашнее задание:\n напишите версию \nseq()\n с использованием лямбды.\n\r\n\n\r\n\n▍ Выбор\n\r\nСоздание последовательности требует, чтобы все парсеры совпадали. Какой будет обратная операция? Возможно, это функция, которая требует, чтобы совпадал только один из переданных парсеров. Давайте назовём эту операцию \neither()\n:\n\r\n\n\r\n\neither = lambda p1, p2: (lambda inp: p1(inp) or p2(inp))\n\r\nВот пример:\n\r\n\n\r\n\n>>> alnum = either(letter, digit)\n>>> alnum('4a')\n('4', 'a')\n>>> alnum('a4')\n('a', '4')\n>>> alnum('$4')\nFalse\n>>>\n\r\n\neither()\n позволяет создавать необязательные параметры и, наконец, даёт возможность воспользоваться \nnothing\n. Например:\n\r\n\n\r\n\nmaybe = lambda parser: either(parser, nothing)\n\r\n\n>>> maybe(digit)('456')\n('4', '56')\n>>> maybe(digit)('abc')\n(None, 'abc')\n>>>\n\r\nТакже можно использовать её для реализации \nzero_or_more()\n:\n\r\n\n\r\n\nzero_or_more = lambda parser: either(one_or_more(parser), seq())\n\r\n\n>>> zero_or_more(digit)('456')\n(['4','5','6'], '')\n>>> zero_or_more(digit)('abc')\n([], 'abc')\n>>>\n\r\nИ последнее: можно использовать \neither()\n для создания более мощной функции \nchoice()\n, позволяющей выбирают между любым количеством переданных парсеров.\n\r\n\n\r\n\nchoice = lambda parser, *parsers: (\n           either(parser, choice(*parsers)) if parsers else parser)\n\r\n\nДомашнее задание:\n перепишите \nchoice()\n так, чтобы не использовалась рекурсия.\n\r\n\n\r\n\nДомашнее задание:\n а действительно ли нам нужна \nnothing\n? Можно ли создать \nnothing\n из чего-то?\n\r\n\n\r\n\n▍ Пример: числа\n\r\nДавайте рассмотрим задачу парсинга чисел. Предположим, что числа поступают в двух вариациях. Целые числа имеют вид \n1234\n, а десятичные дроби — \n12.34\n. Однако с десятичными дробями всё чуть сложнее, потому что их можно записывать с замыкающим десятичным разделителем, например, \n12.\n или с начальным десятичным разделителем, например, \n.34\n. Допустим, нам нужно преобразовать целые числа в integer языка Python, а десятичные дроби — во float. Как вы будете парсить числа? Вот как это можно сделать:\n\r\n\n\r\n\ndot = char('.')\ndigit = filt(str.isdigit)(shift)\ndigits = fmap(''.join)(one_or_more(digit))\ndecdigits = fmap(''.join)(choice(\n               seq(digits, dot, digits),\n               seq(digits, dot),\n               seq(dot, digits)))\n\ninteger = fmap(int)(digits)\ndecimal = fmap(float)(decdigits)\nnumber = choice(decimal, integer)\n\r\nДавайте проверим нашу функцию \nnumber()\n.\n\r\n\n\r\n\n>>> number('1234')\n(1234, '')\n>>> number('12.3')\n(12.3, '')\n>>> number('.123')\n(0.123, '')\n>>> number('123.')\n(123.0, '')\n>>> number('.xyz')\nFalse\n>>>\n\r\n\n▍ Пример: пары «ключ-значение»\n\r\nДопустим, вам нужно спарсить пары «ключ-значение» в вид \nname=value;\n, где \nname\n состоит из букв, а \nvalue\n — это любое численное значение. Также предположим, что вокруг каждой из пар могут быть произвольные пробелы (которые нужно игнорировать). Вот как это можно сделать:\n\r\n\n\r\n\nletter = filt(str.isalpha)(shift)\nletters = fmap(''.join)(one_or_more(letter))\nws = zero_or_more(filt(str.isspace)(shift))\ntoken = lambda p: right(ws, p)\neq = token(char('='))\nsemi = token(char(';'))\nname = token(letters)\nvalue = token(number)\nkeyvalue = seq(left(name, eq), left(value, semi))\n\r\nДавайте проверим:\n\r\n\n\r\n\n>>> keyvalue('xyz=123;')\n(['xyz', 123], '')\n>>> keyvalue('   pi = 3.14  ;')\n(['pi', 3.14], '')\n>>>\n\r\nОбработка пробелов может потребовать небольшого исследования. Решением станет использование особой функции \ntoken()\n для отбрасывания начального пробела.\n\r\n\n\r\n\n▍ Пример: построение словаря\n\r\nДопустим, вы хотите расширить возможности парсера, чтобы он преобразовывал произвольное число в пары «ключ-значение», записываемые как \nkey1=value1; key2=value2; key3=value3;\n в словарь Python с теми же ключами и значениями. Вот как это сделать:\n\r\n\n\r\n\nkeyvalues = fmap(dict)(zero_or_more(keyvalue))\n\r\nПример:\n\r\n\n\r\n\n>>> keyvalues('x=2; y=3.4; z=.789;')\n({'x': 2, 'y': 3.4, 'z': 0.789}, '')\n>>> keyvalues('')\n({}, '')\n>>>\n\r\n\n▍ Пример: валидация ключей словаря\n\r\nДопустим, нужно написать парсер, принимающий только словари с ключами \nx\n и \ny\n. Для проверки этого можно использовать \nfilt()\n:\n\r\n\n\r\n\nxydict = filt(lambda d: d.keys() == {'x', 'y'})(keyvalues)\n\r\nПример:\n\r\n\n\r\n\n>>> xydict('x=4;y=5;')\n({'x': 4, 'y': 5}, '')\n>>> xydict('y=5;x=4;')\n({'y': 5, 'x': 4}, '')\n>>> xydict('x=4;y=5;z=6;')\nFalse\n>>>\n\r\nЭтот пример показывает, как можно интересным образом сочетать возможности. Раньше функция \nfilt()\n использовалась для фильтрации отдельных символов, а теперь она применяется к словарям.\n\r\n\n\r\n\n▍ Обсуждение: композиция\n\r\nВажнейшее свойство, позволяющее всему работать — это внимание к композиции. Вот как выглядит интерфейс для парсера в своей основе:\n\r\n\n\r\n\ndef parser(inp):\n    ...\n    if success:\n        return (value, remaining)\n    else:\n        return False\n\r\nВсё остальное создаётся на этом фундаменте. Всё множество различных функций наподобие \nfilt()\n, \nfmap()\n, \nzero_or_more()\n, \nseq()\n и \nchoice()\n создаёт новые парсеры, имеющие идентичный интерфейс. Поэтому всё работает со всем и везде. Возможно, основным источником проблем может быть \nfmap()\n. Так как она применяет пользовательскую функцию к подвергающемуся парсингу значению, очевидно, что передаваемая функция должна быть совместимой.\n\r\n\n\r\n\n▍ Обсуждение: анализ концепций\n\r\nЗадумаемся на минуту над формулировкой \nfilt()\n. Когда вы использовали \nfilt()\n, то, вероятно, думали, что это выглядит немного забавно. Примерно так:\n\r\n\n\r\n\ndigit = filt(str.isdigit)(shift)\n\r\nПочему \nshift\n находится снаружи? Кроме того, разве это не внутренняя подробность реализации? Разве мы не можем спрятать её вот так:\n\r\n\n\r\n\nfilt = lambda predicate: (\n         lambda inp: (m:=shift(inp)) and predicate(m[0]) and m)\n\ndigit = filt(str.isdigit)\n\r\nДа, это можно сделать, однако перемещение её внутрь ограничивает полезность \nfilt()\n до отдельных символов. Я предпочту \nfilt()\n, которая опасно гибка. Исходная формулировка позволяет применять предикат к \nЛЮБОМУ\n парсеру, даже к сложным, которые возвращают структуры данных. И это здорово. Этого нельзя было бы сделать, если бы выбор парсера находился внутри.\n\r\n\n\r\nЕщё один вопрос по \nfilt()\n (и связанным с ней функциям) относится к её странному формату вызова. Почему входной предикат и аргументы парсера обрабатываются вызовами отдельной функции, а не передаются вместе одной функции? Например, почему не сделать вот так?\n\r\n\n\r\n\nfilt = lambda predicate, parser: (\n         lambda inp: (m:=parser(inp)) and predicate(m[0]) and m)\n\ndigit = filt(str.isdigit, shift)\n\r\nЕсли сформулировать функцию таким образом, то определять полезные варианты наподобие \nliteral\n станет неудобнее. Ранее \nliteral\n определялась, завися только от части с предикатом. Вот так:\n\r\n\n\r\n\nliteral = lambda value: filt(lambda v: v == value)\n\r\nЭто сжатая и изящная запись. Однако, если \nfilt()\n потребуется дополнительный аргумент, это этот аргумент растекается на внешние функции, заставляя нас писать такой код:\n\r\n\n\r\n\nliteral = lambda value, parser: filt(lambda v: v == value, parser)\n\r\nЭто некрасиво. В исходном случае нам не требуется знать дополнительные подробности о \nfilt()\n.\n\r\n\n\r\n\n▍ Магия\n\r\nДавайте обсудим функцию \nshift()\n. В исходной формулировке она разбивает входную строку на её первый символ и оставшийся текст. Вот как выглядел код:\n\r\n\n\r\n\nshift = lambda inp: bool(inp) and (inp[0], inp[1:])\n\r\nА вот как он работал:\n\r\n\n\r\n\n>>> shift('hello world')\n('h', 'ello world')\n>>>\n\r\nЭто неэффективный способ обработки текста на Python. На самом деле, вероятно, это наихудший способ обработки текста, который только можно придумать. На моей машине тест парсинга строки с 100000 парами «ключ-значение» в словарь занял почти 2,5 минуты! Ужас!\n\r\n\n\r\nОсновная проблема заключается в копировании памяти, происходящем при вычислении \ninp[1:]\n. На самом деле, каждый вызов \nshift()\n выполняет почти полную копию входного текста. Можно ли этого избежать?\n\r\n\n\r\nВнимательный наблюдатель заметит, что во всём представленном коде ничего не делается со входным значением \ninp\n, за исключением его передачи в другое место. Единственный код, который с ним работает — это функция \nshift()\n! Более того, никакой код никогда не смотрит на значение оставшегося текста. Поэтому мы можем поменять описание данных для этих частей на нечто совершенно иное. Вместо того чтобы представлять входные данные в виде строки, мы, возможно, сможем использовать кортеж \n(text, n)\n, где \nn\n — это integer, обозначающий текущую позицию. Давайте попробуем переписать \nshift()\n следующим образом:\n\r\n\n\r\n\ndef shift(inp):\n    text, n = inp\n    return n < len(text) and (text[n], (text, n+1))\n\r\nВот как работает новая версия:\n\r\n\n\r\n\n>>> shift(('abc', 0))     # Обратите внимание, что теперь используется кортеж\n('a', ('abc', 1))\n>>> shift(('abc', 1))\n('b', ('abc', 2))\n>>> shift(('abc', 2))\n('c', ('abc' 3))\n>>> shift(('abc', 3))\nFalse\n>>>\n\r\nОбратите внимание, что входная строка никогда не меняется. Не происходит ни копирования, ни нарезания. По сути, Python передаёт строку как ссылку. Единственное изменяющееся значение — это целочисленный индекс.\n\r\n\n\r\nНикакой другой код менять не требуется. Вы можете убедиться, что всё по-прежнему идеально работает, при условии передачи входных данных в ожидаемом формате.\n\r\n\n\r\n\n>>> keyvalues(('x=2; y=3.4; z=.789;', 0))    # Обратите внимание: здесь кортеж\n({'x': 2, 'y': 3.4, 'z': 0.789}, ('x=2; y=3.4; z=.789;', 19))\n>>>\n\r\nЧтобы сокрыть часть подробностей входных данных, я могу добавить специальную функцию \nInput()\n для преобразования передаваемого пользователем ввода в мой внутренний формат. Например:\n\r\n\n\r\n\nInput = lambda inp: (inp, 0)\n\n# Пример\nresult = keyvalues(Input('x=2; y=3.4; z=.789'))\n\r\nЯ назвал \nInput\n с заглавной, чтобы оставить пространство для манёвра. Возможно, когда-нибудь я превращу её в класс. Возможно, я просто делаю это, чтобы высказать своё «фи» PEP-8. Кто знает?\n\r\n\n\r\nКак бы то ни было, при тестировании на тех же входных данных с 100000 парами «ключ-значение» время парсинга упало с 2,5 минуты до примерно 2,3 секунды. Это потрясающе. Мы решили проблему производительности, изменив представление входных данных и подправив одну строку кода.\n\r\n\n\r\nПочему это сработало? Думаю, потому, что всё написанная нами функциональность основана не на прямых манипуляциях с входными данными, а на композиции функций. Изменение представления данных не повлияло на композицию частей.\n\r\n\n\r\n\n▍ Ещё немного магии\n\r\nХотя мы сильно повысили производительность, всё равно выполняется много низкоуровневых манипуляций с отдельными символами. Возможно, будет логично использовать более правильный токенизатор. Например, можно просто использовать мой инструмент \nSLY\n для написания такого лексического анализатора:\n\r\n\n\r\n\nfrom sly import Lexer\n\nclass KVLexer(Lexer):\n    tokens = { EQ, SEMI, NAME, INTEGER, FLOAT }\n    ignore = ' \\t\\n'\n    FLOAT = r'(\\d+\\.\\d+)|(\\d+\\.)|(\\.\\d+)'\n    INTEGER = r'\\d+'\n    NAME = r'[a-zA-Z]+'    \n    EQ = r'='\n    SEMI = r';'\n\r\nЛексический анализатор создаёт токены, не символы. Например:\n\r\n\n\r\n\n>>> lexer = KVLexer()\n>>> list(lexer.tokenize(\"x=2;\"))\n[Token(type='NAME', value='x', lineno=1, index=0, end=1),\n Token(type='EQ', value='=', lineno=1, index=1, end=2),\n Token(type='INTEGER', value='2', lineno=1, index=2, end=3),\n Token(type='SEMI', value=';', lineno=1, index=3, end=4)]\n>>>\n\r\nМожем ли мы использовать свой фреймворк парсинга с таким токенизатором? Конечно! Для этого нужно заменить низкоуровневую работу с символами на использвание токенов, но во всём оставив парсер неизменным. Вот как выглядит новый парсер:\n\r\n\n\r\n\nexpect = lambda ty:\\\n           fmap(lambda tok: tok.value)(filt(lambda tok: tok.type == ty)(shift))\nname = expect('NAME')\ninteger = fmap(int)(expect('INTEGER'))\ndecimal = fmap(float)(expect('FLOAT'))\nvalue = choice(decimal, integer)\nkeyvalue = seq(left(name, expect('EQ')), left(value, expect('SEMI')))\nkeyvalues = fmap(dict)(zero_or_more(keyvalue))\n\nInput = lambda inp: (list(KVLexer().tokenize(inp)), 0)\n\r\nДавайте убедимся, что он работает:\n\r\n\n\r\n\n>>> r = keyvalues(Input('x=2; y=3.4; z=.789;'))\n>>> r[0]\n{'x': 2, 'y': 3.4, 'z': 0.789}\n>>>\n\r\nПри тестировании на моих больших входных данных эта версия выполняется около 0,9 секунды, что примерно в 2,5 раза быстрее, чем раньше.\n\r\n\n\r\n\n▍ Бум!\n\r\nЯ задумался о бесчисленных часах, потраченных мной на микрооптимизации парсера LALR(1) в моих инструментах наподобие \nPLY\n и \nSLY\n. Серьёзно, я потратил \nкучу\n времени на изучение этого кода, пытаясь убрать всё, что мешало производительности. Поэтому эти инструменты уже давно являются самыми быстрыми реализациями парсеров на чистом Python. Как моя новая методика будет выглядеть по сравнению с ними?\n\r\n\n\r\nЧтобы протестировать это, я задал схожий парсер пар «ключ-значение» в SLY:\n\r\n\n\r\n\nfrom sly import Parser\n\nclass KVParser(Parser):\n    tokens = KVLexer.tokens\n\n    @_('{ keyvalue }')\n    def keyvalues(self, p):\n        return dict(p.keyvalue)\n\n    @_('NAME EQ value SEMI')\n    def keyvalue(self, p):\n        return (p.NAME, p.value)\n\n    @_('INTEGER')\n    def value(self, p):\n        return int(p.INTEGER)\n\n    @_('FLOAT')\n    def value(self, p):\n        return float(p.FLOAT)\n\r\nВот как его можно использовать для парсинга нашего примера:\n\r\n\n\r\n\n>>> lexer = KVLexer()\n>>> parser = KVParser()\n>>> tokens = lexer.tokenize('x=2; y=3.4; z=.789;')\n>>> parser.parse(tokens)\n{'x': 2, 'y': 3.4, 'z': 0.789}\n>>>\n\r\nА теперь я попробую его на моих входных данных с 100000 пар «ключ-значение». Выполнение заняло 2,3 секунды. Это в три раза медленнее предыдущего теста, в котором использовался тот же поток токенов! Он даже немного медленнее, чем исходная «магическая» версия, которая просто работала с отдельными символами. Как такое может быть?\n\r\n\n\r\nТакого результата я не ожидал. Парсер LALR(1) управляется только поиском по таблице и автоматом. В нём отсутствует поиск с возвратом и он не использует глубокий стек композиции функций.\n\r\n\n\r\nВ качестве последней надежды я решил заново реализовать весь парсер при помощи PLY, имеющего более оптимизированную реализацию. Он выполняется примерно за 1,2 секунды. Получается, он тоже проигрывает.\n\r\n\n\r\nДолжен сказать, что моя цель не заключается в подробном анализе производительности, существует множество патологических пограничных случаев, в которых у новой методики могут возникнуть проблемы. Главный вывод заключается в том, что она намного быстрее, чем я предполагал.\n\r\n\n\r\n\n▍ Примечание: итерация\n\r\nВ Python концепция итерации определена чётко. Логично спросить, почему не использовать для этого итератор или функцию-генератор? Можно ли переписать \nshift()\n таким образом?\n\r\n\n\r\n\nshift = lambda inp: (x:=next(inp, False)) and (x, inp)\n\r\nПохоже, эксперимент подтверждает, что это может сработать:\n\r\n\n\r\n\n>>> inp = iter('abc')\n>>> shift(inp)\n('a', <str_iterator object at 0x10983e140>)\n>>> shift(inp)\n('b', <str_iterator object at 0x10983e140>)\n>>> shift(inp)\n('c', <str_iterator object at 0x10983e140>)\n>>> shift(inp)\nFalse\n>>>\n\r\nК сожалению, на самом деле это не работает, потому что в нашей методике парсинга используется поиск с возвратом, особенно при принятии решения в функциях \neither()\n, \nmaybe()\n и \nchoice()\n. При обработке \neither()\n парсинг какое-то время может выполняться успешно, а потом внезапно завершиться неудачно. Когда такое происходит, всё откатывается назад и проверяется другая ветвь парсинга.\n\r\n\n\r\nДля отката назад стандартного итератора Python механизмы отсутствуют. Хотя иногда можно скопировать итератор или использовать какую-нибудь магию из \nitertools\n, это выглядит довольно сложным. Хуже того, чтобы это заработало, необходимо вносить тонкие изменения во всей реализации, а не изолировать их в одном месте. Пока я оставлю это в качестве домашнего задания.\n\r\n\n\r\n\n▍ Полный код\n\r\nВот полная реализация базового фреймворка парсинга, описанного в этой статье. Я решил, что будет интересно просто увидеть его целиком.\n\r\n\n\r\n\n# -- Фреймворк парсинга\n\ndef shift(inp):\n    text, n = inp\n    return n < len(text) and (text[n], (text, n+1))\n\nnothing = lambda inp: (None, inp)\n\nfilt = lambda predicate: (\n         lambda parser:\n           lambda inp: (m:=parser(inp)) and predicate(m[0]) and m)\n\nliteral = lambda value: filt(lambda v: v==value)\nchar = lambda value: literal(value)(shift)\n\nfmap = lambda func: (\n         lambda parser:\n           lambda inp: (m:=parser(inp)) and (func(m[0]), m[1]))\n\neither = lambda p1, p2: (lambda inp: p1(inp) or p2(inp))\nmaybe  = lambda parser: either(parser, nothing)\nchoice = lambda parser, *parsers: either(parser, choice(*parsers)) if parsers else parser\n\ndef seq(*parsers):\n    def parse(inp):\n        result = [ ]\n        for p in parsers:\n            if not (m:=p(inp)):\n                return False\n            value, inp = m\n            result.append(value)\n        return (result, inp)\n    return parse\n\nleft = lambda p1, p2: fmap(lambda p: p[0])(seq(p1, p2))\nright = lambda p1, p2: fmap(lambda p: p[1])(seq(p1, p2))\n\ndef one_or_more(parser):\n    def parse(inp):\n        result = [ ]\n        while (m:=parser(inp)):\n            value, inp = m\n            result.append(value)\n        return bool(result) and (result, inp)\n    return parse\n\nzero_or_more = lambda parser: either(one_or_more(parser), seq())\n\nInput = lambda inp: (inp, 0)\n\n# -- Пример: преобразование \"key1=value1; key2=value2; ...\" в словарь\n\n# числа и значения\ndot = char('.')\ndigit = filt(str.isdigit)(shift)\ndigits = fmap(''.join)(one_or_more(digit))\ndecdigits = fmap(''.join)(choice(\n               seq(digits, dot, digits),\n               seq(digits, dot),\n               seq(dot, digits)))\n\ninteger = fmap(int)(digits)\ndecimal = fmap(float)(decdigits)\nnumber = choice(decimal, integer)\n\n# имена\nletter = filt(str.isalpha)(shift)\nletters = fmap(''.join)(one_or_more(letter))\n\n# Пробел\nws = zero_or_more(filt(str.isspace)(shift))\ntoken = lambda p: right(ws, p)\n\n# Токены (с удалённым пробелом)\neq = token(char('='))\nsemi = token(char(';'))\nname = token(letters)\nvalue = token(number)\n\n# Одна пара key=value;\nkeyvalue = seq(left(name, eq), left(value, semi))\n\n# Несколько пар \"ключ-значение\"\nkeyvalues = fmap(dict)(zero_or_more(keyvalue))\n\n# Пример\nresult, remaining = keyvalues(Input(\"x=2; y=3.4; z=.789;\"))\nprint(result)\n\r\n\n▍ Похожие работы\n\r\nЕсли вас потряс этот пост, то вам следует изучить \nкомбинаторы парсеров\n. Программирование при помощи комбинаторов — не самая распространённая вещь в Python, но это может быть интересным способом достижения необычной чрезвычайной гибкости.\n\r\n\n\r\nВ мире Python есть множество связанных с парсингом инструментов, построенных на схожих концепциях. Большинство из них содержит излишества, но их стоит изучить:\n\r\n\n\r\n\n\r\n\nPyParsing\n\r\n\nparsy\n\r\n\nreparsec\n\r\n\nparsita\n\r\n\n\r\n\n▍ Размышления\n\r\nЗа последние двадцать лет я много работал с парсингом на Python. В том числе я разработал множество пакетов генераторов парсеров LALR(1) и преподавал курс по компиляторам, на котором обычно даю студентам задание написать парсер с рекурсивным спуском. Хотя я раньше слышал сочетание слов «комбинатор парсеров», впервые попробовал поработать с ними только недавно.\n\r\n\n\r\nВ этом январе я решил, что попробую в качестве хобби-проекта реализовать свой курс по компиляторам на Haskell. Ранее никогда не программировав на Haskell, я был вынужден перестроить часть своего мозга. Я начал работу с книги «Programming in Haskell» Грэма Хаттона. Как оказалось, Грэм обладает существенным опытом в монадных комбинаторах парсеров (см., например, \nэту статью\n) и многие примеры в его книге используют эту технику.\n\r\n\n\r\nМне никогда не доводилось писать парсер таким образом. Поэтому по большей части в моём посте приведён «ремикс» этих идей на Python. Я свободно использовал идиомы Python и делал всё немного иным способом. Однако код всё равно воплощает общую большую идею. Кроме того, я писал код на Python в «эквациональном стиле», отражающем упор на композицию функций, а не на реализацию функций.\n\r\n\n\r\nМеня поразил минимализм и выразительность кода. Здесь не используется никакого сложного метапрограммирования, перегрузки операторов и даже классов. Если вы хотите посмотреть, как всё работает, то код у вас прямо перед глазами. Большинство функций очень компактно. Удивительно то, как они сочетаются друг с другом.\n\r\n\n\r\nПримечание: иногда люди спрашивают меня: «Как мне улучшить свои навыки Python?». К их удивлению, я часто предлагаю создать проект на совершенно другом языке или за рамками их сферы опыта. Я думаю, что основной выигрыш от этого заключается в том, что часто вы видите совершенно иной способ мышления о задачах, который можно привнести в свои проекты.\n\r\n\n\r\n\n▍ В заключение: буду ли я действительно использовать это?\n\r\nКак уже говорилось, я изучил эту технику в версии для Haskell. Однако позже я применил её к реализации на Python в моём проекте курса по компиляторам. Получившийся в результате парсер был примерно вдвое меньше написанного вручную парсера рекурсивного спуска и включал в себя меньшее количество концепций. Кроме того, получившийся код напрямую отражал грамматику языка, определённую при помощи PEG. И, наконец, оказалось, что новая реализация имеет множество хороших свойств, связанных с обработкой ошибок и с сообщениями об ошибках. В конечном итоге, она понравилась мне намного больше.\n\r\n\n\r\nБолее подробное обсуждение написания полного парсера языка программирования стоит отложить для другой статьи. Однако в заключение надо сказать, что я действительно могу использовать эту технику для создания парсера чего-то реального.\n\r\n\n\r\n\nTelegram-канал с розыгрышами призов, новостями IT и постами о ретроиграх 🕹️\n \n ",
    "tags": [
        "ruvds_перевод",
        "regex",
        "regexp",
        "парсинг данных",
        "парсеры",
        "композиция"
    ]
}