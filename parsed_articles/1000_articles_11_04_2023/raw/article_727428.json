{
    "article_id": "727428",
    "article_name": "Как не заменить фару сосиской: определение качества изображений в сервисе оценки технического состояния автомобиля",
    "content": "В научно‑популярных статьях и докладах, обучающих материалах по системам компьютерного зрения упор нередко делается на основную компоненту — тяжелые (или не очень) нейронные сети, которые неким волшебным образом обрабатывают картинку, и на выходе отдают результат. \nОднако каждый ли вход в сеть стоит обрабатывать? Обучающие датасеты заранее подобраны и размечены, мусора и шума там чаще всего относительно мало, чего нельзя сказать о данных на входе в реально работающие системы. Особенно если данные загружаются обычными пользователями.\nМы не можем гарантировать, что сеть корректно обработает любой вход. Да, есть способы оценить, насколько модель уверена в своем ответе, но уже после обработки входа, когда мы потратили вычислительные ресурсы. Можем ли мы сказать заранее, что корректно обработать изображение не получится, что оно скорее всего не содержит достаточно информации? Давайте попробуем разобраться на примере реальной задачи. \nИтак, мы продолжаем цикл статей о создании \nCarDamageTest\n — сервиса дистанционной оценки технического состояния автомобилей на основе фото‑ и видеоматериалов. Сегодня мы познакомимся с задачей image quality assessment — оценкой качества изображения.\nВ статье вы узнаете, зачем системам компьютерного зрения вообще нужна оценка качества входа, когда с ней лучше, чем без нее, как ее построить и какие проблемы могут возникнуть в процессе. \nПервая половина статьи носит скорее развлекательный характер, тогда как вторая половина рассчитана на начинающих специалистов в компьютерном зрении (хотя вполне может быть полезна и более опытным коллегам) и просто интересующихся обработкой изображений.\nМотивация - а давайте сделаем свой сервис?\nДля начала определимся, зачем нам вообще строить лишний модуль на входе. Разве нейронка не может сама определить, «нравится» ей вход или нет? А может, можно «скормить» изображение сети, а затем определить по выходу, мусор там был или нет?\nДавайте представим программиста Васю, который хочет сделать приложение на смартфон. Вася немного увлекается ML и хочет применить знания на практике. Приложение принимает на вход фотографию и оценивает, кот на фотографии или собака. Наш программист полон энтузиазма и без тени сомнения заходит на первую итерацию!\nГотовые датасеты \nкотопсов\n уже есть. Вася берет довольно маленькую доступную предобученную модель (хочется же легкое и удобное приложение), доучивает ее на нашем датасете. Метрики отличные! Пишется приложение, встраивается модель, проверяется на своем коте, вуаля! Все работает. Однако, первый же человек, которому Вася дает смартфон с приложением, неожиданно для него, фотографирует свой палец. А приложение говорит ему, что это кот. «Плохой ты, Вася, программист, и приложение у тебя негодное», — говорит знакомый, отдает обратно телефон и уходит по своим делам, оставляя Василия наедине со своими мыслями.\nНу, все ясно! У сети всего два выхода, один для кота, второй для собаки. Василий решает прикрутить третий выход для всего остального, тогда модель должна начать работать корректно. Парсит еще десять тысяч изображений из поисковика для чего угодно, кроме котов и собак, учит сеть и … метрики по котопсам падают. Очевидно, что распределение данных на входе в сеть стало шире, и ее (сети) емкости уже не хватает для построения более сложных правил. Вася увеличивает размер сети. Учится она уже дольше (ведь докинули еще десять тысяч фотографий), размер приложения стал больше, работает оно дольше, но на фотографию пальца оно уже корректно отвечает, что ни кота, ни собаки тут нет. Наш программист приходит в гости к своему другу и дает ему приложение. А друг берет и наводит его на диван, на котором в обнимку спят кот и собака. Приложение отвечает, что на картинке кот. «Плохой ты, Вася, программист, и приложение у тебя неприглядное», — говорит наш товарищ, отдает обратно телефон и уходит на кухню за горячими напитками, оставляя Василия наедине со своими мыслями.\nДействительно! На картинке может быть много котов или собак, или одновременно коты и собаки. Василий видит два варианта: можно прикрутить к классификатору еще один выход, который говорил бы, что на картинке больше одного животного. А можно заменить классификатор на \nдетектор\n. Тогда детектор бы выделял на фотографии рамкой кота или собаку и делал подпись. Для первого варианта Васе придется самому делать датасет, и от этого ему становится грустно. Для второго варианта \nдатасет уже есть\n. Да и предобученные модели, определяющие котов и собак, \nтоже есть\n. Проблема с пальцем решается сама собой — его модель просто не будет детектировать. Сделал! Дал приложение коллеге с работы. А у коллеги на смартфоне камера хуже, чем на \nNokia 7700\n. Приложение видит котов и собак повсюду! Но только не там, где они реально есть. Проверять приложение на штанах коллеги Василий не рискнул. \nШтаны коллеги Василия\nСтрадающий от бренности бытия программист докидывает все больше и больше данных в модель. Применяет сильные аугментации — замыливает, затемняет, обрезает картинки. Вариативность обучающих данных растет просто неприлично. Чтобы сохранить метрики на приемлемом качестве, Вася увеличивает размер модели. И вот уже и учится модель не 10 минут, а весь рабочий день, \nкажется пора покупать \nDGX-2\n,\n да и приложение весит вместо пяти мегабайт все сто, хотя работать стало, безусловно лучше. Только никто ставить его не хочет, бесполезное. Вот если бы оно породу определяло… «Плохой я, наверное, программист, да и эти ваши нейронки все скверные. Пойду тиктоки снимать», — думает Василий, осознавая наконец сложность задачи.\nИсточник\n мема\nКонечно, ситуация выше весьма утрированная (но это не точно). Подытожим. Несмотря на имеющиеся у нас возможности учить нейронные сети end‑to‑end (т. е. получили сырой вход и отдали готовый результат, с нюансами пусть сеть разбирается сама), такой способ построения системы имеет недостатки:\nПредопределенная человеком структура, ограничения и правила вносят априорные знания о данных. Если такой структуры нет, сети придется строить их самостоятельно. Значит, сеть скорее всего будет больше.\nВ обучающем датасете обязательно должно быть достаточно примеров, на которых сеть сможет выстроить упомянутые правила. Если правил больше, то и данных, весьма вероятно, потребуется больше.\nДля большого черного ящика страдает интерпретируемость: становится сложнее сказать, почему сеть приняла то или иное решение.\nКонечно, плюсы у end‑to‑end систем тоже присутствуют. Сеть может построить правила, которые вручную запрограммировать очень сложно в разумное время. Или правила, о которых человек и не подозревал (в том числе правила, которых не существует). Если данных много, вычислительные возможности позволяют, а сама задача позволяет обучение без предварительной разметки человеком, то end‑to‑end системы могут быть довольно мощными, за примерами далеко ходить не надо: последние успехи генеративных моделей действительно впечатляют.\nЕсли же данных мало (или вообще нет), данные имеют сложную структуру, а вычислительные ресурсы ограничены, то компонентные системы за счет априорных знаний проявляют себя во всей красе.\nНаша система по оценке повреждений автомобилей состоит из множества модулей. Входные модули сужают вариативность поступающих данных. Один из них — модуль оценки качества изображения, наряду с другими, которые определяют наличие автомобиля, наличие необходимых ракурсов и т. д. Меньше вариативность данных — меньше нужно собственно данных для обучения модели, да и сама модель тоже будет меньше и быстрее.\nФактически, в основные модули, которые ищут повреждения, попадают только те изображения, на которых в принципе можно найти повреждения, и эти изображения качественные настолько, чтобы мы были уверены в результате. \nХорошо, с необходимостью оценки качества изображений разобрались. А какие вообще есть способы оценки качества изображений?\nМетоды оценки качества изображений\nВообще, методы оценки качества отдельных изображений можно разделить на три большие группы:\nРеференсные (Full‑reference). Если у нас есть исходная картинка и «испорченная» картинка, то мы можем измерить, насколько конкретно плохая картинка дальше от хорошей по каким‑либо критериям. Сюда относятся, например \nPSNR\n и \nSSIM\n.\nПолуреференсные (Reduced‑reference). У нас есть «испорченная» картинка и некоторая информация, извлеченная из оригинального изображения (бинарные маски, негативы, статистики о распределении цвета и пр). Или только информация, извлеченная из обоих изображений, без наличия самих изображений.\nНереференсные (No‑reference). Есть только одно изображение, качество которого нужно измерить. Здесь есть как продвинутые методы типа \nBRISQUE\n, так и попроще, вроде оценки контрастности.\nНас интересуют нереференсные методы. С ними все тоже непросто. Мы, как настоящие ML инженеры, можем \nзабить все гвозди микроскопом\n собрать датасет с «плохими» картинками и обучить очередной классификатор. Что же нас останавливает? По большому счету это не решение проблемы, а эскалация ее на один уровень выше. Ведь мы по‑прежнему не знаем, как именно отличать плохие изображения от хороших, а значит, не можем разметить датасет. Размечать «на глаз», очевидно, плохая идея. Ведь модели не «видят» также, как человек, а значит и ориентироваться на человеческий глаз мы не можем. К тому же это субъективная история, зависящая от разметчика.\nТакже, при бинарной постановке задачи (т. е. хорошая или плохая картинка), мы не сможем объяснить пользователю, почему мы не приняли эту фотографию и как ему все‑таки снять свою машину, чтобы быть уверенным в результате.\nАх да, мы еще не обозначили требования к системе оценки качества изображений? Ну тогда…\nТребования к модулю оценки качества\nИтак, мы хотим:\nЧтобы фотографии, на которых модель (например, сегментации повреждений) будет работать плохо, в эту модель не попадали.\nВнятно объяснить пользователю, почему то или иное изображение было отклонено, и дать совет, как это исправить.\nОно должно работать быстро. Мы также работаем с видео, а в видео много кадров, поэтому мы хотим их отфильтровать за минимальное время.\nЖелательно, чтобы все работало на CPU просто потому, что инференс на GPU дороже.\nЗакатаем рукава\nПо п.1 наших требований снова напрашивается решение: а давайте соберем и разметим некий датасет для этой модели (сегментации повреждений), проверим метрики на каждом изображении, и те изображения, на которых модель будет работать плохо, и будут считаться плохими? Резонно. Однако, во‑первых, где гарантия того, что мы сможем собрать в нашем датасете достаточно граничных случаев для генерализации моделью‑фильтром? Во‑вторых, покроем ли мы все возможные граничные случаи? И как проверять итоговое решение? Собрать второй датасет? Но ведь он будет смещен точно также. К тому же, мы не выполним п.2 требований.\nДавайте посмотрим на проблему с другой стороны: а что вообще может пойти не так? Ведь мы знаем, что снимать автомобиль будут на камеру смартфона. Исключим моменты, когда на кадре вообще нет автомобиля или его части, или автомобиль будет перекрываться препятствием, или он будет слишком грязным, или погребен под снегом — этим будут заниматься другие модули. А мы сосредоточимся на качестве изображения. Что у нас остается?\n1. Размытое и замыленное изображение. 2. Непроглядная тьма. 3. Контрастность. 4. Слепящий свет.\nХороших новостей сразу две. Можно заметить, что такие артефакты легко доступно объяснить пользователю: «изображение слишком смазанное» или «автомобиль плохо освещен». А еще все эти проблемы можно измерить численно.\nНам понадобится Python (3) и библиотеки \nnumpy,\n \nopencv\n, \nalbumentations\n. \nimport numpy as np\nimport cv2\nimport albumentations as A\n\ndef read_image(path: str) -> np.ndarray:\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img\nРазмытие\nСтрого говоря, заблюренной может быть не вся картинка, а только задний фон, или только автомобиль. Можно построить карту размытия, как описано в работе \nLBP‑based Segmentation of Defocus Blur\n, и сопоставить ее с положением автомобиля на изображении. \nСегментация размытия из статьи \nLBP-based Segmentation of Defocus Blur\n (Fig. 14)\nНо не будем пока все усложнять, и для первой версии рассмотрим очень простой \nметод\n:\ndef blur_score(img: np.ndarray) -> np.float64:\n    return cv2.Laplacian(img, cv2.CV_64F).var()\nОн оценивает блюр по всему изображению. Если считать блюр после детекции, когда автомобиль вырезается из полного изображения, то работает не сильно хуже более сложных методов.\nДля проверки методов будем использовать аугментации из библиотеки \nalbumentations\n. Возьмем относительно чистое референсное изображение и будем его ухудшать, проверяя результат численно и визуально. В качестве параметра, контролирующего ухудшение, используем параметр \nblur_limit\n функции \nBlur\n в виде \nblur_limit=(kernel_size, kernel_size)\n.\nИзображение и выход функции blur_score при различных значениях kernel_size.\nГрафик зависимости blur score от размера ядра\nКак видим, зависимость нелинейная, и blur_score быстро падает с ростом размера ядра. Следует помнить, что нелинейные зависимости сулят дополнительные сложности при подборе порогов. Посмотрим, как blue_score поведет себя при фокусе.\nВарианты фокусировки. Изображение слева взято \nотсюда\n, справа \nотсюда\n.\nНаша функция реагирует на изменения, но, как и ожидалось, значение пропорционально «смазанной» площади изображения. Поэтому, если высок риск получения сервисом подобных изображений, стоит рассмотреть более продвинутые способы оценки блюра.\nЗатемненность\nЧем отличается темная картинка от светлой на уровне пикселей? Очевидно, интенсивностью. Поэтому давайте просто конвертируем изображение в черно‑белое и посчитаем среднее.\ndef gray_score(img: np.ndarray) -> float:\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    rating = cv2.mean(img)[0] / 255.0\n    return rating\nПроверить тоже просто - будем умножать значения пикселей на коэффициент меньше 1.\ndef darken_image(img: np.ndarray, mult: float = 1.0) -> np.ndarray:\n    augmented_image = (img * mult).round().astype(np.uint8)\n    return augmented_image\nИзображение и выход функции gray_score при различных значениях mult.\nПонятно, что зависимость между multiplier и gray_score будет линейной, но проверим для порядка.\nНедостаток тот же, что и у функции по оценке блюра: часть изображения может быть светлой, а часть темной. Мы же оцениваем среднее, что может дать ложный результат.\nКонтрастность\nКонтрастность - это разница в яркости между соседними светлыми и темными участками изображения. Посчитать контрастность изображения можно \nпо-разному\n. Для демонстрации возьмем RMS метод. Перед расчетом контрастности лучше немного заблюрить изображение, так результаты будут более стабильными.\ndef contrast_score(\n    img: np.ndarray, \n    blur_kernel: tuple[int, int] = (11, 11)\n    ) -> np.float64:\n    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n    blurred = cv2.GaussianBlur(gray, blur_kernel, 0)\n    contrast = blurred.std()\n    return contrast\nДля изменения контрастности можно воспользоваться \nэтим советом\n:\ndef adjust_image_contrast(\n    img: np.ndarray, \n    contrast: float = 1.0\n    ) -> np.ndarray:\n    brightness = int(round(255*(1-contrast)/2))\n    augmented_image = cv2.addWeighted(img, contrast, img, 0, brightness)\n    return augmented_image\nИзображение и выход функции contrast_score при различных значениях contrast.\nГрафик зависимости contrast score от коэффициента затемнения\nЗависимость скора от контрастности также линейная, ломается только на этапе насыщения.\nЗасветка\nСтрого говоря, свет может быть прямым или отраженным. Прямой - когда свет бьет прямо в камеру, отраженный видно на 4 изображении выше. Эффекты от такого воздействия могут отличаться на разных камерах с разными настройками. \nПодходы тоже могут быть разными. Например, можно \nнаходить яркие места на изображении\n, а затем считать их суммарную или максимальную площадь. \nРезультат детекции ярких участков на изображении из статьи \nDetecting multiple bright spots in an image with Python and OpenCV\nДля примера возьмем самый простой подход. Конвертируем изображение в \nHSL\n цветовую модель, и посчитаем среднее по светлоте (канал L). Возможно, это не в точности то, что мы хотим, но в качестве базового решения может сработать.\ndef luminance_score(img: np.ndarray) -> np.float64:\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n    luminance = img.reshape(-1, 3)[:, 1].mean()\n    return luminance\nПросто поднимать luminance мы не будем: очевидно, что зависимость будет линейной, и наш скор будет прекрасно детектировать изменения. Вместо этого, снова воспользуемся \nalbumentations\n, и функцией \nRandomSunFlare\n. Эта аугментация помещает на изображение случайную солнечную вспышку. Нам случайность не нужна, поэтому зафиксируем все параметры, кроме радиуса вспышки.\ndef image_sunflare(img: np.ndarray, radius: int = 100) -> np.ndarray:\n    transform = A.RandomSunFlare(\n        flare_roi=(0.7, 0.2, 0.71, 0.21), \n        angle_lower=0.5, \n        angle_upper=0.51,\n        num_flare_circles_lower=2, \n        num_flare_circles_upper=3, \n        src_radius=radius, \n        always_apply=True\n        )\n    augmented_image = transform(image=img)['image']\n    return augmented_image\nИзображение и выход функции luminance_score при различных значениях radius функции  image_sunflare.\nХорошо. А что насчет более темного изображения?\nИзображение и выход функции luminance_score при различных значениях radius функции  image_sunflare.\nПри радиусе вспышки в 500 для второго изображения выход функции равен 150, тогда как для снежного изображения без вспышки функция оценивает засветку в 170. Поэтому для фильтрации изображений с яркими бликами данный метод не подходит, что ожидаемо: глобальный метод вряд ли будет адекватно оценивать локальные артефакты. Тем не менее, он отбросит сильно засвеченные изображения. Справедливости ради, в реальности такие изображения будут приходить очень редко, поэтому для первой итерации можно эту историю пока оставить в таком виде.\nОтчасти этой же проблеме (смещенной оценке локальных артефактов) подвержены глобальные методы по оценке контрастности и затемненности — темные машины будут в среднем всегда получать скоры хуже, чем светлые. Однако, физику тоже никто не отменял: для оценки светлой машины в принципе нужно меньше света, чем для темной.\nХорошо, с методами оценки изображений для первой итерации определились. Что дальше?\nКалибруем фильтр\nДля начала договоримся о терминологии.\nЦелевая модель — это модель, под которую мы делаем фильтрацию изображений. Например, если мы хотим сегментировать повреждения, тогда целевой моделью будет сегментатор. Мы должны уметь измерять интересующую нас метрику целевой модели.\nЧто делать, если целевых моделей несколько? Например, после фильтров работает каскад моделей. Тогда нужно разработать метрику, которая учитывает выход со всего каскада, и калибровать фильтр уже на ней.\nОценочные функции — это наши методы оценки качества изображения: gray_score, luminance_score и т. д. \nМы научились численно измерять качество изображений. Теперь нужно как‑то установить пороги, при пересечении которых мы будем изображение отфильтровывать, а пользователю возвращать фидбэк — что отфильтровано, почему и как это исправить. Либо, если входящих изображений несколько, а ответ нужно дать только один, мы можем выбрать, какое изображение использовать.\nДля поиска порогов нам нужен калибровочный датасет для целевой модели. Для начала вполне сойдет валидационный датасет, на котором проверяется целевая модель при обучении, хотя нужно помнить, что оценка на нем может быть смещена. \nНаивный метод\nЗдесь мы предполагаем, что оценочные функции между собой не коррелируют, что, по крайней мере в нашем случае, неправда. Но на то он и наивный метод.\nВыбираем одну оценочную функцию и воздействующую на оценки этой функции аугментацию изображений. Например, оценочной функцией будет \ncontrast_score,\n измеряющая контрастность изображения, а аугментацией \nadjust_image_contrast,\n которая эту контрастность изменяет.\nИзмеряем метрику целевой модели на калибровочном датасете, а также среднее значение оценочной функции на этом датасете. Метрикой может быть \nF1\n, \nIoU\n, \nmAP\n, или любая другая метрика, которая нас интересует.\nСлегка «портим» датасет выбранной аугментацией. т. е. проходимся по всем изображением функцией adjust_image_contrast, установив параметр \ncontrast\n чуть меньше или больше единицы. Снова выполняем пункт 2.\nЕще больше «портим» датасет, т. е. проходимся по \nисходному\n датасету функцией adjust_image_contrast, изменив contrast немного больше, чем в прошлый раз. И снова замеряем метрику целевой модели. и т. д.\nВ результате мы можем построить функцию зависимости метрики целевой модели от значения оценочной функции. Из которой, исходя из допустимого падения метрик, можно выбрать пороги. Повторяем для каждой оценочной функции.\nК достоинствам данного метода можно отнести простоту и скорость реализации, к недостаткам, как и было сказано, низкую точность, т.к. метод не учитывает взаимосвязь оценочных функций. Также придется вычислять метрику по датасету каждый раз при изменении силы аугментации, что может быть долго при большом датасете и большом количестве оцениваемых параметров изображения.\nМетод аппроксимации\nС точки зрения математики, мы пытаемся аппроксимировать некую функцию, которая принимает на вход значения оценочных функций, а на выход отдает метрику целевой модели. Зная рельеф функции в окрестности точки максимума (или минимума, в зависимости от того, с какой метрикой мы работаем), можно выбрать пороги оценочных функций.\nЗдесь можно действовать несколькими способами. Если метрика целевой модели подразумевает достаточно гладкую оценку качества каждого сэмпла (например, \nIoU\n), то можем оценивать каждый сэмпл независимо, и наша задача упрощается.\nПроходим по калибровочному датасету, фиксируя значение метрики на каждом сэмпле. В качестве метрики целевой модели возьмем IoU.\nФайл\nIoU\n1.jpg\n0.68\n2.jpg\n0.88\n«Портим» каждый сэмпл случайными аугментациями (случайные аугментации, случайные комбинации аугментаций и случайная сила аугментаций), записываем значения всех оценочных функций и значение метрики. Прогоняем датасет N итераций. На выходе получаем следующую таблицу. В таблице для примера указаны все рассмотренные нами оценочные функции.\nФайл\nblur\ncontrast\nluminance\ngray\nIoU\n1.jpg\n312\n30\n97\n0.45\n0.67\n1.jpg\n351\n22\n98\n0.20\n0.59\n1.jpg\n764\n30\n67\n0.50\n0.34\n2.jpg\n76\n10\n52\n0.54\n0.88\n2.jpg\n45\n12\n63\n0.33\n0.76\nОпределяем порог падения метрик — насколько мы готовы пожертвовать метриками ради удобства пользователя. Например, мы готовы допустить снижение метрик до 10%, в противном случае изображение возвращаем. Тогда в примере выше для 1.jpg минимальным значением IoU будет 0.612, а для 2.jpg 0.792.\nФормируем табличный датасет. Каждая строка в нем, это «испорченный» сэмпл из калибровочного датасета с зафиксированным снижением целевой метрики и значениями оценочных функций. Переходим к задаче бинарной классификации. Классом будет снижение метрик не выше порога: 1 — метрики остались в коридоре 10%, 0 — ниже порога. IoU^ — это порог метрики, рассчитанный из самой первой таблицы в п.1 умножением на 0.9.\nФайл\nblur\ncontrast\nluminance\ngray\nIoU\nIoU^\ntarget\n1.jpg\n312\n30\n97\n0.45\n0.67\n0.612\n1\n1.jpg\n351\n22\n98\n0.20\n0.59\n0.612\n0\n1.jpg\n764\n30\n67\n0.50\n0.34\n0.612\n0\n2.jpg\n76\n10\n52\n0.54\n0.88\n0.792\n1\n2.jpg\n45\n12\n63\n0.33\n0.76\n0.792\n0\nСтроим на новом датасете интерпретируемую модель, например \nлогистическую регрессию\n. Фичами для модели будут значения оценочных функций. Не забудьте удалить значения целевой метрики, ведь на реальных данных метрик не будет. Итоговая таблица для обучения модели будет выглядеть так:\nblur\ncontrast\nluminance\ngray\ntarget\n312\n30\n97\n0.45\n1\n351\n22\n98\n0.20\n0\n764\n30\n67\n0.50\n0\n76\n10\n52\n0.54\n1\n45\n12\n63\n0.33\n0\nТеперь, при поступлении изображения в наш сервис, мы можем получить значения оценочных функций, определить уровень риска и принять решение о дальнейших действиях. В случае, если мы изображение не принимаем, достаем из модели‑фильтра фичи, которые сильно повлияли на решение, и на их основе формируем фидбэк.\nХорошо, а что если метрика не подразумевает гладкую оценку каждого сэмпла? Например, у нас задача классификации, метрика целевой модели — \nF1\n. Эта метрика оценивает весь датасет целиком, на каждом сэмпле ее не посчитать. Навскидку можно предложить два варианта:\nПерейти к оценке каждого сэмпла. Например, можно оценивать уверенность модели в классе при классификации.\nМасштабироваться на уровень выше. Теперь, каждой строкой в табличном датасете для калибровки будет не отдельный сэмпл, а целый проход по всему датасету с фиксированными аугментациями. Фичами будут средние значения оценочных метрик по датасету. Этот вариант может быть вычислительно очень тяжелым, особенно в случае с нейросетями.\nК достоинствам метода можно отнести довольно высокую точность. К недостаткам необходимость оценки каждого сэмпла, что не всегда возможно. Либо вычислительную сложность, если оценивать метрику на уровне всего датасета, а не отдельного сэмпла. \nСтатистические методы\nДля оценки порогов можно воспользоваться статистическими тестами. Например, методом \nLearn then test (LTT)\n. Суть метода заключается в множественных статистических тестах для каждого набора параметров (например, порогов оценочных функций). Нулевой гипотезой выступает превышение риском (который рассчитывается из выходов целевой модели) некоторого порога. Собственно, мы ищем набор параметров, отклоняющих нулевую гипотезу, не забывая делать поправку на \nмножественную проверку гипотез\n.\nК сожалению, на выходе мы скорее всего получим множество наборов параметров, среди которых все еще нужно выбрать лучший. К тому же необходимо определиться, как именно собирать наборы порогов для статистических тестов — это может быть случайное сэмплирование или изменение порогов по какому‑либо закону. \nЗаключение\nИтак, мы выполнили поставленные требования. Научились отсеивать изображения, на которых целевая модель будет работать неудовлетворительно. Причем, делать это по понятным интерпретируемым критериям. Метод работает быстро, позволяя фильтровать изображения на дешевых CPU, легко распараллеливания вычисления и достигая высокой пропускной способности.\nМы описали базовый, но законченный подход к фильтрации изображений, который вы можете использовать в своем сервисе. Фильтрация изображений позволяет:\nУскорить работу сервиса за счет экономии вычислительных ресурсов, поскольку сервис будет обрабатывать запросы избирательно, не тратя вычисления на шум и «мусор».\nПовысить качество ответов сервиса: пользователь оценивает работу искусственного интеллекта с точки зрения интеллекта естественного. Когда сервис отвечает на замыленное изображение, что оно замылено, вместо уверенного неправильного ответа, это повышает уровень доверия клиента. \nУдешевить работу сервиса в случае избыточных поступающих сигналов. Если из каждой секунды видео обрабатывать два кадра вместо тридцати, то это даст возможность обслуживать в десяток раз больше запросов в единицу времени на том же железе. Оценка качества изображения выступает в роли критерия для выбора лучших кадров.\nЕсли вам интересно самостоятельно протестировать нашу систему фильтрации, вы можете легко сделать это в приложении CarDamageTest (\nAndroid\n/\nAppStore\n/\nAppGallery\n) на собственном или арендованном авто. Попробуйте снять автомобиль ночью или при ярком свете дня, за забором или в сугробе снега, а может, вы захотите обмануть систему и снять разные автомобили? Всегда будем рады обратной связи.\nP. S. У некоторых читателей, возможно, возник вопрос: а при чем тут сосиска из заголовка? А мы сами не знаем:) Этот заголовок предложила GPT после ознакомления со статьей. Почему бы и нет? \nСсылки\nДатасет для классификации котов и собак: \nhttps://www.kaggle.com/competitions/dogs-vs-cats/overview\nЗадача детекции в компьютерном зрении: \nhttps://en.wikipedia.org/wiki/Object_detection\nCommon objects in context - открытый датасет для детекции: \nhttps://cocodataset.org/#home\nРепозиторий open-source детектора YOLOv8: \nhttps://github.com/ultralytics/ultralytics\nПиковое отношение сигнала к шуму (PSNR): \nhttps://ru.wikipedia.org/wiki/%D0%9F%D0%B8%D0%BA%D0%BE%D0%B2%D0%BE%D0%B5_%D0%BE%D1%82%D0%BD%D0%BE%D1%88%D0%B5%D0%BD%D0%B8%D0%B5_%D1%81%D0%B8%D0%B3%D0%BD%D0%B0%D0%BB%D0%B0_%D0%BA_%D1%88%D1%83%D0%BC%D1%83\nИндекс структурного сходства (SSIM): \nhttps://ru.wikipedia.org/wiki/SSIM\nNo-Reference Image Quality Assessment in the Spatial Domain (BRISQUE): \nhttps://live.ece.utexas.edu/publications/2012/TIP%20BRISQUE.pdf\nLBP-based Segmentation of Defocus Blur: \nhttps://www.cs.usask.ca/faculty/eramian/defocusseg/defocusSeg.pdf\nBlur detection with OpenCV: \nhttps://pyimagesearch.com/2015/09/07/blur-detection-with-opencv/\nБиблиотека для аугментации изображений Albumentations: \nhttps://albumentations.ai/\nБиблиотека для обработки изображений OpenCV: \nhttps://opencv.org/\nБиблиотека для вычислений numpy: \nhttps://numpy.org/\nКонтрастность изображений: \nhttps://en.wikipedia.org/wiki/Contrast_(vision)#Formula\nDetecting multiple bright spots in an image with Python and OpenCV: \nhttps://pyimagesearch.com/2016/10/31/detecting-multiple-bright-spots-in-an-image-with-python-and-opencv/\nLogistic regression: \nhttps://en.wikipedia.org/wiki/Logistic_regression\nLearn then Test: Calibrating Predictive Algorithms to Achieve Risk Control: \nhttps://arxiv.org/pdf/2110.01052.pdf\nMultiple comparisons problem: \nhttps://en.wikipedia.org/wiki/Multiple_comparisons_problem\n \n ",
    "tags": [
        "оценка повреждений",
        "machine learning",
        "computer vision",
        "image quality assessment",
        "обработка изображений",
        "оценка качества изображений",
        "фильтрация изображений"
    ]
}