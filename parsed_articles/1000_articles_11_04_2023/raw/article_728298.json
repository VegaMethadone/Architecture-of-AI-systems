{
    "article_id": "728298",
    "article_name": "Запуск аналогов ChatGPT на домашнем ПК в пару кликов и с интерфейсом",
    "content": "В течении последнего месяца в сфере текстовых нейронок всё кипит - после слитой в сеть модели \nLlama\n, \naka \"ChatGPT у себя на пекарне\"\n люди ощутили, что никакой зацензуренный OpenAI по сути им и не нужен, а хорошие по мощности нейронки можно запускать локально, имея минимум \n16ГБ обычной ОЗУ и хороший процессор.\nПока технические паблики только начинают отдуплять что происходит, и выкладывают какие-то протухшие гайды месячной давности, я вам закину пару вещей прямо с фронта.\nГде запускать?\nСпособ первый - на процессоре\nЯ бы мог вставить сюда ссылку на репозиторий \nllama.cpp\n, который запускали чуть ли не на кофеварке, и сказать - пользуйтесь!\nНо как бы там ни было, это - для гиков. А у нас всё в пару кликов и без командной строки.\nИ работать должно нормально, а не «на 4ГБ».\nПоэтому, вот обещанная возможность запустить хорошую модель (13B параметров) на 16ГБ обычной ОЗУ без лишних мозгоделок - \nkoboldcpp\n.\nkoboldcpp - это форк репозитория llama.cpp, с несколькими дополнениями, и в частности интегрированным интерфейсом Kobold AI Lite, позволяющим \"общаться\" с нейросетью в нескольких режимах, создавать персонажей, сценарии, сохранять чаты и многое другое.\nСкачиваем любую стабильную версию скомпилированного exe, запускаем, выбираем модель \n(где их взять ниже)\n, переходим в браузер и пользуемся. \nВсё!\nЕсли у вас 32ГБ ОЗУ, то можно запустить и 30B модель - качество будет сильно лучше, но скорость ниже.\nДанный способ принимает модели в формате \nggml\n, и не требует видеокарты\nP.S. Если у кого-то есть сомнения о запуске exe, то вы всегда можете проверить исходники и собрать всё самостоятельно - программа открыта.\nKobold AI Lite, Alpaca 13B. Ни одна собака не пострадала.\nСпособ второй - запускать на видеокарте\nТребует много VRAM, но скорость генерации выше. Запуск чуть сложнее, но также без выноса мозгов.\nСкачиваем вот этот репозиторий \noobabooga/one-click-installers\n и читаем приложенные инструкции - нужно будет запустить несколько батников.\nК вам в ту же папку загрузится репозиторий \noobabooga/text-generation-webui\n, и подтянет за собой все необходимые зависимости. Установка проходит чисто, используется виртуальная среда.\nДальше, для запуска моделей llama на домашней видеокарте, придётся  прописать параметры запуска, а именно:\nУказать битность модели\n --wbits 4\n (Все модели, что здесь указаны, работают в 4bit)\nИ\n --groupsize 128\n, если он был указан при конвертации модели. Узнать это можно при скачивании модели - обычно это пишут.\nПодробнее о параметрах в репозитории\nК сожалению, в повсеместные 8ГБ VRAM поместится только 7B модель в 4bit режиме, что по факту будет хуже модели 13B из первого способа. 13B влезет только в 16GB VRAM видеокарту.\nА если у вас есть 24ГБ VRAM (RTX 4090, ага), то к вам влезет даже 30B модель! Но это, конечно, меньшая часть людей.\nТакже, есть способ разделить загруженную модель на VRAM и RAM - CPU Offloading. Для этого прописываем \n--pre_layer <число разделённых слоёв>\n, к примеру 20. Но работать возможно будет даже хуже, чем на полной загрузке в ОЗУ из первого способа.\nЭтот способ запуска принимает модели в формате \ngptq\n.\nИнтерфейс чуть менее удобен и функционален, чем в первом способе. Чуток тормозной. Единственный плюс - есть extensions, такие как встроенный Google Translate, который позволит общаться с моделью на русском языке.\noobabooga - cкриншот со страницы проекта на github\nИз двух способов я советую использовать первый, т.к. он банально стабильнее, менее заморочен, и точно сможет запуститься у 80% пользователей.\nЕсли у вас есть крутая видюха с хотя бы 16ГБ VRAM - пробуйте запускать на втором.\nГде брать модели?\nСейчас есть 3 качественных модели, которые действительно имеет смысл попробовать - LLama, Alpaca и Vicuna.\nLlama \n- оригинал слитой в первые дни модели. По заявлениям синей компании, запрещённой в РФ, 13B версия в тестах равносильна ChatGPT (135B).\nПо моим ощущениям - на 80% это может быть и правда, но и не с нашей 4bit моделью.\nAlpaca \n- дотренировка Llama на данных с инструкциями. \nСделай мне то, расскажи мне это и т.д.\nЭта модель лучше чем LLama в чат режиме.\nVicuna\n - дотренировка LLama прямо на диалогах с ChatGPT. Максимально похожа на ChatGPT. Есть только 13b версия, на данный момент.\nПодчеркну - МАКСИМАЛЬНО похожа. А значит - также как и ChatGPT процензурена.\nСкачать каждую из них можно вот здесь - \nhttps://rentry.org/nur779\nОбратите внимание на формат перед скачиванием - \nggml или gptq\n.\nКонкретно я советую в данный момент использовать \nAlpaca\n. В ней всё адекватно с цензурой, она есть в 30B, и прекрасно умеет чатиться.\nВарианты использования?\nОба интерфейса позовляют создавать персонажа, в роли которого будет работать AI.\nПоэтому, вариантов использования может быть довольно много.\nПропишите персонажу, что он - AI-ассистент программист, и он будет помогать с кодом.\nСкажите, что он повар - и он поможет с рецептами.\nСкажите, что он милая девушка - и придумайте сами там что-нибудь…\nВ общем, тут всё как с ChatGPT - взаимодействие в чате мало чем отличается.\nТакже, в первом интерфейсе есть режимы Adventure и Story - позволяющие играть с нейросетью, или писать истории.\nПродвинутые же пользователи могут подключиться к API запущенных моделей, и использовать их в своих проектах. Оба интерфейса позволяют подключиться по API.\nЕсли у вас остались какие-то вопросы - пишите мне в любом удобном для вас месте.\nМой только что сделанный под статью Telegram\n \n ",
    "tags": [
        "llama",
        "нейросеть",
        "chatgpt",
        "нейросеть локально",
        "alpaca",
        "koboldai",
        "text generation"
    ]
}