{
    "article_id": "726964",
    "article_name": "Как мы управляем распределенными транзакциями в сервисах #CloudMTS. Без саг",
    "content": "\r\n\n\r\nХабр, привет! \n\r\n\n\r\nМеня зовут Илья Казначеев (\nColor\n), я техлид в команде #CloudMTS.\n\r\n\n\r\nПредставьте, что у вас есть распределенный процесс. Он состоит из сотни шагов: часть из них выполняется последовательно, часть — параллельно. Каждый шаг подразумевает один или несколько вызовов отдельных сервисов. Управление состоянием в такой системе — задача сложная. Как сделать так, чтобы изменение состояния происходило детерминированно: чтобы транзакция либо выполнилась, либо нет, чтобы процесс либо завершился полностью, либо откатился полностью. Как понять, что произошла ошибка, а главное – что нужно сделать, чтобы процесс пошел дальше или перезапустился. \n\r\n\n\r\nТакую задачу мы решали для одного из наших сервисов \nContainerum Kubernetes Service\n, и в этой статье я расскажу, как мы научились управлять распределенными транзакциями, включающими 200–300 шагов и дюжину сервисов.\n\r\n\n\r\nСразу скажу, что в статье речь пойдет не о распределенных транзакциях баз данных, а о транзакциях уровня бизнес-логики приложения. \n\r\n\n\r\nИтак, когда клиент в \nКонсоли управления\n нажимает на кнопку «Создать кластер Kubernetes», запускается многоступенчатый процесс, задействующий несколько сервисов: создаются виртуальные машины, виртуальные сети, диски, сам кластер и его сущности.\n\r\n\n\r\nВ очень сокращенном виде путь выглядит примерно так:\n\r\n\n\r\n\n\r\n\n\r\nНа каждом этапе этого верхнеуровневого пайплайна спрятана еще одна или даже несколько цепочек процессов. Ниже пример того, на какие процессы раскладывается шаг \nworker creation\n из картинки выше.\n\r\n\n\r\n\n\r\n\n\r\nИ такая «матрешка» ожидает нас на каждом этапе. \n\r\n\n\r\nА что, если у одной из ВМ не стартанула ОС? Как нам обработать ошибку? Удалить эту проблемную машину и пересоздать ее? Или нужно всю группу машин удалить? И какой компонент системы должен принимать решение о дальнейших действиях?\n\r\n\n\r\n\n\r\n\n\r\nЧтобы ответить на эти вопросы, нам нужно:\n\r\n\n\r\n\nпонимать, в каком состоянии находится каждый элемент системы;\n\r\n\nобеспечить однозначность этого состояния (создается, изменяется и прочее) и его согласованную обработку. \n\r\n\n\r\n\n\r\n\nDomain-driven design\n\r\nДля решения этих задач мы пошли по пути \nDomain-driven design\n (DDD). Описали сущности сервиса в виде доменов. Каждый отдельный сервис (сетевой сервис, платформа виртуализации, кластер Kubernetes и так далее) — это доменный агрегат, представляющий собой древовидную структуру из доменов. \n\r\n\n\r\nВот, например, так выглядит доменный агрегат кластера Kubernetes, где кластер — это корень доменного агрегата, а нода, группа нод, LB — домены. \n\r\n\n\r\n\n\r\n\n\r\nПрименяя подходы DDD, мы смогли разделить сложную логику сервиса на отдельные слабосвязанные компоненты (домены), каждый из которых имеет свое состояние. Это позволило значительно упростить работу над параллельными процессами и обработку множества событий, происходящих на протяжении жизненного цикла кластера. \n\r\n\n\r\n\nDDD и конечные автоматы (FSM)\n\r\nСледующий момент — это понять, как управлять состояниями доменных сущностей и переходами между ними. Для этого мы описали для каждого домена свой набор состояний с помощью \nконечных автоматов (Finite-state machine, FSM)\n.\n\r\n\n\r\nВот как выглядит схема конечных автоматов для одного домена — ноды внутри кластера Kubernetes: \n\r\n\n\r\n\n\r\n\n\r\nИспользование конечных автоматов помогло нам сделать обработку ошибок, согласованную с состоянием. Если при создании виртуальной машины у нас происходит ошибка, мы понимаем, что она не создалась, и сразу переходим к статусу «Удалено». Если мы ее создали, но она застревает в статусе OS_Startup_Pending, то мы сразу можем перейти к удалению ВМ.\n\r\n\n\r\nВ результате весь процесс не зависает при ошибке, а откатывается назад и перезапускается. \n\r\n\n\r\n\n\r\n\n\r\nТеперь поднимемся на один уровень выше и посмотрим, как происходит обработка событий в рамках одного доменного агрегата — кластера Kubernetes. В этой древовидной схеме у нас появляются дочерние и родительские сущности. Сущность, которая находится на более высоком уровне, является родительской по отношению к той, которая ниже. Например, кластер — родительская сущность для групп worker-нод, master-нод, load balancer. Изменение состояния одной дочерней сущности запускает действия или изменения состояний родительской сущности и наоборот. Когда происходит ошибка, родительская сущность принимает решение, что делать дальше.\n\r\n\n\r\nТеперь посмотрим на примере и картинках. \n\r\n\n\r\nВот наш доменный агрегат «кластер Kubernetes». В момент времени каждая доменная сущность находится в своем состоянии, например, у \nNode Group #1\n ноды еще создаются, а у \nNode Group #2\n все машины уже запущены.\n\r\n\n\r\n\n\r\n\n\r\nОбмен данными о состоянии между доменными сущностями происходит следующим образом:\n\r\n\n\r\n1. От сервиса виртуализации приходит ответ, что \nNode Worker #1\n создалась. \nNode Worker #1\n переходит в состояние Running.\n\r\n\n\r\n\n\r\n\n\r\n2. \nNode Worker #1\n отправляет родительской сущности \nNode Group #1\n событие о том, что машина создалась. Родительская сущность \nNode Group #1\n проверяет, все ли Worker’ы созданы. Если не все, значит ждем. \n\r\n\n\r\n\n\r\n\n\r\n3. От сервиса виртуализации приходит событие о создании ВМ для \nWorker #2\n.\n\r\n\n\r\n\n\r\n\n\r\n4. \nWorker #2\n переходит в состояние Running и отправляет событие родительской сущности \nNode Group #1\n. \n\r\n\n\r\n\n\r\n\n\r\n5. Родительская сущность \nNode Group #1\n проверяет, все ли Worker’ы находятся в статусе Running. Если да, значит \nNode Group #1\n тоже меняет состояние на Running. \n\r\n\n\r\n\n\r\n\n\r\n6. \nNode Group #1\n отправляет своей родительской сущности \nCluster\n событие о своем состоянии. Cluster проверяет, все ли \nNode Group\n готовы. В случае утвердительного ответа также меняет свое состояние на Running. \n\r\n\n\r\n\n\r\n\n\r\nПри обработке ошибок происходит похожий процесс: \n\r\n\n\r\n1. Если приходит ошибка о том, что не получилось создать виртуальную машину, мы меняем статус \nWorker Node #1\n и сообщаем родителю, что произошла ошибка. \n\r\n\n\r\n\n\r\n\n\r\n2. Родитель \nNode Group #1\n решает, что делать в этом случае: пересоздать одну ноду / удалить все ноды или отправить сообщение наверх своему родителю \nCluster\n сообщение об ошибке. Тогда уже \nCluster\n должен решать, что делать дальше.\n\r\n\n\r\n\n\r\n\n\r\nПодытожим. Каждый доменный агрегат состоит из группы доменных сущностей. Конечный автомат состояний и переходов между ними описывает поведение каждой сущности. При этом изменение состояния каждой доменной сущности может вызывать события как вниз, так и вверх по древовидной структуре доменного агрегата. \n\r\n\n\r\nТакая архитектура делает доменные сущности независимыми в своем поведении (логика состояний, действий и проверок не просачивается вниз или вверх по дереву), и при этом они связаны в рамках одного доменного агрегата. \n\r\n\n\r\nПару слов про то, что мы использовали для воплощения этой логики. Для FSM мы взяли статическую реализацию через switch. На наш взгляд, он не портит логику и читабельность кода. Если вы, как и мы, используете компилируемый язык (в нашем случае — Go), это дает дополнительные возможности для проверки при компиляции.\n\r\n\n\r\nСостояния доменов хранятся персистентно в базе данных PostgreSQL.\n\r\n\n\r\nВзаимодействие между сервисами происходит по модели CQRS (Command and Query Responsibility Segregation). Мы разделили все запросы на синхронные queries (Read operations) и асинхронные commands (CUD operations). Первые выполняются по протоколу gRPC, команды — через Apache Kafka. \n\r\n\n\r\n\nСобственно распределенные транзакции \n\r\nВыше мы разобрали, как схема с DDD и конечными автоматами работает с состояниями в одной доменной сущности и одном доменном агрегате (микросервисе). Теперь посмотрим, как тот же принцип работает в рамках распределенного процесса, охватывающего несколько доменных агрегатов, и как между ними будут передаваться данные о состоянии.\n\r\n\n\r\nДля примера возьмем сервис управления кластером Kubernetes и сетевой сервис. Для развертывания кластера Kubernetes нужно создать балансировщик нагрузки. Для этого в домен, который отвечает за сеть, отправляется команда «создать LB с таким названием и параметрами». У этого Provisioning Service описан свой конечный автомат. Он по нему проходит, создает LB и сообщает о том, что все готово. Мы это сообщение принимаем в первом сервисе и переходим на следующее состояние.\n\r\n\n\r\n\n\r\n\n\r\nЕсли смотреть на это более укрупненно, то получается следующая картина:\n\r\n\n\r\n\n\r\n\n\r\nВот эта цепочка операций между двумя сервисами и будет называться \nраспределенной транзакцией\n. Ее атомарность обеспечивается тем, что у нас в любой момент времени есть состояние каждого компонента этой транзакции: каждой доменной сущности и каждого доменного агрегата, которые входят в эту транзакцию. \n\r\n\n\r\nЭто состояние хранится в базе данных соответствующего сервиса. Если что-то происходит не по плану, у нас есть четкий сценарий, описанный в виде конечного автомата.\n\r\n\n\r\n\n\r\n\n\r\nПри разворачивании кластера Kubernetes мы имеем дело с целой цепочкой таких транзакций. Например, когда Cluster service обращается к VM service, а он в свою очередь обращается к Virtualisation platform с тем, чтобы она запустила ВМ. Для Cluster service множество шагов выглядят как один шаг, и он не знает, что VM service обращается к Virtualisation platform.\n\r\n\n\r\n\n\r\n\n\r\nУ нас получаются вложенные друг в друга транзакции: транзакция уровня Cluster service включает в себя более низкоуровневую транзакцию — VM service. \n\r\n\n\r\n\n\r\n\n\r\nВ итоге у нас получается, что cluster service «оркестрирует» VM service, а VM service «управляет» Virtualization platform. \n\r\n\n\r\n\n\r\n\n\r\nРаспределенная транзакция Cluster service может быть частью другой, более высокоуровневой транзакции. \n\r\n\n\r\n\n\r\n\n\r\nДопустим, мы создаем какой-нибудь SaaS-сервис, для которого нужен кластер Kubernetes. \n\r\n\n\r\nТогда в ней будет спрятана транзакция по созданию кластера, где, как в матрешке, живут другие транзакции. Причем для более высокоуровневой транзакции это будет выглядеть как один шаг «Запросить создание кластера».\n\r\n\n\r\n\n\r\n\n\r\n\nПри чем тут саги?\n\r\nКогда я предлагаю решить вышеописанную задачку на собеседовании, люди либо не знают, как это сделать, либо говорят: «Ой да \nсагу\n добавим, и все заработает». В действительности это не так. \n\r\n\n\r\nОписанный подход действительно может напоминать \nхореографическую сагу\n. И это была бы она, если бы не одно но: в распределенной транзакции нет последовательности шагов, как в хореографический саге, зато есть много вложенных транзакций.\n\r\n\n\r\nКроме того, в хореографической саге нужно читать код разных сервисов, чтобы разобраться, как выполняется процесс. То есть мы не можем в каком-то одном месте увидеть целиком процесс, только реакцию на отдельные события. Поэтому с хореографическими сагами в случае поломок трудно понять, что именно сломалось, после какого деплоя процесс перестал работать. \n\r\n\n\r\nВ нашем случае весь процесс целиком в рамках отдельного домена можно посмотреть в одном месте.\n\r\n\n\r\nНаша реализация также преодолевает ограничения и недостатки \nоркестрируемой саги\n. В ней, как понятно из названия, есть оркестратор, и он должен знать про все сервисы, которые задействованы в процессе. Получается, доменная логика сервисов просачивается в оркестратор. Это означает, что когда мы изменяем сервис, нам нужно менять и оркестратор. \n\r\n\n\r\nВ нашем решении нет оркестратора. Доменная логика сервиса А инкапсулирована внутрь сервиса А, она не выходит за его рамки. При этом домен А не знает про доменную логику внутри домена B. Он лишь знает некий контракт домена B, который заключается в том, что он может создать, удалить сеть/ВМ/LB, но никак не управляет тем, что внутри домена А. Такие же отношения между доменами B и С. \n\r\n\n\r\n\n\r\n\n\r\nВ итоге получается вот такое дерево транзакций. У нас есть корневая транзакция в корневом домене, к которому мы обращаемся, например, через API. Она порождает дочерние транзакции в других доменах, которые в свою очередь запускают транзакции в дочерних доменах. Каждый из этих слоев управляет только тем слоем/транзакцией, который он порождает. Управляет с точки зрения ее запуска, но не управляет тем, что внутри. \n\r\n\n\r\n\n\r\n\n\r\n\nЧто в итоге\n\r\nСхема, которую мы получили с помощью доменной модели и конечных автоматов, помогла выстроить управление распределенными транзакциями в 200–300 шагов, которые могут отрабатывать минуты. Вот ее основные преимущества:\n\r\n\n\r\n\nДоменная логика «не вытекает» из домена, а доменные транзакции независимы\n. Это дает возможность комбинировать какие угодно сервисы и в каком угодно порядке. \n\r\n\nДомен не знает о внутренности других доменов\n. Если идем вверх по этой цепочке вызова, то мы знаем, кто нас вызывает, если вниз, то мы знаем только его контракт. В обоих случаях мы не знаем, что внутри другого домена, и нам не нужно управлять его транзакцией.\n\r\n\nРаспределенная транзакция наблюдаема\n. Ее легко мониторить, легко добавить в какой-нибудь распределенный трейсинг, где будет видна вся транзакция, сервисы, которые в ней задействованы. Это нам позволяет за минуту находить и локализовывать ошибки в продакшене в экосистеме из пары дюжин микросервисов. \n\r\n\n \n ",
    "tags": [
        "транзакции",
        "саги",
        "микросервисы",
        "Kubernetes",
        "домен"
    ]
}